{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "\n",
    "# os.chdir('codes')\n",
    "import models\n",
    "import sklearn.metrics as metrics\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "data_path = 'C:/Users/admin/Desktop/pytorch_ovo/data/' # the path of the 'npc_v4_data.h5' file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeysView(<HDF5 file \"npc_v4_data.h5\" (mode r+)>)\n",
      "(36, 640, 52)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(os.path.join(data_path, 'npc_v4_data.h5')) as hf:\n",
    "    print(hf.keys())\n",
    "    images_n = np.array(hf['images']['naturalistic'])\n",
    "    neural_n = np.array(hf['neural']['naturalistic']['monkey_m']['stretch']['session_2'])\n",
    "\n",
    "print(neural_n.shape)\n",
    "n_images = neural_n.shape[1]\n",
    "n_neurons = neural_n.shape[2]\n",
    "size_imags = images_n.shape[0]\n",
    "insp_layer = 'conv3'\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(640, 299, 299, 3) (640, 3, 299, 299)\n",
      "torch.Size([1, 384, 17, 17])\n"
     ]
    }
   ],
   "source": [
    "img = np.zeros((images_n.shape[0], images_n[0].shape[0], images_n[0].shape[1], 3))\n",
    "for i in range(n_images):\n",
    "    rgb_img = np.zeros((images_n[i].shape[0], images_n[i].shape[1], 3), dtype=np.uint8)\n",
    "    rgb_img[:, :, 0] = images_n[i]\n",
    "    rgb_img[:, :, 1] = images_n[i]\n",
    "    rgb_img[:, :, 2] = images_n[i]\n",
    "    img[i] = rgb_img\n",
    "\n",
    "device = 'cuda:0' # device where you put your data and models\n",
    "alexnet = models.alexnet(pretrained=True)\n",
    "alexnet.to(device)\n",
    "alexnet.eval()\n",
    "for param in alexnet.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "\n",
    "data_x = np.transpose(img, (0, 3, 1, 2))\n",
    "print(img.shape, data_x.shape)\n",
    "neurons = neural_n[1]\n",
    "x = torch.from_numpy(data_x[0:1]).float().to(device)\n",
    "fmap = alexnet(x, layer=insp_layer)\n",
    "print(fmap.shape)\n",
    "sizes = fmap.shape[2:]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images_n (640, 299, 299)\n",
      "(640, 299, 299, 3)\n",
      "<class 'numpy.ndarray'>\n",
      "(640, 299, 299, 3)\n",
      "data_x (640, 3, 299, 299)\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:0' # device where you put your data and models\n",
    "data_path = './' # the path of the 'npc_v4_data.h5' file\n",
    "batch_size = 50 # the batch size of the data loader\n",
    "insp_layer = 'conv3' # the middle layer extracted from alexnet, available in {'conv1', 'conv2', 'conv3', 'conv4', 'conv5'}\n",
    "reps = neural_n.shape[0]\n",
    "rand_ind = np.arange(reps)\n",
    "np.random.shuffle(rand_ind)\n",
    "\n",
    "\n",
    "data_y_train = neural_n[:,:576].mean(0).astype(np.float32)\n",
    "data_y_val_origin = neural_n[:, 576:].astype(np.float32)\n",
    "data_y_val = data_y_val_origin.mean(0)\n",
    "\n",
    "#data_x = images_n[:, np.newaxis].astype(np.float32)\n",
    "data_x = img\n",
    "print('images_n', images_n.shape)\n",
    "print(data_x.shape)\n",
    "\n",
    "data_x = data_x / 255 # (640, 1, 299, 299)\n",
    "\n",
    "print(type(data_x))\n",
    "\n",
    "print(data_x.shape)\n",
    "#data_x = np.tile(data_x, [1, 3, 1, 1])\n",
    "data_x = np.transpose(data_x, (0, 3, 1, 2))\n",
    "print('data_x', data_x.shape)\n",
    "data_x_train = data_x[:576]\n",
    "data_x_val = data_x[576:]\n",
    "\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_x, data_y):\n",
    "        self.data_x = data_x\n",
    "        self.data_y = data_y\n",
    "    def __getitem__(self, index):\n",
    "        return index, self.data_x[index], self.data_y[index]\n",
    "    def __len__(self):\n",
    "        return self.data_x.shape[0]\n",
    "\n",
    "imagenet_mean = torch.tensor([0.485, 0.456, 0.406], dtype=torch.float32).view(1, 3, 1, 1).to(device)\n",
    "imagenet_std = torch.tensor([0.229, 0.224, 0.225], dtype=torch.float32).view(1, 3, 1, 1).to(device)\n",
    "transform = lambda x : (x - imagenet_mean) / imagenet_std\n",
    "\n",
    "dataset_train = Dataset(data_x_train, data_y_train)\n",
    "dataset_val = Dataset(data_x_val, data_y_val)\n",
    "\n",
    "loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "loader_val = torch.utils.data.DataLoader(dataset_val, batch_size=batch_size, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([640, 384, 17, 17])\n"
     ]
    }
   ],
   "source": [
    "feature_map = torch.Tensor(n_images, fmap.shape[1], fmap.shape[2], fmap.shape[3])\n",
    "feature_map.to(device)\n",
    "print(feature_map.shape)\n",
    "for i in range(n_images):\n",
    "    x = torch.from_numpy(data_x[i:i + 1]).float().to(device)\n",
    "    fmap = alexnet(x, layer = insp_layer)\n",
    "    feature_map[i] = fmap"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([1, 3, 299, 299])\n",
      "fmap:  torch.Size([1, 384, 17, 17])\n",
      "size:  torch.Size([17, 17])\n",
      "52 torch.Size([17, 17])\n",
      "torch.Size([52, 17, 17])\n"
     ]
    }
   ],
   "source": [
    "# CHOOSE THE AUGMENTS IF NECESSARY\n",
    "lamd_s, lamd_d = [1e-1, 1e-1] # the coefficients of the losses. Try other coefficients!\n",
    "epoches = 100 # total epochs for training the encoder\n",
    "lr = 1e-1 # the learing rate for training the encoder\n",
    "\n",
    "alexnet = models.alexnet(pretrained=True)\n",
    "alexnet.to(device)\n",
    "alexnet.eval()\n",
    "for param in alexnet.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "x = torch.from_numpy(data_x[0:1]).float().to(device)\n",
    "print(\"x:\", x.shape)\n",
    "fmap = alexnet(x, layer=insp_layer)\n",
    "\n",
    "neurons = data_y_train.shape[1]\n",
    "sizes = fmap.shape[2:]\n",
    "print(\"fmap: \", fmap.shape)\n",
    "print(\"size: \", sizes)\n",
    "channels = fmap.shape[1]\n",
    "print(neurons, sizes)\n",
    "w_s = nn.Parameter(torch.randn(size=(neurons,) + sizes))\n",
    "print(w_s.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "class conv_encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, neurons, sizes, channels):\n",
    "        super(conv_encoder, self).__init__()\n",
    "        # PUT YOUR CODES HERE\n",
    "        self.W_s = nn.Parameter(torch.randn(size=(neurons,) + sizes))\n",
    "        self.W_d = nn.Parameter(torch.randn(size = (neurons,channels,1,1)))\n",
    "        self.W_b = nn.Parameter(torch.randn(size = (1,neurons)))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # PUT YOUR CODES HERE\n",
    "        out = torch.einsum('bchw , nhw -> bnchw',x,self.W_s) # dimension : N,n,C,h,w\n",
    "        out = torch.stack(\n",
    "            [F.conv2d(out[:,n,:,:,:],torch.unsqueeze(self.W_d[n],0)) for n in range(neurons)],dim=1)\n",
    "            #dimension:N,n,1,h,w\n",
    "        out = torch.sum(out,dim=(2,3,4))\n",
    "        out = out + self.W_b\n",
    "        return out\n",
    "\n",
    "def L_e(y,pred):\n",
    "    return torch.mean(torch.sqrt(torch.sum((y-pred)**2,dim=1)))\n",
    "\n",
    "def L_2(W_s,W_d,lamd_s=lamd_s,lamd_d=lamd_d):\n",
    "    return lamd_s * torch.sum(W_s**2) + lamd_d * torch.sum(W_d**2)\n",
    "\n",
    "K = torch.tensor([\n",
    "    [0,-1,0],\n",
    "    [-1,4,-1],\n",
    "    [0,-1,0]],dtype=torch.float).to(device)\n",
    "def L_laplace(W_s,lamd_s=lamd_s):\n",
    "    return lamd_s * torch.sum(F.conv2d(torch.unsqueeze(W_s,1),K.unsqueeze(0).unsqueeze(0))**2)\n",
    "\n",
    "\n",
    "#encoder = conv_encoder(neurons, sizes, channels).to(device)\n",
    "encoder = conv_encoder(neurons, sizes, channels).to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([460, 285,  44, 122,  83,  61, 522, 359,  20, 562, 565, 260, 284, 507,\n",
      "         37, 543, 449, 493, 361, 453, 420, 242, 255, 563, 253, 249, 411, 138,\n",
      "        295,  30, 109, 172,  18, 327, 272, 525,  13,  15, 317, 560, 140, 186,\n",
      "        550,   1, 124, 162, 450, 205, 429, 381])\n"
     ]
    }
   ],
   "source": [
    "for i,(z, x, y) in enumerate(loader_train):\n",
    "    print(z)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\n    You need to define the conv_encoder() class and train the encoder.\\n    The code of alexnet has been slightly modified from the torchvision, for convenience\\n    of extracting the middle layers.\\n\\n    Example:\\n        >>> x = x.to(device) # x is a batch of images\\n        >>> x = transform(x)\\n        >>> fmap = alexnet(x, layer=insp_layer)\\n        >>> out= encoder(fmap)\\n        >>> ...\\n'"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_model(encoder, optimizer):\n",
    "    losses = []\n",
    "    encoder.train()\n",
    "    for i,(z, x,y) in enumerate(loader_train):\n",
    "        optimizer.zero_grad()\n",
    "        x = x.float().to(device)\n",
    "        y = y.float().to(device)\n",
    "        z = z.to(device)\n",
    "        x = transform(x)\n",
    "        fmap = feature_map[z - 1].to(device)\n",
    "        out = encoder(fmap)\n",
    "        l_e = L_e(y,out)\n",
    "        l_2 = L_2(encoder.W_s,encoder.W_d)\n",
    "        l_l = L_laplace(encoder.W_s)\n",
    "#         print(f'L_e = {l_e} , L_2 = {l_2} , L_l = {l_l}')\n",
    "        loss = L_e(y,out) + L_2(encoder.W_s,encoder.W_d) + L_laplace(encoder.W_s)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "#         print(f'iteration {i}, train loss: {losses[-1]}')\n",
    "\n",
    "    return losses\n",
    "\n",
    "def validate_model(encoder):\n",
    "    encoder.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    losses = []\n",
    "    for i,(z, x,y) in enumerate(loader_val):\n",
    "        x = x.float().to(device)\n",
    "        y = y.float().to(device)\n",
    "        z = z.to(device)\n",
    "        x = transform(x)\n",
    "        fmap = feature_map[z - 1].to(device)\n",
    "        out = encoder(fmap)\n",
    "        y_pred.append(out)\n",
    "        y_true.append(y)\n",
    "        l_e = L_e(y,out)\n",
    "        l_2 = L_2(encoder.W_s,encoder.W_d)\n",
    "        l_l = L_laplace(encoder.W_s)\n",
    "        print(f'L_e = {l_e} , L_2 = {l_2} , L_l = {l_l}')\n",
    "        loss = L_e(y,out) + L_2(encoder.W_s,encoder.W_d) + L_laplace(encoder.W_s)\n",
    "        losses.append(loss.item())\n",
    "    y_pred = torch.cat(y_pred)\n",
    "    y_true = torch.cat(y_true)\n",
    "    explained_variance = metrics.explained_variance_score(y_true = y_true.detach().cpu().numpy(),y_pred = y_pred.detach().cpu().numpy())\n",
    "    return explained_variance,sum(losses)/len(losses)\n",
    "\n",
    "\"\"\"\n",
    "    You need to define the conv_encoder() class and train the encoder.\n",
    "    The code of alexnet has been slightly modified from the torchvision, for convenience\n",
    "    of extracting the middle layers.\n",
    "\n",
    "    Example:\n",
    "        >>> x = x.to(device) # x is a batch of images\n",
    "        >>> x = transform(x)\n",
    "        >>> fmap = alexnet(x, layer=insp_layer)\n",
    "        >>> out= encoder(fmap)\n",
    "        >>> ...\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "# losses_train = []\n",
    "# losses_val = []\n",
    "# EVs = []\n",
    "\n",
    "losses_train = []\n",
    "losses_val = []\n",
    "EVs = []"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "\n",
    "lr = 1e-1\n",
    "optimizer = torch.optim.Adam(encoder.parameters(), lr=lr)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\envs\\neural_control\\lib\\site-packages\\ipykernel_launcher.py:1: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1de7169c41df410cad132febe00e852b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L_e = 1503.5555419921875 , L_2 = 1967.69580078125 , L_l = 3312.328857421875\n",
      "L_e = 1517.6104736328125 , L_2 = 1967.69580078125 , L_l = 3312.328857421875\n",
      "epoch 0, EV = -62782.504845252406, val  loss = 6790.607421875 , train loss 14083.74345703125\n",
      "L_e = 643.3994750976562 , L_2 = 1225.086669921875 , L_l = 973.2647705078125\n",
      "L_e = 619.8322143554688 , L_2 = 1225.086669921875 , L_l = 973.2647705078125\n",
      "epoch 1, EV = -13038.190079909105, val  loss = 2829.96728515625 , train loss 4252.468896484375\n",
      "L_e = 305.5891418457031 , L_2 = 793.1348876953125 , L_l = 285.9554138183594\n",
      "L_e = 276.92645263671875 , L_2 = 793.1348876953125 , L_l = 285.9554138183594\n",
      "epoch 2, EV = -2854.688223031851, val  loss = 1370.3480834960938 , train loss 1941.6661376953125\n",
      "L_e = 137.84507751464844 , L_2 = 534.4629516601562 , L_l = 90.95877838134766\n",
      "L_e = 139.381591796875 , L_2 = 534.4629516601562 , L_l = 90.95877838134766\n",
      "epoch 3, EV = -598.4357815522415, val  loss = 764.0350952148438 , train loss 1018.3526428222656\n",
      "L_e = 72.16681671142578 , L_2 = 373.9234924316406 , L_l = 28.656875610351562\n",
      "L_e = 76.62093353271484 , L_2 = 373.9234924316406 , L_l = 28.656875610351562\n",
      "epoch 4, EV = -179.5511247561528, val  loss = 476.9742431640625 , train loss 618.7586730957031\n",
      "L_e = 910.8668823242188 , L_2 = 273.4722900390625 , L_l = 9.69726848602295\n",
      "L_e = 935.6388549804688 , L_2 = 273.4722900390625 , L_l = 9.69726848602295\n",
      "epoch 5, EV = -867.1151143220754, val  loss = 1206.42236328125 , train loss 584.8488555908203\n",
      "L_e = 907.788330078125 , L_2 = 207.75390625 , L_l = 3.8779263496398926\n",
      "L_e = 946.4404907226562 , L_2 = 207.75390625 , L_l = 3.8779263496398926\n",
      "epoch 6, EV = -1684.1815339372708, val  loss = 1138.7462158203125 , train loss 1111.2181610107423\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in tqdm_notebook(range(epoches)):\n",
    "    losses_train += train_model(encoder,optimizer)\n",
    "    ev,loss = validate_model(encoder)\n",
    "    EVs.append(ev)\n",
    "    losses_val.append(loss)\n",
    "    print(f'epoch {epoch}, EV = {ev}, val  loss = {loss} , train loss {sum(losses_train[-10:])/10}')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
