{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "\n",
    "# os.chdir('codes')\n",
    "import models\n",
    "import sklearn.metrics as metrics\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "data_path = 'C:/Users/admin/Desktop/pytorch_ovo/data/' # the path of the 'npc_v4_data.h5' file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeysView(<HDF5 file \"npc_v4_data.h5\" (mode r+)>)\n",
      "(36, 640, 52)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(os.path.join(data_path, 'npc_v4_data.h5')) as hf:\n",
    "    print(hf.keys())\n",
    "    images_n = np.array(hf['images']['naturalistic'])\n",
    "    neural_n = np.array(hf['neural']['naturalistic']['monkey_m']['stretch']['session_2'])\n",
    "\n",
    "print(neural_n.shape)\n",
    "n_images = neural_n.shape[1]\n",
    "n_neurons = neural_n.shape[2]\n",
    "size_imags = images_n.shape[0]\n",
    "insp_layer = 'conv3'\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(640, 299, 299, 3) (640, 3, 299, 299)\n",
      "torch.Size([1, 384, 17, 17])\n"
     ]
    }
   ],
   "source": [
    "img = np.zeros((images_n.shape[0], images_n[0].shape[0], images_n[0].shape[1], 3))\n",
    "for i in range(n_images):\n",
    "    rgb_img = np.zeros((images_n[i].shape[0], images_n[i].shape[1], 3), dtype=np.uint8)\n",
    "    rgb_img[:, :, 0] = images_n[i]\n",
    "    rgb_img[:, :, 1] = images_n[i]\n",
    "    rgb_img[:, :, 2] = images_n[i]\n",
    "    img[i] = rgb_img\n",
    "\n",
    "device = 'cuda:0' # device where you put your data and models\n",
    "alexnet = models.alexnet(pretrained=True)\n",
    "alexnet.to(device)\n",
    "alexnet.eval()\n",
    "for param in alexnet.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "\n",
    "data_x = np.transpose(img, (0, 3, 1, 2))\n",
    "print(img.shape, data_x.shape)\n",
    "neurons = neural_n[1]\n",
    "x = torch.from_numpy(data_x[0:1]).float().to(device)\n",
    "fmap = alexnet(x, layer=insp_layer)\n",
    "print(fmap.shape)\n",
    "sizes = fmap.shape[2:]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images_n (640, 299, 299)\n",
      "(640, 299, 299, 3)\n",
      "<class 'numpy.ndarray'>\n",
      "(640, 299, 299, 3)\n",
      "data_x (640, 3, 299, 299)\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:0' # device where you put your data and models\n",
    "data_path = './' # the path of the 'npc_v4_data.h5' file\n",
    "batch_size = 16 # the batch size of the data loader\n",
    "insp_layer = 'conv3' # the middle layer extracted from alexnet, available in {'conv1', 'conv2', 'conv3', 'conv4', 'conv5'}\n",
    "reps = neural_n.shape[0]\n",
    "rand_ind = np.arange(reps)\n",
    "np.random.shuffle(rand_ind)\n",
    "\n",
    "\n",
    "data_y_train = neural_n[:,:576].mean(0).astype(np.float32)\n",
    "data_y_val_origin = neural_n[:, 576:].astype(np.float32)\n",
    "data_y_val = data_y_val_origin.mean(0)\n",
    "\n",
    "#data_x = images_n[:, np.newaxis].astype(np.float32)\n",
    "data_x = img\n",
    "print('images_n', images_n.shape)\n",
    "print(data_x.shape)\n",
    "\n",
    "data_x = data_x / 255 # (640, 1, 299, 299)\n",
    "\n",
    "print(type(data_x))\n",
    "\n",
    "print(data_x.shape)\n",
    "#data_x = np.tile(data_x, [1, 3, 1, 1])\n",
    "data_x = np.transpose(data_x, (0, 3, 1, 2))\n",
    "print('data_x', data_x.shape)\n",
    "data_x_train = data_x[:576]\n",
    "data_x_val = data_x[576:]\n",
    "\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_x, data_y):\n",
    "        self.data_x = data_x\n",
    "        self.data_y = data_y\n",
    "    def __getitem__(self, index):\n",
    "        return index, self.data_x[index], self.data_y[index]\n",
    "    def __len__(self):\n",
    "        return self.data_x.shape[0]\n",
    "\n",
    "imagenet_mean = torch.tensor([0.485, 0.456, 0.406], dtype=torch.float32).view(1, 3, 1, 1).to(device)\n",
    "imagenet_std = torch.tensor([0.229, 0.224, 0.225], dtype=torch.float32).view(1, 3, 1, 1).to(device)\n",
    "transform = lambda x : (x - imagenet_mean) / imagenet_std\n",
    "\n",
    "dataset_train = Dataset(data_x_train, data_y_train)\n",
    "dataset_val = Dataset(data_x_val, data_y_val)\n",
    "\n",
    "loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "loader_val = torch.utils.data.DataLoader(dataset_val, batch_size=batch_size, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# feature_map = torch.Tensor(n_images, fmap.shape[1], fmap.shape[2], fmap.shape[3])\n",
    "# feature_map.to(device)\n",
    "# print(feature_map.shape)\n",
    "# for i in range(n_images):\n",
    "#     x = torch.from_numpy(data_x[i:i + 1]).float().to(device)\n",
    "#     fmap = alexnet(x, layer = insp_layer)\n",
    "#     feature_map[i] = fmap"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([1, 3, 299, 299])\n",
      "fmap:  torch.Size([1, 384, 17, 17])\n",
      "size:  torch.Size([17, 17])\n",
      "52 torch.Size([17, 17])\n",
      "torch.Size([52, 17, 17])\n"
     ]
    }
   ],
   "source": [
    "# CHOOSE THE AUGMENTS IF NECESSARY\n",
    "lamd_s, lamd_d = [1e-1, 1e-1] # the coefficients of the losses. Try other coefficients!\n",
    "epoches = 100 # total epochs for training the encoder\n",
    "lr = 1e-1 # the learing rate for training the encoder\n",
    "\n",
    "alexnet = models.alexnet(pretrained=True)\n",
    "alexnet.to(device)\n",
    "alexnet.eval()\n",
    "for param in alexnet.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "x = torch.from_numpy(data_x[0:1]).float().to(device)\n",
    "print(\"x:\", x.shape)\n",
    "fmap = alexnet(x, layer=insp_layer)\n",
    "\n",
    "neurons = data_y_train.shape[1]\n",
    "sizes = fmap.shape[2:]\n",
    "print(\"fmap: \", fmap.shape)\n",
    "print(\"size: \", sizes)\n",
    "channels = fmap.shape[1]\n",
    "print(neurons, sizes)\n",
    "w_s = nn.Parameter(torch.randn(size=(neurons,) + sizes))\n",
    "print(w_s.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "mse_weight = 1.0\n",
    "l1_weight = 0\n",
    "spa_weight = 1e-1\n",
    "ch_weight = 1e-1\n",
    "lap_weight = 1e-1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "class conv_encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, neurons, sizes, channels):\n",
    "        super(conv_encoder, self).__init__()\n",
    "        # PUT YOUR CODES HERE\n",
    "        self.W_s = nn.Parameter(torch.randn(size=(neurons,) + sizes))\n",
    "        self.W_d = nn.Parameter(torch.randn(size = (neurons,channels,1,1)))\n",
    "        self.W_b = nn.Parameter(torch.randn(size = (1,neurons)))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # PUT YOUR CODES HERE\n",
    "        out = torch.einsum('bchw , nhw -> bnchw',x,self.W_s) # dimension : N,n,C,h,w\n",
    "        out = torch.stack(\n",
    "            [F.conv2d(out[:,n,:,:,:],torch.unsqueeze(self.W_d[n],0)) for n in range(neurons)],dim=1)\n",
    "            #dimension:N,n,1,h,w\n",
    "        out = torch.sum(out,dim=(2,3,4))\n",
    "        out = out + self.W_b\n",
    "        return out\n",
    "\n",
    "def L_e(y,pred):\n",
    "    return torch.mean(torch.sqrt(torch.sum((y-pred)**2,dim=1)))\n",
    "\n",
    "def L_2(W_s,W_d,lamd_s=lamd_s,lamd_d=lamd_d):\n",
    "    return lamd_s * torch.sum(W_s**2) + lamd_d * torch.sum(W_d**2)\n",
    "\n",
    "K = torch.tensor([\n",
    "    [0,-1,0],\n",
    "    [-1,4,-1],\n",
    "    [0,-1,0]],dtype=torch.float).to(device)\n",
    "def L_laplace(W_s,lamd_s=lamd_s):\n",
    "    return lamd_s * torch.sum(F.conv2d(torch.unsqueeze(W_s,1),K.unsqueeze(0).unsqueeze(0))**2)\n",
    "\n",
    "\n",
    "def Loss(y, pred, W_s, W_d):\n",
    "    return mse_loss(y, pred) * mse_weight + \\\n",
    "          l2_norm_regularizer(W_s) * spa_weight + \\\n",
    "          smoothness_regularizer_2d(W_s) * lap_weight + \\\n",
    "          l2_norm_regularizer(W_d) * ch_weight\n",
    "#encoder = conv_encoder(neurons, sizes, channels).to(device)\n",
    "encoder = conv_encoder(neurons, sizes, channels).to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([108, 308, 166, 311, 296, 106, 171, 175, 202, 375, 211, 393, 475,  74,\n",
      "        239, 382])\n"
     ]
    }
   ],
   "source": [
    "for i,(z, x, y) in enumerate(loader_train):\n",
    "    print(z)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def mse_loss(prediction, response, weight=None):\n",
    "    if weight is None:\n",
    "        mse_loss = torch.mean(torch.mean((prediction - response)**2, dim=1))\n",
    "    else:\n",
    "        mse_loss = torch.sum(weight*torch.mean((prediction - response)**2, dim=1))\n",
    "    return mse_loss\n",
    "\n",
    "def l2_norm_regularizer(W):\n",
    "    with torch.autograd.profiler.record_function('l2_norm'):\n",
    "        penalty = torch.mean(torch.sum(W**2))\n",
    "        return penalty\n",
    "\n",
    "def l1_norm_regularizer(W):\n",
    "    with torch.autograd.profiler.record_function('l1_norm'):\n",
    "        penalty = torch.mean(torch.sum(torch.abs(W)))\n",
    "        return penalty\n",
    "\n",
    "def smoothness_regularizer_2d(W_s):\n",
    "    K = torch.tensor([\n",
    "    [0,-1,0],\n",
    "    [-1,4,-1],\n",
    "    [0,-1,0]],dtype=torch.float).to(device)\n",
    "    return torch.sum(F.conv2d(torch.unsqueeze(W_s,1),K.unsqueeze(0).unsqueeze(0))**2)\n",
    "\n",
    "def pearson_corr(prediction, response):\n",
    "    prediction = torch.transpose(prediction, 1, 0)\n",
    "    response = torch.transpose(response, 1, 0)\n",
    "\n",
    "    prediction_mean = torch.mean(prediction, dim=0)\n",
    "    response_mean = torch.mean(response, dim=0)\n",
    "\n",
    "    num = torch.sum((prediction - prediction_mean)*(response - response_mean), dim=0)\n",
    "    den = torch.sqrt(torch.sum((prediction - prediction_mean)**2, dim=0) *\n",
    "                     torch.sum((response - response_mean)**2, dim=0))\n",
    "    pcc = torch.mean(num * (1 / den))\n",
    "    return pcc\n",
    "\n",
    "def explained_variance(y_true, y_pred):\n",
    "    total_var = torch.var(y_true)\n",
    "    residual_var = torch.var(y_true - y_pred)\n",
    "    explained_var = total_var - residual_var\n",
    "    return explained_var.item()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\n    You need to define the conv_encoder() class and train the encoder.\\n    The code of alexnet has been slightly modified from the torchvision, for convenience\\n    of extracting the middle layers.\\n\\n    Example:\\n        >>> x = x.to(device) # x is a batch of images\\n        >>> x = transform(x)\\n        >>> fmap = alexnet(x, layer=insp_layer)\\n        >>> out= encoder(fmap)\\n        >>> ...\\n'"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_model(encoder, optimizer):\n",
    "    losses = []\n",
    "    encoder.train()\n",
    "    for i,(z, x,y) in enumerate(loader_train):\n",
    "        optimizer.zero_grad()\n",
    "        x = x.float().to(device)\n",
    "        y = y.float().to(device)\n",
    "        z = z.to(device)\n",
    "        x = transform(x)\n",
    "        fmap = alexnet(x,layer = insp_layer)\n",
    "        #x = transform(x)\n",
    "        #fmap = feature_map[z - 1].to(device)\n",
    "        out = encoder(fmap)\n",
    "#         print(f'L_e = {l_e} , L_2 = {l_2} , L_l = {l_l}')\n",
    "        loss = Loss(y, out, encoder.W_s, encoder.W_d)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        del loss\n",
    "#         print(f'iteration {i}, train loss: {losses[-1]}')\n",
    "\n",
    "    return losses\n",
    "\n",
    "def validate_model(encoder):\n",
    "    encoder.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    losses = []\n",
    "    for i,(z, x,y) in enumerate(loader_val):\n",
    "        x = x.float().to(device)\n",
    "        y = y.float().to(device)\n",
    "        z = z.to(device)\n",
    "        #x = transform(x)\n",
    "        #fmap = feature_map[z - 1].to(device)\n",
    "        fmap = alexnet(x,layer = insp_layer)\n",
    "        out = encoder(fmap)\n",
    "        y_pred.append(out)\n",
    "        y_true.append(y)\n",
    "        loss = Loss(y, out, encoder.W_s, encoder.W_d)\n",
    "        losses.append(loss.item())\n",
    "        del loss\n",
    "    y_pred = torch.cat(y_pred)\n",
    "    y_true = torch.cat(y_true)\n",
    "    ev = explained_variance(y_true, y_pred)\n",
    "    pcc = pearson_corr(y_pred, y_true)\n",
    "    return pcc, ev,sum(losses)/len(losses)\n",
    "    #return explained_variance,sum(losses)/len(losses)\n",
    "\n",
    "\"\"\"\n",
    "    You need to define the conv_encoder() class and train the encoder.\n",
    "    The code of alexnet has been slightly modified from the torchvision, for convenience\n",
    "    of extracting the middle layers.\n",
    "\n",
    "    Example:\n",
    "        >>> x = x.to(device) # x is a batch of images\n",
    "        >>> x = transform(x)\n",
    "        >>> fmap = alexnet(x, layer=insp_layer)\n",
    "        >>> out= encoder(fmap)\n",
    "        >>> ...\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# losses_train = []\n",
    "# losses_val = []\n",
    "# EVs = []\n",
    "\n",
    "losses_train = []\n",
    "losses_val = []\n",
    "EVs = []\n",
    "pccs = []"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "optimizer = torch.optim.Adam(encoder.parameters(), lr=lr)\n",
    "#optimizer = torch.optim.SGD(encoder.parameters(), lr=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=200, gamma=0.1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\envs\\neural_control\\lib\\site-packages\\ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "10d5561f8e7045f49cd979eb159cde36"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, EV = -94474.953125, val  loss = 121497.7578125 , train loss 974324.9, pcc = -0.009674177505075932\n",
      "epoch 1, EV = -56988.1171875, val  loss = 83327.337890625 , train loss 544319.696875, pcc = -0.02460622228682041\n",
      "epoch 2, EV = -41007.88671875, val  loss = 66853.2373046875 , train loss 337531.609375, pcc = -0.006384260952472687\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 354.00 MiB (GPU 0; 8.00 GiB total capacity; 5.55 GiB already allocated; 128.98 MiB free; 6.07 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\2\\ipykernel_10712\\1387766017.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mendure\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m10\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mepoch\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mtqdm_notebook\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mepoches\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m     \u001B[0mlosses_train\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0mtrain_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mencoder\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0moptimizer\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      7\u001B[0m     \u001B[0mpcc\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mev\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mvalidate_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mencoder\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m     \u001B[1;31m#ev,loss = validate_model(encoder)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\2\\ipykernel_10712\\4218516259.py\u001B[0m in \u001B[0;36mtrain_model\u001B[1;34m(encoder, optimizer)\u001B[0m\n\u001B[0;32m     11\u001B[0m         \u001B[1;31m#x = transform(x)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     12\u001B[0m         \u001B[1;31m#fmap = feature_map[z - 1].to(device)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 13\u001B[1;33m         \u001B[0mout\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mencoder\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfmap\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     14\u001B[0m \u001B[1;31m#         print(f'L_e = {l_e} , L_2 = {l_2} , L_l = {l_l}')\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     15\u001B[0m         \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mLoss\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mout\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mencoder\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mW_s\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mencoder\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mW_d\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\neural_control\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m    725\u001B[0m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    726\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 727\u001B[1;33m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    728\u001B[0m         for hook in itertools.chain(\n\u001B[0;32m    729\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\2\\ipykernel_10712\\3543687692.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     11\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     12\u001B[0m         \u001B[1;31m# PUT YOUR CODES HERE\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 13\u001B[1;33m         \u001B[0mout\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0meinsum\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'bchw , nhw -> bnchw'\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mW_s\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;31m# dimension : N,n,C,h,w\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     14\u001B[0m         out = torch.stack(\n\u001B[0;32m     15\u001B[0m             [F.conv2d(out[:,n,:,:,:],torch.unsqueeze(self.W_d[n],0)) for n in range(neurons)],dim=1)\n",
      "\u001B[1;32m~\\anaconda3\\envs\\neural_control\\lib\\site-packages\\torch\\functional.py\u001B[0m in \u001B[0;36meinsum\u001B[1;34m(equation, *operands)\u001B[0m\n\u001B[0;32m    342\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0meinsum\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mequation\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0m_operands\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    343\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 344\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0m_VF\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0meinsum\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mequation\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moperands\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# type: ignore\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    345\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    346\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA out of memory. Tried to allocate 354.00 MiB (GPU 0; 8.00 GiB total capacity; 5.55 GiB already allocated; 128.98 MiB free; 6.07 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "epoches = 2000\n",
    "best_loss = 1e100\n",
    "not_improve = 0\n",
    "endure = 10\n",
    "for epoch in tqdm_notebook(range(epoches)):\n",
    "    losses_train += train_model(encoder,optimizer)\n",
    "    pcc, ev,loss = validate_model(encoder)\n",
    "    #ev,loss = validate_model(encoder)\n",
    "    EVs.append(ev)\n",
    "    pccs.append(pcc)\n",
    "    losses_val.append(loss)\n",
    "    train_loss = sum(losses_train[-10:])/10\n",
    "    if train_loss < best_loss - 1e-5:\n",
    "        not_improve = 0\n",
    "    else:\n",
    "        not_improve += 1\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'epoch {epoch}, EV = {ev}, val  loss = {loss} , train loss {sum(losses_train[-10:])/10}, pcc = {pcc}')\n",
    "        #print(f'epoch {epoch}, EV = {ev}, val  loss = {loss} , train loss {sum(losses_train[-10:])/10}')\n",
    "    if not_improve == endure:\n",
    "        print(\"Early stopping!\")\n",
    "\n",
    "    scheduler.step()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(max(EVs))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
