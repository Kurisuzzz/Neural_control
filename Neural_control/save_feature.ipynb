{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "\n",
    "# os.chdir('codes')\n",
    "import models\n",
    "import sklearn.metrics as metrics\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "data_path = 'C:/Users/admin/Desktop/pytorch_ovo/data/' # the path of the 'npc_v4_data.h5' file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeysView(<HDF5 file \"npc_v4_data.h5\" (mode r+)>)\n",
      "(36, 640, 52)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(os.path.join(data_path, 'npc_v4_data.h5')) as hf:\n",
    "    print(hf.keys())\n",
    "    images_n = np.array(hf['images']['naturalistic'])\n",
    "    neural_n = np.array(hf['neural']['naturalistic']['monkey_m']['stretch']['session_2'])\n",
    "\n",
    "print(neural_n.shape)\n",
    "n_images = neural_n.shape[1]\n",
    "n_neurons = neural_n.shape[2]\n",
    "size_imags = images_n.shape[0]\n",
    "insp_layer = 'conv3'\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(640, 299, 299, 3) (640, 3, 299, 299)\n",
      "torch.Size([1, 384, 17, 17])\n"
     ]
    }
   ],
   "source": [
    "img = np.zeros((images_n.shape[0], images_n[0].shape[0], images_n[0].shape[1], 3))\n",
    "for i in range(n_images):\n",
    "    rgb_img = np.zeros((images_n[i].shape[0], images_n[i].shape[1], 3), dtype=np.uint8)\n",
    "    rgb_img[:, :, 0] = images_n[i]\n",
    "    rgb_img[:, :, 1] = images_n[i]\n",
    "    rgb_img[:, :, 2] = images_n[i]\n",
    "    img[i] = rgb_img\n",
    "\n",
    "device = 'cuda:0' # device where you put your data and models\n",
    "alexnet = models.alexnet(pretrained=True)\n",
    "alexnet.to(device)\n",
    "alexnet.eval()\n",
    "for param in alexnet.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "\n",
    "data_x = np.transpose(img, (0, 3, 1, 2))\n",
    "print(img.shape, data_x.shape)\n",
    "neurons = neural_n[1]\n",
    "x = torch.from_numpy(data_x[0:1]).float().to(device)\n",
    "fmap = alexnet(x, layer=insp_layer)\n",
    "print(fmap.shape)\n",
    "sizes = fmap.shape[2:]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images_n (640, 299, 299)\n",
      "(640, 299, 299, 3)\n",
      "<class 'numpy.ndarray'>\n",
      "(640, 299, 299, 3)\n",
      "data_x (640, 3, 299, 299)\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:0' # device where you put your data and models\n",
    "data_path = './' # the path of the 'npc_v4_data.h5' file\n",
    "batch_size = 50 # the batch size of the data loader\n",
    "insp_layer = 'conv3' # the middle layer extracted from alexnet, available in {'conv1', 'conv2', 'conv3', 'conv4', 'conv5'}\n",
    "reps = neural_n.shape[0]\n",
    "rand_ind = np.arange(reps)\n",
    "np.random.shuffle(rand_ind)\n",
    "\n",
    "\n",
    "data_y_train = neural_n[:,:576].mean(0).astype(np.float32)\n",
    "data_y_val_origin = neural_n[:, 576:].astype(np.float32)\n",
    "data_y_val = data_y_val_origin.mean(0)\n",
    "\n",
    "#data_x = images_n[:, np.newaxis].astype(np.float32)\n",
    "data_x = img\n",
    "print('images_n', images_n.shape)\n",
    "print(data_x.shape)\n",
    "\n",
    "data_x = data_x / 255 # (640, 1, 299, 299)\n",
    "\n",
    "print(type(data_x))\n",
    "\n",
    "print(data_x.shape)\n",
    "#data_x = np.tile(data_x, [1, 3, 1, 1])\n",
    "data_x = np.transpose(data_x, (0, 3, 1, 2))\n",
    "print('data_x', data_x.shape)\n",
    "data_x_train = data_x[:576]\n",
    "data_x_val = data_x[576:]\n",
    "\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_x, data_y):\n",
    "        self.data_x = data_x\n",
    "        self.data_y = data_y\n",
    "    def __getitem__(self, index):\n",
    "        return index, self.data_x[index], self.data_y[index]\n",
    "    def __len__(self):\n",
    "        return self.data_x.shape[0]\n",
    "\n",
    "imagenet_mean = torch.tensor([0.485, 0.456, 0.406], dtype=torch.float32).view(1, 3, 1, 1).to(device)\n",
    "imagenet_std = torch.tensor([0.229, 0.224, 0.225], dtype=torch.float32).view(1, 3, 1, 1).to(device)\n",
    "transform = lambda x : (x - imagenet_mean) / imagenet_std\n",
    "\n",
    "dataset_train = Dataset(data_x_train, data_y_train)\n",
    "dataset_val = Dataset(data_x_val, data_y_val)\n",
    "\n",
    "loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "loader_val = torch.utils.data.DataLoader(dataset_val, batch_size=batch_size, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([640, 384, 17, 17])\n"
     ]
    }
   ],
   "source": [
    "feature_map = torch.Tensor(n_images, fmap.shape[1], fmap.shape[2], fmap.shape[3])\n",
    "feature_map.to(device)\n",
    "print(feature_map.shape)\n",
    "for i in range(n_images):\n",
    "    x = torch.from_numpy(data_x[i:i + 1]).float().to(device)\n",
    "    fmap = alexnet(x, layer = insp_layer)\n",
    "    feature_map[i] = fmap"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([1, 3, 299, 299])\n",
      "fmap:  torch.Size([1, 384, 17, 17])\n",
      "size:  torch.Size([17, 17])\n",
      "52 torch.Size([17, 17])\n",
      "torch.Size([52, 17, 17])\n"
     ]
    }
   ],
   "source": [
    "# CHOOSE THE AUGMENTS IF NECESSARY\n",
    "lamd_s, lamd_d = [1e-1, 1e-1] # the coefficients of the losses. Try other coefficients!\n",
    "epoches = 100 # total epochs for training the encoder\n",
    "lr = 1e-1 # the learing rate for training the encoder\n",
    "\n",
    "alexnet = models.alexnet(pretrained=True)\n",
    "alexnet.to(device)\n",
    "alexnet.eval()\n",
    "for param in alexnet.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "x = torch.from_numpy(data_x[0:1]).float().to(device)\n",
    "print(\"x:\", x.shape)\n",
    "fmap = alexnet(x, layer=insp_layer)\n",
    "\n",
    "neurons = data_y_train.shape[1]\n",
    "sizes = fmap.shape[2:]\n",
    "print(\"fmap: \", fmap.shape)\n",
    "print(\"size: \", sizes)\n",
    "channels = fmap.shape[1]\n",
    "print(neurons, sizes)\n",
    "w_s = nn.Parameter(torch.randn(size=(neurons,) + sizes))\n",
    "print(w_s.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "class conv_encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, neurons, sizes, channels):\n",
    "        super(conv_encoder, self).__init__()\n",
    "        # PUT YOUR CODES HERE\n",
    "        self.W_s = nn.Parameter(torch.randn(size=(neurons,) + sizes))\n",
    "        self.W_d = nn.Parameter(torch.randn(size = (neurons,channels,1,1)))\n",
    "        self.W_b = nn.Parameter(torch.randn(size = (1,neurons)))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # PUT YOUR CODES HERE\n",
    "        out = torch.einsum('bchw , nhw -> bnchw',x,self.W_s) # dimension : N,n,C,h,w\n",
    "        out = torch.stack(\n",
    "            [F.conv2d(out[:,n,:,:,:],torch.unsqueeze(self.W_d[n],0)) for n in range(neurons)],dim=1)\n",
    "            #dimension:N,n,1,h,w\n",
    "        out = torch.sum(out,dim=(2,3,4))\n",
    "        out = out + self.W_b\n",
    "        return out\n",
    "\n",
    "def L_e(y,pred):\n",
    "    return torch.mean(torch.sqrt(torch.sum((y-pred)**2,dim=1)))\n",
    "\n",
    "def L_2(W_s,W_d,lamd_s=lamd_s,lamd_d=lamd_d):\n",
    "    return lamd_s * torch.sum(W_s**2) + lamd_d * torch.sum(W_d**2)\n",
    "\n",
    "K = torch.tensor([\n",
    "    [0,-1,0],\n",
    "    [-1,4,-1],\n",
    "    [0,-1,0]],dtype=torch.float).to(device)\n",
    "def L_laplace(W_s,lamd_s=lamd_s):\n",
    "    return lamd_s * torch.sum(F.conv2d(torch.unsqueeze(W_s,1),K.unsqueeze(0).unsqueeze(0))**2)\n",
    "\n",
    "\n",
    "#encoder = conv_encoder(neurons, sizes, channels).to(device)\n",
    "encoder = conv_encoder(neurons, sizes, channels).to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([460, 285,  44, 122,  83,  61, 522, 359,  20, 562, 565, 260, 284, 507,\n",
      "         37, 543, 449, 493, 361, 453, 420, 242, 255, 563, 253, 249, 411, 138,\n",
      "        295,  30, 109, 172,  18, 327, 272, 525,  13,  15, 317, 560, 140, 186,\n",
      "        550,   1, 124, 162, 450, 205, 429, 381])\n"
     ]
    }
   ],
   "source": [
    "for i,(z, x, y) in enumerate(loader_train):\n",
    "    print(z)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\n    You need to define the conv_encoder() class and train the encoder.\\n    The code of alexnet has been slightly modified from the torchvision, for convenience\\n    of extracting the middle layers.\\n\\n    Example:\\n        >>> x = x.to(device) # x is a batch of images\\n        >>> x = transform(x)\\n        >>> fmap = alexnet(x, layer=insp_layer)\\n        >>> out= encoder(fmap)\\n        >>> ...\\n'"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_model(encoder, optimizer):\n",
    "    losses = []\n",
    "    encoder.train()\n",
    "    for i,(z, x,y) in enumerate(loader_train):\n",
    "        optimizer.zero_grad()\n",
    "        x = x.float().to(device)\n",
    "        y = y.float().to(device)\n",
    "        z = z.to(device)\n",
    "        x = transform(x)\n",
    "        fmap = feature_map[z - 1].to(device)\n",
    "        out = encoder(fmap)\n",
    "        l_e = L_e(y,out)\n",
    "        l_2 = L_2(encoder.W_s,encoder.W_d)\n",
    "        l_l = L_laplace(encoder.W_s)\n",
    "#         print(f'L_e = {l_e} , L_2 = {l_2} , L_l = {l_l}')\n",
    "        loss = L_e(y,out) + L_2(encoder.W_s,encoder.W_d) + L_laplace(encoder.W_s)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "#         print(f'iteration {i}, train loss: {losses[-1]}')\n",
    "\n",
    "    return losses\n",
    "\n",
    "def validate_model(encoder):\n",
    "    encoder.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    losses = []\n",
    "    for i,(z, x,y) in enumerate(loader_val):\n",
    "        x = x.float().to(device)\n",
    "        y = y.float().to(device)\n",
    "        z = z.to(device)\n",
    "        x = transform(x)\n",
    "        fmap = feature_map[z - 1].to(device)\n",
    "        out = encoder(fmap)\n",
    "        y_pred.append(out)\n",
    "        y_true.append(y)\n",
    "        l_e = L_e(y,out)\n",
    "        l_2 = L_2(encoder.W_s,encoder.W_d)\n",
    "        l_l = L_laplace(encoder.W_s)\n",
    "        print(f'L_e = {l_e} , L_2 = {l_2} , L_l = {l_l}')\n",
    "        loss = L_e(y,out) + L_2(encoder.W_s,encoder.W_d) + L_laplace(encoder.W_s)\n",
    "        losses.append(loss.item())\n",
    "    y_pred = torch.cat(y_pred)\n",
    "    y_true = torch.cat(y_true)\n",
    "    explained_variance = metrics.explained_variance_score(y_true = y_true.detach().cpu().numpy(),y_pred = y_pred.detach().cpu().numpy())\n",
    "    return explained_variance,sum(losses)/len(losses)\n",
    "\n",
    "\"\"\"\n",
    "    You need to define the conv_encoder() class and train the encoder.\n",
    "    The code of alexnet has been slightly modified from the torchvision, for convenience\n",
    "    of extracting the middle layers.\n",
    "\n",
    "    Example:\n",
    "        >>> x = x.to(device) # x is a batch of images\n",
    "        >>> x = transform(x)\n",
    "        >>> fmap = alexnet(x, layer=insp_layer)\n",
    "        >>> out= encoder(fmap)\n",
    "        >>> ...\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "# losses_train = []\n",
    "# losses_val = []\n",
    "# EVs = []\n",
    "\n",
    "losses_train = []\n",
    "losses_val = []\n",
    "EVs = []"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "\n",
    "lr = 1e-1\n",
    "optimizer = torch.optim.Adam(encoder.parameters(), lr=lr)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\envs\\neural_control\\lib\\site-packages\\ipykernel_launcher.py:1: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1de7169c41df410cad132febe00e852b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L_e = 1503.5555419921875 , L_2 = 1967.69580078125 , L_l = 3312.328857421875\n",
      "L_e = 1517.6104736328125 , L_2 = 1967.69580078125 , L_l = 3312.328857421875\n",
      "epoch 0, EV = -62782.504845252406, val  loss = 6790.607421875 , train loss 14083.74345703125\n",
      "L_e = 643.3994750976562 , L_2 = 1225.086669921875 , L_l = 973.2647705078125\n",
      "L_e = 619.8322143554688 , L_2 = 1225.086669921875 , L_l = 973.2647705078125\n",
      "epoch 1, EV = -13038.190079909105, val  loss = 2829.96728515625 , train loss 4252.468896484375\n",
      "L_e = 305.5891418457031 , L_2 = 793.1348876953125 , L_l = 285.9554138183594\n",
      "L_e = 276.92645263671875 , L_2 = 793.1348876953125 , L_l = 285.9554138183594\n",
      "epoch 2, EV = -2854.688223031851, val  loss = 1370.3480834960938 , train loss 1941.6661376953125\n",
      "L_e = 137.84507751464844 , L_2 = 534.4629516601562 , L_l = 90.95877838134766\n",
      "L_e = 139.381591796875 , L_2 = 534.4629516601562 , L_l = 90.95877838134766\n",
      "epoch 3, EV = -598.4357815522415, val  loss = 764.0350952148438 , train loss 1018.3526428222656\n",
      "L_e = 72.16681671142578 , L_2 = 373.9234924316406 , L_l = 28.656875610351562\n",
      "L_e = 76.62093353271484 , L_2 = 373.9234924316406 , L_l = 28.656875610351562\n",
      "epoch 4, EV = -179.5511247561528, val  loss = 476.9742431640625 , train loss 618.7586730957031\n",
      "L_e = 910.8668823242188 , L_2 = 273.4722900390625 , L_l = 9.69726848602295\n",
      "L_e = 935.6388549804688 , L_2 = 273.4722900390625 , L_l = 9.69726848602295\n",
      "epoch 5, EV = -867.1151143220754, val  loss = 1206.42236328125 , train loss 584.8488555908203\n",
      "L_e = 907.788330078125 , L_2 = 207.75390625 , L_l = 3.8779263496398926\n",
      "L_e = 946.4404907226562 , L_2 = 207.75390625 , L_l = 3.8779263496398926\n",
      "epoch 6, EV = -1684.1815339372708, val  loss = 1138.7462158203125 , train loss 1111.2181610107423\n",
      "L_e = 704.1671752929688 , L_2 = 163.8990020751953 , L_l = 1.91815185546875\n",
      "L_e = 759.256591796875 , L_2 = 163.8990020751953 , L_l = 1.91815185546875\n",
      "epoch 7, EV = -3458.661163320908, val  loss = 897.5290222167969 , train loss 1112.0213623046875\n",
      "L_e = 548.2462158203125 , L_2 = 128.08148193359375 , L_l = 1.111292839050293\n",
      "L_e = 566.2001342773438 , L_2 = 128.08148193359375 , L_l = 1.111292839050293\n",
      "epoch 8, EV = -1574.604291009215, val  loss = 686.4159240722656 , train loss 879.07529296875\n",
      "L_e = 345.30438232421875 , L_2 = 101.00721740722656 , L_l = 0.8623188138008118\n",
      "L_e = 373.8530578613281 , L_2 = 101.00721740722656 , L_l = 0.8623188138008118\n",
      "epoch 9, EV = -494.022883697198, val  loss = 461.4482421875 , train loss 591.2057037353516\n",
      "L_e = 172.77078247070312 , L_2 = 81.66574096679688 , L_l = 0.5456681847572327\n",
      "L_e = 151.5232391357422 , L_2 = 81.66574096679688 , L_l = 0.5456681847572327\n",
      "epoch 10, EV = -207.68936643921413, val  loss = 244.3584213256836 , train loss 467.7492431640625\n",
      "L_e = 48.14591598510742 , L_2 = 66.3102035522461 , L_l = 0.45621129870414734\n",
      "L_e = 50.191585540771484 , L_2 = 66.3102035522461 , L_l = 0.45621129870414734\n",
      "epoch 11, EV = -72.79295491713744, val  loss = 115.93516540527344 , train loss 246.14598083496094\n",
      "L_e = 22.50127410888672 , L_2 = 53.88478469848633 , L_l = 0.3727656304836273\n",
      "L_e = 19.651952743530273 , L_2 = 53.88478469848633 , L_l = 0.3727656304836273\n",
      "epoch 12, EV = -15.226887518396744, val  loss = 75.33416366577148 , train loss 124.29427032470703\n",
      "L_e = 12.236071586608887 , L_2 = 43.957767486572266 , L_l = 0.338060587644577\n",
      "L_e = 12.46873950958252 , L_2 = 43.957767486572266 , L_l = 0.338060587644577\n",
      "epoch 13, EV = -3.757565886928485, val  loss = 56.64823532104492 , train loss 69.7051959991455\n",
      "L_e = 666.2010498046875 , L_2 = 36.590641021728516 , L_l = 0.3281489610671997\n",
      "L_e = 704.9471435546875 , L_2 = 36.590641021728516 , L_l = 0.3281489610671997\n",
      "epoch 14, EV = -494.6419099534933, val  loss = 722.4928588867188 , train loss 66.03210678100587\n",
      "L_e = 64.5925064086914 , L_2 = 30.643808364868164 , L_l = 0.3360581398010254\n",
      "L_e = 59.573707580566406 , L_2 = 30.643808364868164 , L_l = 0.3360581398010254\n",
      "epoch 15, EV = -44.662590916340164, val  loss = 93.06297302246094 , train loss 122.81731224060059\n",
      "L_e = 679.7025756835938 , L_2 = 27.114112854003906 , L_l = 0.3506399095058441\n",
      "L_e = 648.2572021484375 , L_2 = 27.114112854003906 , L_l = 0.3506399095058441\n",
      "epoch 16, EV = -3756.041612135676, val  loss = 691.4446716308594 , train loss 903.8526748657226\n",
      "L_e = 268.5980224609375 , L_2 = 23.30816650390625 , L_l = 0.3832027018070221\n",
      "L_e = 295.58740234375 , L_2 = 23.30816650390625 , L_l = 0.3832027018070221\n",
      "epoch 17, EV = -915.481567847041, val  loss = 305.7840881347656 , train loss 409.17490539550784\n",
      "L_e = 456.98162841796875 , L_2 = 19.947998046875 , L_l = 0.42345571517944336\n",
      "L_e = 436.6107482910156 , L_2 = 19.947998046875 , L_l = 0.42345571517944336\n",
      "epoch 18, EV = -414.4356014568072, val  loss = 467.1676483154297 , train loss 363.27001037597654\n",
      "L_e = 145.898681640625 , L_2 = 18.26451873779297 , L_l = 0.632417619228363\n",
      "L_e = 155.3445587158203 , L_2 = 18.26451873779297 , L_l = 0.632417619228363\n",
      "epoch 19, EV = -80.31778698471877, val  loss = 169.5185546875 , train loss 303.7102478027344\n",
      "L_e = 44.36298751831055 , L_2 = 15.39190673828125 , L_l = 0.41818833351135254\n",
      "L_e = 43.02947235107422 , L_2 = 15.39190673828125 , L_l = 0.41818833351135254\n",
      "epoch 20, EV = -68.48105855171497, val  loss = 59.50632667541504 , train loss 96.92563705444336\n",
      "L_e = 19.21680450439453 , L_2 = 13.434286117553711 , L_l = 0.4652036726474762\n",
      "L_e = 20.768795013427734 , L_2 = 13.434286117553711 , L_l = 0.4652036726474762\n",
      "epoch 21, EV = -19.95862698669617, val  loss = 33.89228820800781 , train loss 50.08120918273926\n",
      "L_e = 9.592320442199707 , L_2 = 11.76690673828125 , L_l = 0.32564055919647217\n",
      "L_e = 9.92452621459961 , L_2 = 11.76690673828125 , L_l = 0.32564055919647217\n",
      "epoch 22, EV = -2.593036163311738, val  loss = 21.850969314575195 , train loss 29.85450897216797\n",
      "L_e = 418.87603759765625 , L_2 = 11.059252738952637 , L_l = 0.35025379061698914\n",
      "L_e = 406.04217529296875 , L_2 = 11.059252738952637 , L_l = 0.35025379061698914\n",
      "epoch 23, EV = -140.932549021565, val  loss = 423.8686218261719 , train loss 695.5510719299316\n",
      "L_e = 88.28480529785156 , L_2 = 11.545489311218262 , L_l = 0.3250351846218109\n",
      "L_e = 85.88848114013672 , L_2 = 11.545489311218262 , L_l = 0.3250351846218109\n",
      "epoch 24, EV = -176.9652037689319, val  loss = 98.9571647644043 , train loss 261.2599075317383\n",
      "L_e = 1276.94775390625 , L_2 = 10.351156234741211 , L_l = 0.7608353495597839\n",
      "L_e = 1291.6099853515625 , L_2 = 10.351156234741211 , L_l = 0.7608353495597839\n",
      "epoch 25, EV = -853.3540728963338, val  loss = 1295.3909301757812 , train loss 555.470851135254\n",
      "L_e = 1193.2044677734375 , L_2 = 10.67324161529541 , L_l = 1.0354301929473877\n",
      "L_e = 1255.8662109375 , L_2 = 10.67324161529541 , L_l = 1.0354301929473877\n",
      "epoch 26, EV = -3021.3584556315955, val  loss = 1236.2439575195312 , train loss 736.0311614990235\n",
      "L_e = 109.80716705322266 , L_2 = 9.245849609375 , L_l = 1.0292177200317383\n",
      "L_e = 87.77771759033203 , L_2 = 9.245849609375 , L_l = 1.0292177200317383\n",
      "epoch 27, EV = -430.4801830488902, val  loss = 109.06751251220703 , train loss 306.646044921875\n",
      "L_e = 74.74113464355469 , L_2 = 7.547823905944824 , L_l = 0.7775326371192932\n",
      "L_e = 68.14478302001953 , L_2 = 7.547823905944824 , L_l = 0.7775326371192932\n",
      "epoch 28, EV = -294.73910482571677, val  loss = 79.76831436157227 , train loss 170.96139793395997\n",
      "L_e = 27.06424331665039 , L_2 = 6.226736068725586 , L_l = 0.478481262922287\n",
      "L_e = 32.06308364868164 , L_2 = 6.226736068725586 , L_l = 0.478481262922287\n",
      "epoch 29, EV = -27.28201465881788, val  loss = 36.26887893676758 , train loss 65.62688522338867\n",
      "L_e = 14.202533721923828 , L_2 = 5.453793525695801 , L_l = 0.31545490026474\n",
      "L_e = 12.491183280944824 , L_2 = 5.453793525695801 , L_l = 0.31545490026474\n",
      "epoch 30, EV = -4.302869223631346, val  loss = 19.116106033325195 , train loss 28.30912342071533\n",
      "L_e = 689.9724731445312 , L_2 = 5.202840805053711 , L_l = 0.22451582551002502\n",
      "L_e = 734.5015869140625 , L_2 = 5.202840805053711 , L_l = 0.22451582551002502\n",
      "epoch 31, EV = -531.3825670618278, val  loss = 717.6643371582031 , train loss 592.033715057373\n",
      "L_e = 193.55894470214844 , L_2 = 6.32112979888916 , L_l = 0.296307772397995\n",
      "L_e = 245.2801513671875 , L_2 = 6.32112979888916 , L_l = 0.296307772397995\n",
      "epoch 32, EV = -1074.5331849364134, val  loss = 226.03699493408203 , train loss 1028.9173629760742\n",
      "L_e = 960.3331909179688 , L_2 = 6.090569019317627 , L_l = 0.5512370467185974\n",
      "L_e = 1003.6151123046875 , L_2 = 6.090569019317627 , L_l = 0.5512370467185974\n",
      "epoch 33, EV = -2082.074677070746, val  loss = 988.6159362792969 , train loss 712.5463470458984\n",
      "L_e = 308.44573974609375 , L_2 = 5.524003982543945 , L_l = 0.7059404253959656\n",
      "L_e = 314.5231628417969 , L_2 = 5.524003982543945 , L_l = 0.7059404253959656\n",
      "epoch 34, EV = -344.4101949322682, val  loss = 317.7144012451172 , train loss 515.2562263488769\n",
      "L_e = 70.70580291748047 , L_2 = 4.74045991897583 , L_l = 0.7044810652732849\n",
      "L_e = 66.13738250732422 , L_2 = 4.74045991897583 , L_l = 0.7044810652732849\n",
      "epoch 35, EV = -108.05695161911157, val  loss = 73.86653900146484 , train loss 92.49374275207519\n",
      "L_e = 44.68883514404297 , L_2 = 4.37581205368042 , L_l = 0.507956326007843\n",
      "L_e = 39.26625061035156 , L_2 = 4.37581205368042 , L_l = 0.507956326007843\n",
      "epoch 36, EV = -18.08566671724503, val  loss = 46.86131286621094 , train loss 35.26980972290039\n",
      "L_e = 42.20215606689453 , L_2 = 3.6404824256896973 , L_l = 0.30713698267936707\n",
      "L_e = 40.64759826660156 , L_2 = 3.6404824256896973 , L_l = 0.30713698267936707\n",
      "epoch 37, EV = -16.138755107155212, val  loss = 45.37249755859375 , train loss 43.428009796142575\n",
      "L_e = 13.625865936279297 , L_2 = 3.1784305572509766 , L_l = 0.3447590172290802\n",
      "L_e = 13.421131134033203 , L_2 = 3.1784305572509766 , L_l = 0.3447590172290802\n",
      "epoch 38, EV = -8.434181248912445, val  loss = 17.046688079833984 , train loss 18.094740867614746\n",
      "L_e = 26.465301513671875 , L_2 = 3.041808843612671 , L_l = 0.38634178042411804\n",
      "L_e = 24.59648895263672 , L_2 = 3.041808843612671 , L_l = 0.38634178042411804\n",
      "epoch 39, EV = -11.12917335675313, val  loss = 28.95904541015625 , train loss 1037.6037307739257\n",
      "L_e = 274.01629638671875 , L_2 = 3.826004981994629 , L_l = 0.4641238749027252\n",
      "L_e = 265.7451171875 , L_2 = 3.826004981994629 , L_l = 0.4641238749027252\n",
      "epoch 40, EV = -555.7854723827197, val  loss = 274.17083740234375 , train loss 538.3361289978027\n",
      "L_e = 27.153188705444336 , L_2 = 2.9780428409576416 , L_l = 0.3844163119792938\n",
      "L_e = 25.90631866455078 , L_2 = 2.9780428409576416 , L_l = 0.3844163119792938\n",
      "epoch 41, EV = -10.860267448883791, val  loss = 29.892212867736816 , train loss 88.63339557647706\n",
      "L_e = 2915.48779296875 , L_2 = 4.538359642028809 , L_l = 0.30576083064079285\n",
      "L_e = 2715.621826171875 , L_2 = 4.538359642028809 , L_l = 0.30576083064079285\n",
      "epoch 42, EV = -8214.277634139244, val  loss = 2820.3988037109375 , train loss 890.6424436569214\n",
      "L_e = 5839.49609375 , L_2 = 6.8577880859375 , L_l = 0.3480701446533203\n",
      "L_e = 5496.29541015625 , L_2 = 6.8577880859375 , L_l = 0.3480701446533203\n",
      "epoch 43, EV = -32907.70699298038, val  loss = 5675.101806640625 , train loss 3809.8840270996093\n",
      "L_e = 3757.22998046875 , L_2 = 5.658352375030518 , L_l = 0.36656972765922546\n",
      "L_e = 3859.1669921875 , L_2 = 5.658352375030518 , L_l = 0.36656972765922546\n",
      "epoch 44, EV = -9721.47283709737, val  loss = 3814.223388671875 , train loss 2574.6006454467774\n",
      "L_e = 294.1010437011719 , L_2 = 4.550440788269043 , L_l = 0.3364037871360779\n",
      "L_e = 389.9145812988281 , L_2 = 4.550440788269043 , L_l = 0.3364037871360779\n",
      "epoch 45, EV = -4265.903044864535, val  loss = 346.8946533203125 , train loss 1753.5292755126952\n",
      "L_e = 186.83929443359375 , L_2 = 3.081538438796997 , L_l = 0.31429243087768555\n",
      "L_e = 144.7649383544922 , L_2 = 3.081538438796997 , L_l = 0.31429243087768555\n",
      "epoch 46, EV = -1007.53609443284, val  loss = 169.19794464111328 , train loss 488.9330146789551\n",
      "L_e = 203.01724243164062 , L_2 = 2.2990424633026123 , L_l = 0.2999783158302307\n",
      "L_e = 195.36102294921875 , L_2 = 2.2990424633026123 , L_l = 0.2999783158302307\n",
      "epoch 47, EV = -54.90460637784921, val  loss = 201.78814697265625 , train loss 307.0689758300781\n",
      "L_e = 80.27556610107422 , L_2 = 2.2676913738250732 , L_l = 0.2665295898914337\n",
      "L_e = 66.4461669921875 , L_2 = 2.2676913738250732 , L_l = 0.2665295898914337\n",
      "epoch 48, EV = -100.54478601767467, val  loss = 75.89509201049805 , train loss 85.63786468505859\n",
      "L_e = 27.662532806396484 , L_2 = 1.828559398651123 , L_l = 0.23279154300689697\n",
      "L_e = 29.40888023376465 , L_2 = 1.828559398651123 , L_l = 0.23279154300689697\n",
      "epoch 49, EV = -14.544154340258011, val  loss = 30.597058296203613 , train loss 71.23037834167481\n",
      "L_e = 10.592503547668457 , L_2 = 1.4779717922210693 , L_l = 0.19341139495372772\n",
      "L_e = 11.82064151763916 , L_2 = 1.4779717922210693 , L_l = 0.19341139495372772\n",
      "epoch 50, EV = -2.7539406441725216, val  loss = 12.87795639038086 , train loss 18.774615478515624\n",
      "L_e = 343.1591796875 , L_2 = 1.5576895475387573 , L_l = 0.19503773748874664\n",
      "L_e = 357.6897888183594 , L_2 = 1.5576895475387573 , L_l = 0.19503773748874664\n",
      "epoch 51, EV = -38.66913630412175, val  loss = 352.1772003173828 , train loss 447.5561519622803\n",
      "L_e = 1540.7841796875 , L_2 = 4.507915019989014 , L_l = 0.37133464217185974\n",
      "L_e = 1584.2713623046875 , L_2 = 4.507915019989014 , L_l = 0.37133464217185974\n",
      "epoch 52, EV = -5368.239179768241, val  loss = 1567.4070434570312 , train loss 1065.0150287628173\n",
      "L_e = 40.45024871826172 , L_2 = 2.866161346435547 , L_l = 0.5617467164993286\n",
      "L_e = 47.96628952026367 , L_2 = 2.866161346435547 , L_l = 0.5617467164993286\n",
      "epoch 53, EV = -86.48566549443282, val  loss = 47.636178970336914 , train loss 385.4633201599121\n",
      "L_e = 53.83456039428711 , L_2 = 2.02311372756958 , L_l = 0.5853440165519714\n",
      "L_e = 50.67924499511719 , L_2 = 2.02311372756958 , L_l = 0.5853440165519714\n",
      "epoch 54, EV = -31.869772708186737, val  loss = 54.86535835266113 , train loss 80.3131160736084\n",
      "L_e = 614.5126953125 , L_2 = 2.7815651893615723 , L_l = 0.6527324318885803\n",
      "L_e = 576.2357788085938 , L_2 = 2.7815651893615723 , L_l = 0.6527324318885803\n",
      "epoch 55, EV = -463.67762820537274, val  loss = 598.8085021972656 , train loss 1997.8492137908936\n",
      "L_e = 203.3817596435547 , L_2 = 2.6228904724121094 , L_l = 0.6423174738883972\n",
      "L_e = 183.95887756347656 , L_2 = 2.6228904724121094 , L_l = 0.6423174738883972\n",
      "epoch 56, EV = -243.47661525698808, val  loss = 196.93553161621094 , train loss 508.5580764770508\n",
      "L_e = 1010.2426147460938 , L_2 = 3.7927865982055664 , L_l = 0.6091878414154053\n",
      "L_e = 1074.2093505859375 , L_2 = 3.7927865982055664 , L_l = 0.6091878414154053\n",
      "epoch 57, EV = -330.2644291118934, val  loss = 1046.6279602050781 , train loss 554.9014991760254\n",
      "L_e = 114.55995178222656 , L_2 = 2.4000301361083984 , L_l = 0.5062773823738098\n",
      "L_e = 138.51170349121094 , L_2 = 2.4000301361083984 , L_l = 0.5062773823738098\n",
      "epoch 58, EV = -128.5654925325742, val  loss = 129.44213104248047 , train loss 423.3823760986328\n",
      "L_e = 77.2154541015625 , L_2 = 2.2114667892456055 , L_l = 0.5143308043479919\n",
      "L_e = 99.3639144897461 , L_2 = 2.2114667892456055 , L_l = 0.5143308043479919\n",
      "epoch 59, EV = -379.09537915312325, val  loss = 91.01547622680664 , train loss 355.78272705078126\n",
      "L_e = 34.23855209350586 , L_2 = 1.9529643058776855 , L_l = 0.39859694242477417\n",
      "L_e = 24.82118034362793 , L_2 = 1.9529643058776855 , L_l = 0.39859694242477417\n",
      "epoch 60, EV = -49.65738897713331, val  loss = 31.881428718566895 , train loss 341.21527671813965\n",
      "L_e = 30.074604034423828 , L_2 = 1.7303327322006226 , L_l = 0.40150293707847595\n",
      "L_e = 29.490312576293945 , L_2 = 1.7303327322006226 , L_l = 0.40150293707847595\n",
      "epoch 61, EV = -43.94071183869472, val  loss = 31.914294242858887 , train loss 83.63503074645996\n",
      "L_e = 16.213348388671875 , L_2 = 1.5158568620681763 , L_l = 0.38757339119911194\n",
      "L_e = 18.45888900756836 , L_2 = 1.5158568620681763 , L_l = 0.38757339119911194\n",
      "epoch 62, EV = -5.499594768652549, val  loss = 19.23954963684082 , train loss 22.677251625061036\n",
      "L_e = 12.454936027526855 , L_2 = 1.0699304342269897 , L_l = 0.2914285659790039\n",
      "L_e = 12.602265357971191 , L_2 = 1.0699304342269897 , L_l = 0.2914285659790039\n",
      "epoch 63, EV = -1.3486032004539783, val  loss = 13.889959335327148 , train loss 14.20615177154541\n",
      "L_e = 6.369577407836914 , L_2 = 0.8476735949516296 , L_l = 0.18340398371219635\n",
      "L_e = 7.048079013824463 , L_2 = 0.8476735949516296 , L_l = 0.18340398371219635\n",
      "epoch 64, EV = -0.4156657010316849, val  loss = 7.739905834197998 , train loss 9.825558280944824\n",
      "L_e = 3779.0185546875 , L_2 = 4.645096778869629 , L_l = 0.29738742113113403\n",
      "L_e = 3583.327392578125 , L_2 = 4.645096778869629 , L_l = 0.29738742113113403\n",
      "epoch 65, EV = -23430.282394010286, val  loss = 3686.1153564453125 , train loss 3313.097962188721\n",
      "L_e = 238.93377685546875 , L_2 = 4.923795700073242 , L_l = 0.5564486384391785\n",
      "L_e = 259.89801025390625 , L_2 = 4.923795700073242 , L_l = 0.5564486384391785\n",
      "epoch 66, EV = -1266.5305001598138, val  loss = 254.8961410522461 , train loss 4612.260949707032\n",
      "L_e = 3163.907958984375 , L_2 = 11.627696990966797 , L_l = 1.2748544216156006\n",
      "L_e = 3221.871337890625 , L_2 = 11.627696990966797 , L_l = 1.2748544216156006\n",
      "epoch 67, EV = -11475.559461436593, val  loss = 3205.792236328125 , train loss 4060.1893920898438\n",
      "L_e = 3212.78564453125 , L_2 = 18.372299194335938 , L_l = 1.7875789403915405\n",
      "L_e = 3115.78662109375 , L_2 = 18.372299194335938 , L_l = 1.7875789403915405\n",
      "epoch 68, EV = -12595.680534715835, val  loss = 3184.446044921875 , train loss 6207.690576171875\n",
      "L_e = 786.5780029296875 , L_2 = 10.021382331848145 , L_l = 1.7384175062179565\n",
      "L_e = 749.6559448242188 , L_2 = 10.021382331848145 , L_l = 1.7384175062179565\n",
      "epoch 69, EV = -1948.0391105241501, val  loss = 779.8767395019531 , train loss 1008.3489074707031\n",
      "L_e = 216.7732391357422 , L_2 = 6.807808876037598 , L_l = 1.543444037437439\n",
      "L_e = 169.6330108642578 , L_2 = 6.807808876037598 , L_l = 1.543444037437439\n",
      "epoch 70, EV = -219.7513558451946, val  loss = 201.55438232421875 , train loss 554.7794677734375\n",
      "L_e = 177.9906463623047 , L_2 = 6.187163829803467 , L_l = 1.2264790534973145\n",
      "L_e = 161.0659942626953 , L_2 = 6.187163829803467 , L_l = 1.2264790534973145\n",
      "epoch 71, EV = -234.36675642086908, val  loss = 176.9419708251953 , train loss 239.81741485595703\n",
      "L_e = 60.64190673828125 , L_2 = 3.730884075164795 , L_l = 0.8491120338439941\n",
      "L_e = 59.99119567871094 , L_2 = 3.730884075164795 , L_l = 0.8491120338439941\n",
      "epoch 72, EV = -72.54396975728181, val  loss = 64.89654541015625 , train loss 133.35639190673828\n",
      "L_e = 42.63958740234375 , L_2 = 2.8690733909606934 , L_l = 0.608330488204956\n",
      "L_e = 57.5166130065918 , L_2 = 2.8690733909606934 , L_l = 0.608330488204956\n",
      "epoch 73, EV = -33.504103694970794, val  loss = 53.55550193786621 , train loss 70.14765853881836\n",
      "L_e = 28.175106048583984 , L_2 = 2.0503110885620117 , L_l = 0.442875474691391\n",
      "L_e = 33.66501235961914 , L_2 = 2.0503110885620117 , L_l = 0.442875474691391\n",
      "epoch 74, EV = -4.482467772868963, val  loss = 33.413246154785156 , train loss 33.337796974182126\n",
      "L_e = 16.665260314941406 , L_2 = 1.561131238937378 , L_l = 0.34220775961875916\n",
      "L_e = 16.741682052612305 , L_2 = 1.561131238937378 , L_l = 0.34220775961875916\n",
      "epoch 75, EV = -3.2876999424054074, val  loss = 18.60680866241455 , train loss 22.94970121383667\n",
      "L_e = 8.490872383117676 , L_2 = 1.191868543624878 , L_l = 0.26986193656921387\n",
      "L_e = 7.955387115478516 , L_2 = 1.191868543624878 , L_l = 0.26986193656921387\n",
      "epoch 76, EV = -0.47573265777184415, val  loss = 9.684860706329346 , train loss 13.600130748748779\n",
      "L_e = 575.06396484375 , L_2 = 3.7787492275238037 , L_l = 0.5239496231079102\n",
      "L_e = 547.0663452148438 , L_2 = 3.7787492275238037 , L_l = 0.5239496231079102\n",
      "epoch 77, EV = -660.7641831544729, val  loss = 565.3678283691406 , train loss 1467.9081340312957\n",
      "L_e = 879.6978149414062 , L_2 = 2.5966577529907227 , L_l = 0.44552475214004517\n",
      "L_e = 867.4014282226562 , L_2 = 2.5966577529907227 , L_l = 0.44552475214004517\n",
      "epoch 78, EV = -876.4002527777965, val  loss = 876.591796875 , train loss 2477.002850341797\n",
      "L_e = 169.67237854003906 , L_2 = 2.4731125831604004 , L_l = 0.4480893611907959\n",
      "L_e = 158.59030151367188 , L_2 = 2.4731125831604004 , L_l = 0.4480893611907959\n",
      "epoch 79, EV = -618.5899623918992, val  loss = 167.05254364013672 , train loss 381.6022125244141\n",
      "L_e = 22.259822845458984 , L_2 = 1.589086890220642 , L_l = 0.4501058757305145\n",
      "L_e = 19.7918643951416 , L_2 = 1.589086890220642 , L_l = 0.4501058757305145\n",
      "epoch 80, EV = -21.52031119282429, val  loss = 23.065035820007324 , train loss 151.83279190063476\n",
      "L_e = 4261.75634765625 , L_2 = 3.1909141540527344 , L_l = 0.5315653681755066\n",
      "L_e = 4094.76806640625 , L_2 = 3.1909141540527344 , L_l = 0.5315653681755066\n",
      "epoch 81, EV = -26904.927977259344, val  loss = 4181.98486328125 , train loss 129.89154262542723\n",
      "L_e = 678.3285522460938 , L_2 = 2.699673891067505 , L_l = 0.5439262390136719\n",
      "L_e = 615.8605346679688 , L_2 = 2.699673891067505 , L_l = 0.5439262390136719\n",
      "epoch 82, EV = -3457.1561019913506, val  loss = 650.338134765625 , train loss 1072.2272384643554\n",
      "L_e = 1659.55810546875 , L_2 = 3.7127161026000977 , L_l = 0.7764054536819458\n",
      "L_e = 1471.7098388671875 , L_2 = 3.7127161026000977 , L_l = 0.7764054536819458\n",
      "epoch 83, EV = -9878.257182339063, val  loss = 1570.1231079101562 , train loss 1057.7847595214844\n",
      "L_e = 793.8411254882812 , L_2 = 3.6589128971099854 , L_l = 1.0031182765960693\n",
      "L_e = 654.4984130859375 , L_2 = 3.6589128971099854 , L_l = 1.0031182765960693\n",
      "epoch 84, EV = -4214.864460909595, val  loss = 728.8318176269531 , train loss 679.3651351928711\n",
      "L_e = 813.1751098632812 , L_2 = 5.380547046661377 , L_l = 1.0552453994750977\n",
      "L_e = 817.0010986328125 , L_2 = 5.380547046661377 , L_l = 1.0552453994750977\n",
      "epoch 85, EV = -1880.2585451350762, val  loss = 821.5238952636719 , train loss 2192.7197326660157\n",
      "L_e = 428.3121643066406 , L_2 = 3.439955711364746 , L_l = 1.021858811378479\n",
      "L_e = 482.7353515625 , L_2 = 3.439955711364746 , L_l = 1.021858811378479\n",
      "epoch 86, EV = -1325.2522058165991, val  loss = 459.9855499267578 , train loss 488.3927200317383\n",
      "L_e = 219.44554138183594 , L_2 = 2.9591972827911377 , L_l = 0.8056541681289673\n",
      "L_e = 194.69985961914062 , L_2 = 2.9591972827911377 , L_l = 0.8056541681289673\n",
      "epoch 87, EV = -397.480108872056, val  loss = 210.8375473022461 , train loss 272.94181671142576\n",
      "L_e = 38.27873992919922 , L_2 = 1.8840620517730713 , L_l = 0.647648811340332\n",
      "L_e = 41.53649139404297 , L_2 = 1.8840620517730713 , L_l = 0.647648811340332\n",
      "epoch 88, EV = -37.74137098170244, val  loss = 42.439327239990234 , train loss 67.01651268005371\n",
      "L_e = 18.75057601928711 , L_2 = 1.3122150897979736 , L_l = 0.5076957941055298\n",
      "L_e = 19.02979850769043 , L_2 = 1.3122150897979736 , L_l = 0.5076957941055298\n",
      "epoch 89, EV = -7.167679413006856, val  loss = 20.71009922027588 , train loss 34.71790409088135\n",
      "L_e = 8.667085647583008 , L_2 = 0.9419957399368286 , L_l = 0.30313989520072937\n",
      "L_e = 9.71929931640625 , L_2 = 0.9419957399368286 , L_l = 0.30313989520072937\n",
      "epoch 90, EV = -1.1942450862664442, val  loss = 10.43832778930664 , train loss 14.48423366546631\n",
      "L_e = 1265.250732421875 , L_2 = 5.413059234619141 , L_l = 0.9049935340881348\n",
      "L_e = 1363.423828125 , L_2 = 5.413059234619141 , L_l = 0.9049935340881348\n",
      "epoch 91, EV = -10943.147533401847, val  loss = 1320.6553955078125 , train loss 3510.179790496826\n",
      "L_e = 3161.5244140625 , L_2 = 5.5899529457092285 , L_l = 0.9920743107795715\n",
      "L_e = 3388.12353515625 , L_2 = 5.5899529457092285 , L_l = 0.9920743107795715\n",
      "epoch 92, EV = -27710.29982793675, val  loss = 3281.406005859375 , train loss 3562.6115478515626\n",
      "L_e = 432.5256042480469 , L_2 = 3.5077948570251465 , L_l = 1.111897349357605\n",
      "L_e = 443.74591064453125 , L_2 = 3.5077948570251465 , L_l = 1.111897349357605\n",
      "epoch 93, EV = -3752.0557922652133, val  loss = 442.7554473876953 , train loss 401.86314086914064\n",
      "L_e = 136.90408325195312 , L_2 = 2.5822184085845947 , L_l = 0.9304847121238708\n",
      "L_e = 282.55816650390625 , L_2 = 2.5822184085845947 , L_l = 0.9304847121238708\n",
      "epoch 94, EV = -823.5578130552402, val  loss = 213.2438201904297 , train loss 158.28432922363282\n",
      "L_e = 41.51816940307617 , L_2 = 1.5252436399459839 , L_l = 0.6878161430358887\n",
      "L_e = 36.35114669799805 , L_2 = 1.5252436399459839 , L_l = 0.6878161430358887\n",
      "epoch 95, EV = -37.54364458528849, val  loss = 41.1477165222168 , train loss 60.68717803955078\n",
      "L_e = 14.284537315368652 , L_2 = 1.155012845993042 , L_l = 0.5312827229499817\n",
      "L_e = 14.582948684692383 , L_2 = 1.155012845993042 , L_l = 0.5312827229499817\n",
      "epoch 96, EV = -1.9777320359761899, val  loss = 16.120038986206055 , train loss 22.67646598815918\n",
      "L_e = 3734.123779296875 , L_2 = 5.723667621612549 , L_l = 0.5717326402664185\n",
      "L_e = 3641.103759765625 , L_2 = 5.723667621612549 , L_l = 0.5717326402664185\n",
      "epoch 97, EV = -14626.628219039394, val  loss = 3693.9091796875 , train loss 5465.915051269531\n",
      "L_e = 2954.921142578125 , L_2 = 5.980841636657715 , L_l = 0.6292731165885925\n",
      "L_e = 2943.4248046875 , L_2 = 5.980841636657715 , L_l = 0.6292731165885925\n",
      "epoch 98, EV = -3353.6355701375464, val  loss = 2955.7833251953125 , train loss 5738.358831787109\n",
      "L_e = 892.280029296875 , L_2 = 3.5535922050476074 , L_l = 0.6740550398826599\n",
      "L_e = 888.1458129882812 , L_2 = 3.5535922050476074 , L_l = 0.6740550398826599\n",
      "epoch 99, EV = -13896.872740877363, val  loss = 894.4405822753906 , train loss 1440.5639434814452\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in tqdm_notebook(range(epoches)):\n",
    "    losses_train += train_model(encoder,optimizer)\n",
    "    ev,loss = validate_model(encoder)\n",
    "    EVs.append(ev)\n",
    "    losses_val.append(loss)\n",
    "    print(f'epoch {epoch}, EV = {ev}, val  loss = {loss} , train loss {sum(losses_train[-10:])/10}')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.4156657010316849\n"
     ]
    }
   ],
   "source": [
    "print(max(EVs))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
