{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "import models\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from PIL import Image\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "device = 'cuda:0' # device where you put your data and models\n",
    "data_path = './' # the path of the 'npc_v4_data.h5' file\n",
    "batch_size = 16 # the batch size of the data loader\n",
    "insp_layer = 'conv3' # the middle layer extracted from alexnet, available in {'conv1', 'conv2', 'conv3', 'conv4', 'conv5'}\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "root_dir = 'C:/Users/admin/Desktop/pytorch_ovo/data/Celldata_S18_155_L76_211203/stimuli/0_presented_images_800/'\n",
    "resolution = 300\n",
    "image_path = os.listdir(root_dir)\n",
    "path_dict = {}\n",
    "for j in image_path:\n",
    "    key = int(j.split('_')[0])  # 刺激呈现的顺序是图像名称下划线前面的数字顺序。\n",
    "    path_dict[key] = j\n",
    "\n",
    "stim_arr = np.zeros((len(image_path), resolution, resolution, 3))\n",
    "# stim_arr_gray3 = np.zeros((len(image_path), resolution, resolution, 3))\n",
    "for i in range(len(image_path)):\n",
    "    img_bgr = cv2.imread(os.path.join(root_dir, path_dict[i+1]))\n",
    "    stim_arr[i] = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "stim_arr = stim_arr.astype('float32')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4  15  16  18  19  40  42  54  61  78 120 124 126 133 142 146 161 165\n",
      " 166 173 174 178 212 215 222 224 262 278 285 293 305 311 336 351 352 364\n",
      " 368 369 393 400 410 412 420 432 442 463 473 483 484 502 509 510 534 541\n",
      " 552 579 582 599 604 605 610 615 620 622 633 637 672 675 681 683 692 698\n",
      " 701 705 711 722 723 727 787 788] [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79] (800, 299, 299, 3)\n"
     ]
    }
   ],
   "source": [
    "id = h5py.File('C:/Users/admin/Desktop/pytorch_ovo/data/Celldata_S18_155_L76_211203/Random_id_80_2021_12_22.mat', 'r')\n",
    "\n",
    "images_n  = np.zeros(shape=(stim_arr.shape[0], 299, 299, 3))\n",
    "for i in range(stim_arr.shape[0]):\n",
    "    images_n[i] = cv2.resize(stim_arr[i], (299, 299))\n",
    "\n",
    "idx = np.array(id['sampleidlist21']).squeeze().astype('int') - 1\n",
    "idx, unique_idx = np.unique(idx, return_index=True)\n",
    "print(idx, unique_idx, images_n.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 880, 114)\n"
     ]
    }
   ],
   "source": [
    "mat_file = h5py.File('C:/Users/admin/Desktop/pytorch_ovo/data/Celldata_S18_155_L76_211203/celldataS_Natural_objects_800_80_50.mat', 'r')\n",
    "#[num_repetitions, num_images, num_neurons]\n",
    "neural_n = np.transpose(np.array(mat_file['celldataS']), (1, 2, 0)).astype('float16')\n",
    "neural_n = neural_n[:,:880, :]\n",
    "print(neural_n.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800 114 (800, 299, 299, 3)\n"
     ]
    }
   ],
   "source": [
    "n_images = 800\n",
    "n_neurons = neural_n.shape[2]\n",
    "size_imags = images_n.shape[0]\n",
    "print(n_images, n_neurons, images_n.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(880, 12, 114)\n",
      "(720, 114)\n",
      "(80, 114)\n"
     ]
    }
   ],
   "source": [
    "reps = neural_n.shape[0] # trials\n",
    "rand_ind = np.arange(reps)\n",
    "np.random.shuffle(rand_ind)\n",
    "\n",
    "data_y_train = np.concatenate((np.delete(neural_n[:, :800, :], idx, 1), neural_n[:, 880:, :]), 1).mean(0)\n",
    "temp = np.transpose(neural_n, (1, 0, 2))\n",
    "print(temp.shape)\n",
    "data_y_val = np.concatenate((temp[idx], temp[800:880][unique_idx]), 1)\n",
    "data_y_val = np.transpose(data_y_val, (1, 0, 2))\n",
    "data_y_val = np.mean(data_y_val, 0)\n",
    "print(data_y_train.shape)\n",
    "print(data_y_val.shape)\n",
    "\n",
    "#\n",
    "# data_x = images_n[:, np.newaxis].astype(np.float16)\n",
    "# print('images_n', images_n.shape)\n",
    "# data_x = data_x / 255 # (640, 1, 299, 299)\n",
    "# data_x = np.tile(data_x, [1, 3, 1, 1])\n",
    "# print('data_x', data_x.shape)\n",
    "# data_x_train = data_x[:576]\n",
    "# data_x_val = data_x[576:]as indices must be"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 299, 299, 3)\n",
      "(800, 299, 299, 3)\n",
      "(800, 3, 299, 299) (720, 3, 299, 299) (80, 3, 299, 299)\n"
     ]
    }
   ],
   "source": [
    "print(images_n.shape)\n",
    "#data_x = images_n[:, np.newaxis].astype(np.float16)\n",
    "data_x = images_n.astype(np.float16)\n",
    "print(data_x.shape)\n",
    "data_x = data_x / 255 # (800, 1, 299, 299)\n",
    "\n",
    "#data_x = np.tile(data_x, [1, 3, 1, 1])\n",
    "data_x_train = np.delete(images_n, idx, 0)\n",
    "data_x_val = images_n[idx]\n",
    "\n",
    "data_x = np.transpose(data_x, (0, 3, 1, 2))\n",
    "data_x_train = np.transpose(data_x_train, (0, 3, 1, 2))\n",
    "data_x_val = np.transpose(data_x_val, (0, 3, 1, 2))\n",
    "print(data_x.shape, data_x_train.shape, data_x_val.shape)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val: (80, 3, 299, 299), (80, 114)\n"
     ]
    }
   ],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_x, data_y):\n",
    "        self.data_x = data_x\n",
    "        self.data_y = data_y\n",
    "    def __getitem__(self, index):\n",
    "        return index, self.data_x[index], self.data_y[index]\n",
    "    def __len__(self):\n",
    "        return self.data_x.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "dataset_train = Dataset(data_x_train, data_y_train)\n",
    "dataset_val = Dataset(data_x_val, data_y_val)\n",
    "\n",
    "loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle = True)\n",
    "loader_val = torch.utils.data.DataLoader(dataset_val, batch_size=batch_size, shuffle = True)\n",
    "\n",
    "print(f'val: {data_x_val.shape}, {data_y_val.shape}')\n",
    "# for i,(x,y) in enumerate(loader_val):\n",
    "#     print(i, x.shape, y.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([1, 3, 299, 299])\n",
      "fmap:  torch.Size([1, 384, 17, 17])\n",
      "size:  torch.Size([17, 17])\n",
      "114 torch.Size([17, 17])\n",
      "torch.Size([114, 17, 17])\n"
     ]
    }
   ],
   "source": [
    "# CHOOSE THE AUGMENTS IF NECESSARY\n",
    "lamd_s, lamd_d = [5e-3, 2e-3] # the coefficients of the losses. Try other coefficients!\n",
    "epoches = 10 # total epochs for training the encoder\n",
    "lr = 1e-3 # the learing rate for training the encoder\n",
    "\n",
    "alexnet = models.alexnet(pretrained=True)\n",
    "\n",
    "#\n",
    "alexnet.to(device)\n",
    "alexnet.eval()\n",
    "for param in alexnet.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "x = torch.from_numpy(data_x[0:1]).to(device)\n",
    "print(\"x:\", x.shape)\n",
    "x = x.float()\n",
    "fmap = alexnet(x, layer=insp_layer)\n",
    "\n",
    "neurons = data_y_train.shape[1]\n",
    "sizes = fmap.shape[2:]\n",
    "print(\"fmap: \", fmap.shape)\n",
    "print(\"size: \", sizes)\n",
    "channels = fmap.shape[1]\n",
    "print(neurons, sizes)\n",
    "w_s = nn.Parameter(torch.randn(size=(neurons,) + sizes))\n",
    "print(w_s.shape)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 384, 17, 17])\n",
      "torch.Size([800, 384, 17, 17])\n"
     ]
    }
   ],
   "source": [
    "x = torch.from_numpy(data_x[0:1]).float().to(device)\n",
    "fmap = alexnet(x, layer=insp_layer)\n",
    "print(fmap.shape)\n",
    "sizes = fmap.shape[2:]\n",
    "\n",
    "imagenet_mean = torch.tensor([0.485, 0.456, 0.406], dtype=torch.float32).view(1, 3, 1, 1).to(device)\n",
    "imagenet_std = torch.tensor([0.229, 0.224, 0.225], dtype=torch.float32).view(1, 3, 1, 1).to(device)\n",
    "transform = lambda x : (x - imagenet_mean) / imagenet_std\n",
    "#data_x = transform(data_x)\n",
    "feature_map = torch.Tensor(n_images, fmap.shape[1], fmap.shape[2], fmap.shape[3])\n",
    "feature_map.to(device)\n",
    "print(feature_map.shape)\n",
    "for i in range(n_images):\n",
    "    x = torch.from_numpy(data_x[i:i + 1]).float().to(device)\n",
    "    x = transform(x)\n",
    "    fmap = alexnet(x, layer = insp_layer)\n",
    "    feature_map[i] = fmap"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "mse_weight = 1.0\n",
    "l1_weight = 0\n",
    "spa_weight = 1e-1\n",
    "ch_weight = 1e-1\n",
    "lap_weight = 1e-1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def mse_loss(prediction, response, weight=None):\n",
    "    if weight is None:\n",
    "        mse_loss = torch.mean(torch.mean((prediction - response)**2, dim=1))\n",
    "    else:\n",
    "        mse_loss = torch.sum(weight*torch.mean((prediction - response)**2, dim=1))\n",
    "    return mse_loss\n",
    "\n",
    "def l2_norm_regularizer(W):\n",
    "    with torch.autograd.profiler.record_function('l2_norm'):\n",
    "        penalty = torch.mean(torch.sum(W**2))\n",
    "        return penalty\n",
    "\n",
    "def l1_norm_regularizer(W):\n",
    "    with torch.autograd.profiler.record_function('l1_norm'):\n",
    "        penalty = torch.mean(torch.sum(torch.abs(W)))\n",
    "        return penalty\n",
    "\n",
    "def smoothness_regularizer_2d(W_s):\n",
    "    K = torch.tensor([\n",
    "    [0,-1,0],\n",
    "    [-1,4,-1],\n",
    "    [0,-1,0]],dtype=torch.float).to(device)\n",
    "    return torch.sum(F.conv2d(torch.unsqueeze(W_s,1),K.unsqueeze(0).unsqueeze(0))**2)\n",
    "\n",
    "def pearson_corr(prediction, response):\n",
    "    prediction = torch.tensor(prediction)\n",
    "    response = torch.tensor(response)\n",
    "    prediction = torch.transpose(prediction, 1, 0)\n",
    "    response = torch.transpose(response, 1, 0)\n",
    "\n",
    "    prediction_mean = torch.mean(prediction, dim=0)\n",
    "    response_mean = torch.mean(response, dim=0)\n",
    "\n",
    "    num = torch.sum((prediction - prediction_mean)*(response - response_mean), dim=0)\n",
    "    den = torch.sqrt(torch.sum((prediction - prediction_mean)**2, dim=0) *\n",
    "                     torch.sum((response - response_mean)**2, dim=0))\n",
    "    pcc = torch.mean(num * (1 / den))\n",
    "    return pcc\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class conv_encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, neurons, sizes, channels):\n",
    "        super(conv_encoder, self).__init__()\n",
    "        # PUT YOUR CODES HERE\n",
    "        self.W_s = nn.Parameter(torch.randn(size=(neurons,) + sizes))\n",
    "        self.W_d = nn.Parameter(torch.randn(size = (neurons,channels,1,1)))\n",
    "        self.W_b = nn.Parameter(torch.randn(size = (1,neurons)))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # PUT YOUR CODES HERE\n",
    "        out = torch.einsum('bchw , nhw -> bnchw',x,self.W_s) # dimension : N,n,C,h,w\n",
    "        out = torch.stack(\n",
    "            [F.conv2d(out[:,n,:,:,:],torch.unsqueeze(self.W_d[n],0)) for n in range(neurons)],dim=1)\n",
    "            #dimension:N,n,1,h,w\n",
    "        out = torch.sum(out,dim=(2,3,4))\n",
    "        out = out + self.W_b\n",
    "        return out\n",
    "\n",
    "def Loss(y, pred, W_s, W_d):\n",
    "    return mse_loss(y, pred) * mse_weight + \\\n",
    "          l2_norm_regularizer(W_s) * spa_weight + \\\n",
    "          smoothness_regularizer_2d(W_s) * lap_weight + \\\n",
    "          l2_norm_regularizer(W_d) * ch_weight\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#encoder = conv_encoder(neurons, sizes, channels).to(device)\n",
    "encoder = conv_encoder(neurons, sizes, channels).to(device)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\n    You need to define the conv_encoder() class and train the encoder.\\n    The code of alexnet has been slightly modified from the torchvision, for convenience\\n    of extracting the middle layers.\\n\\n    Example:\\n        >>> x = x.to(device) # x is a batch of images\\n        >>> x = transform(x)\\n        >>> fmap = alexnet(x, layer=insp_layer)\\n        >>> out= encoder(fmap)\\n        >>> ...\\n'"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_model(encoder, optimizer):\n",
    "    losses = []\n",
    "    encoder.train()\n",
    "    for i,(z, x,y) in enumerate(loader_train):\n",
    "        optimizer.zero_grad()\n",
    "        x = x.float().to(device)\n",
    "        y = y.float().to(device)\n",
    "        z = z.to(device)\n",
    "        #x = transform(x)\n",
    "        fmap = feature_map[z - 1].to(device)\n",
    "        out = encoder(fmap)\n",
    "#         print(f'L_e = {l_e} , L_2 = {l_2} , L_l = {l_l}')\n",
    "        loss = Loss(y, out, encoder.W_s, encoder.W_d)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "#         print(f'iteration {i}, train loss: {losses[-1]}')\n",
    "\n",
    "    return losses\n",
    "\n",
    "def validate_model(encoder):\n",
    "    encoder.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    losses = []\n",
    "    for i,(z, x,y) in enumerate(loader_val):\n",
    "        x = x.float().to(device)\n",
    "        y = y.float().to(device)\n",
    "        z = z.to(device)\n",
    "        #x = transform(x)\n",
    "        fmap = feature_map[z - 1].to(device)\n",
    "        out = encoder(fmap)\n",
    "        y_pred.append(out)\n",
    "        y_true.append(y)\n",
    "        loss = Loss(y, out, encoder.W_s, encoder.W_d)\n",
    "        losses.append(loss.item())\n",
    "    y_pred = torch.cat(y_pred)\n",
    "    y_true = torch.cat(y_true)\n",
    "    explained_variance = metrics.explained_variance_score(y_true = y_true.detach().cpu().numpy(),y_pred = y_pred.detach().cpu().numpy())\n",
    "    pcc = pearson_corr(y_pred.detach().cpu().numpy(), y_true.detach().cpu().numpy())\n",
    "    return pcc, explained_variance,sum(losses)/len(losses)\n",
    "    #return explained_variance,sum(losses)/len(losses)\n",
    "\n",
    "\"\"\"\n",
    "    You need to define the conv_encoder() class and train the encoder.\n",
    "    The code of alexnet has been slightly modified from the torchvision, for convenience\n",
    "    of extracting the middle layers.\n",
    "\n",
    "    Example:\n",
    "        >>> x = x.to(device) # x is a batch of images\n",
    "        >>> x = transform(x)\n",
    "        >>> fmap = alexnet(x, layer=insp_layer)\n",
    "        >>> out= encoder(fmap)\n",
    "        >>> ...\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# losses_train = []\n",
    "# losses_val = []\n",
    "# EVs = []\n",
    "\n",
    "losses_train = []\n",
    "losses_val = []\n",
    "EVs = []\n",
    "pccs = []"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "optimizer = torch.optim.Adam(encoder.parameters(), lr=lr)\n",
    "#optimizer = torch.optim.SGD(encoder.parameters(), lr=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=200, gamma=0.1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\envs\\neural_control\\lib\\site-packages\\ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "937cafc18e4447cda5b8010222d28bd8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, EV = -68021214.69298245, val  loss = 1938405.7 , train loss 2876512.2375, pcc = -0.012109366245567799\n",
      "epoch 1, EV = -37110294.47807018, val  loss = 1081631.425 , train loss 1292983.5875, pcc = -0.006641934160143137\n",
      "epoch 2, EV = -24476697.83991228, val  loss = 731651.75 , train loss 951057.5625, pcc = -0.016134267672896385\n",
      "epoch 3, EV = -17759914.660635963, val  loss = 547073.4875 , train loss 679051.44375, pcc = -0.010018477216362953\n",
      "epoch 4, EV = -13688889.10142544, val  loss = 433820.88125 , train loss 572178.309375, pcc = -0.008877193555235863\n",
      "epoch 5, EV = -10842827.43859649, val  loss = 354936.6125 , train loss 425528.15, pcc = -0.007066414691507816\n",
      "epoch 6, EV = -8910490.695723685, val  loss = 300928.625 , train loss 360246.15, pcc = -0.010911677964031696\n",
      "epoch 7, EV = -7503540.012883772, val  loss = 259578.790625 , train loss 309205.296875, pcc = -0.006256703287363052\n",
      "epoch 8, EV = -6420700.312774123, val  loss = 229492.134375 , train loss 277264.5890625, pcc = -0.011531857773661613\n",
      "epoch 9, EV = -5587002.103070175, val  loss = 205052.2625 , train loss 240928.7953125, pcc = -0.013617058284580708\n",
      "epoch 10, EV = -4910702.39651864, val  loss = 185354.34375 , train loss 217624.290625, pcc = -0.01635078899562359\n",
      "epoch 11, EV = -4311795.6957236845, val  loss = 168268.534375 , train loss 201826.21875, pcc = -0.013069321401417255\n",
      "epoch 12, EV = -3886967.6517269737, val  loss = 154528.9625 , train loss 179761.5546875, pcc = -0.010505910031497478\n",
      "epoch 13, EV = -3470200.607044956, val  loss = 143005.0625 , train loss 160471.4859375, pcc = -0.013162830844521523\n",
      "epoch 14, EV = -3169546.7310855263, val  loss = 133313.853125 , train loss 157229.240625, pcc = -0.014728750102221966\n",
      "epoch 15, EV = -2867839.6120476974, val  loss = 124010.309375 , train loss 142600.05234375, pcc = -0.014879387803375721\n",
      "epoch 16, EV = -2620557.12013432, val  loss = 116303.78125 , train loss 131329.2171875, pcc = -0.01407670509070158\n",
      "epoch 17, EV = -2410129.8898711624, val  loss = 109480.1453125 , train loss 123953.35078125, pcc = -0.012598742730915546\n",
      "epoch 18, EV = -2219457.293654057, val  loss = 103226.36875 , train loss 116645.89921875, pcc = -0.013587405905127525\n",
      "epoch 19, EV = -2045723.9631990131, val  loss = 97775.7703125 , train loss 109386.8296875, pcc = -0.014552285894751549\n",
      "epoch 20, EV = -1907901.7121710526, val  loss = 92826.278125 , train loss 100310.03359375, pcc = -0.013346577063202858\n",
      "epoch 21, EV = -1770309.1687225876, val  loss = 88272.015625 , train loss 99754.45, pcc = -0.017747066915035248\n",
      "epoch 22, EV = -1651098.4597039474, val  loss = 84114.5984375 , train loss 91717.52734375, pcc = -0.010468842461705208\n",
      "epoch 23, EV = -1531887.8083538925, val  loss = 79982.4640625 , train loss 89384.021875, pcc = -0.013657023198902607\n",
      "epoch 24, EV = -1448766.9742324562, val  loss = 76629.8921875 , train loss 83104.62421875, pcc = -0.014882941730320454\n",
      "epoch 25, EV = -1349958.5390282345, val  loss = 73283.409375 , train loss 79581.2296875, pcc = -0.00881425105035305\n",
      "epoch 26, EV = -1264783.0241913376, val  loss = 70083.50078125 , train loss 76017.97109375, pcc = -0.014088399708271027\n",
      "epoch 27, EV = -1191310.0901864036, val  loss = 67172.715625 , train loss 73240.0734375, pcc = -0.01406419463455677\n",
      "epoch 28, EV = -1116669.6671121162, val  loss = 64385.3859375 , train loss 69187.978125, pcc = -0.01238090917468071\n",
      "epoch 29, EV = -1055812.928419682, val  loss = 61895.846875 , train loss 66978.988671875, pcc = -0.012402312830090523\n",
      "epoch 30, EV = -991635.6195175438, val  loss = 59384.31796875 , train loss 63145.491015625, pcc = -0.011256756260991096\n",
      "epoch 31, EV = -942561.9794750548, val  loss = 57024.759375 , train loss 61286.28515625, pcc = -0.011372667737305164\n",
      "epoch 32, EV = -892463.7072711075, val  loss = 54953.09375 , train loss 59655.34296875, pcc = -0.01565810851752758\n",
      "epoch 33, EV = -841245.8154468202, val  loss = 52899.46796875 , train loss 56595.383984375, pcc = -0.012782650999724865\n",
      "epoch 34, EV = -792313.1272957785, val  loss = 50810.8875 , train loss 54803.123046875, pcc = -0.009017175994813442\n",
      "epoch 35, EV = -753829.1992701481, val  loss = 48969.1875 , train loss 51129.015625, pcc = -0.011699296534061432\n",
      "epoch 36, EV = -712005.6912349232, val  loss = 47212.5125 , train loss 49705.026953125, pcc = -0.012946625240147114\n",
      "epoch 37, EV = -674787.6033271656, val  loss = 45391.1859375 , train loss 48653.046484375, pcc = -0.013519024476408958\n",
      "epoch 38, EV = -642757.4688356634, val  loss = 43810.32890625 , train loss 46550.714453125, pcc = -0.014317587018013\n",
      "epoch 39, EV = -609892.8856222588, val  loss = 42238.92890625 , train loss 45477.9859375, pcc = -0.010198012925684452\n",
      "epoch 40, EV = -577998.4186711897, val  loss = 40698.92890625 , train loss 43439.858203125, pcc = -0.00871628150343895\n",
      "epoch 41, EV = -547150.7356599506, val  loss = 39251.5078125 , train loss 41045.10078125, pcc = -0.01094021089375019\n",
      "epoch 42, EV = -520312.08646861295, val  loss = 37834.32109375 , train loss 39644.673046875, pcc = -0.009164203889667988\n",
      "epoch 43, EV = -498369.91123560857, val  loss = 36531.046875 , train loss 38487.23359375, pcc = -0.011236732825636864\n",
      "epoch 44, EV = -470444.0363298383, val  loss = 35221.3640625 , train loss 37363.40234375, pcc = -0.00822153128683567\n",
      "epoch 45, EV = -450466.75932874175, val  loss = 33997.9953125 , train loss 35505.823046875, pcc = -0.013178381137549877\n",
      "epoch 46, EV = -430474.7961211623, val  loss = 32849.66171875 , train loss 34149.798046875, pcc = -0.01224285177886486\n",
      "epoch 47, EV = -406025.80824253015, val  loss = 31623.6640625 , train loss 33375.880078125, pcc = -0.010957482270896435\n",
      "epoch 48, EV = -386890.4882384183, val  loss = 30525.62421875 , train loss 31923.6677734375, pcc = -0.008080825209617615\n",
      "epoch 49, EV = -368083.05192057294, val  loss = 29430.404296875 , train loss 30507.0767578125, pcc = -0.01171092502772808\n",
      "epoch 50, EV = -353088.71392715187, val  loss = 28442.68671875 , train loss 29840.3201171875, pcc = -0.0047217803075909615\n",
      "epoch 51, EV = -338001.3323739035, val  loss = 27486.2109375 , train loss 28702.66015625, pcc = -0.008276533335447311\n",
      "epoch 52, EV = -321506.5604783443, val  loss = 26518.553515625 , train loss 27387.4884765625, pcc = -0.008378813043236732\n",
      "epoch 53, EV = -306283.6861122533, val  loss = 25587.808203125 , train loss 26324.783203125, pcc = -0.010814519599080086\n",
      "epoch 54, EV = -293680.819575795, val  loss = 24742.86484375 , train loss 25657.6357421875, pcc = -0.011426305398344994\n",
      "epoch 55, EV = -277329.1318787692, val  loss = 23809.229296875 , train loss 24733.8486328125, pcc = -0.0057366155087947845\n",
      "epoch 56, EV = -267129.3970711691, val  loss = 23021.025 , train loss 23928.215234375, pcc = -0.008792072534561157\n",
      "epoch 57, EV = -253628.34449955454, val  loss = 22183.9515625 , train loss 23234.59453125, pcc = -0.011214577592909336\n",
      "epoch 58, EV = -241509.85749468888, val  loss = 21409.405859375 , train loss 22367.223046875, pcc = -0.010814715176820755\n",
      "epoch 59, EV = -231222.8250967996, val  loss = 20682.612890625 , train loss 21309.263671875, pcc = -0.0014570020139217377\n",
      "epoch 60, EV = -219985.7860342996, val  loss = 19952.310546875 , train loss 20668.4130859375, pcc = -0.008038281463086605\n",
      "epoch 61, EV = -210100.56642338267, val  loss = 19265.193359375 , train loss 19799.4716796875, pcc = -0.008202647790312767\n",
      "epoch 62, EV = -201595.83989514803, val  loss = 18591.833203125 , train loss 19226.0228515625, pcc = -0.007798413280397654\n",
      "epoch 63, EV = -191405.1184124863, val  loss = 17963.908203125 , train loss 18590.74921875, pcc = -0.0059608640149235725\n",
      "epoch 64, EV = -182543.20978532758, val  loss = 17317.2265625 , train loss 17720.862890625, pcc = -0.008888485841453075\n",
      "epoch 65, EV = -175868.85377261514, val  loss = 16717.932421875 , train loss 17416.0962890625, pcc = -0.013134261593222618\n",
      "epoch 66, EV = -168156.87785473204, val  loss = 16137.9859375 , train loss 16584.22587890625, pcc = -0.006142421625554562\n",
      "epoch 67, EV = -159526.10491407962, val  loss = 15584.347265625 , train loss 15968.55439453125, pcc = -0.005863727070391178\n",
      "epoch 68, EV = -152551.26007829633, val  loss = 15044.3298828125 , train loss 15404.446875, pcc = -0.005745287984609604\n",
      "epoch 69, EV = -145087.3469281113, val  loss = 14517.1234375 , train loss 14804.244140625, pcc = -0.006647163070738316\n",
      "epoch 70, EV = -139079.0343595806, val  loss = 14011.2294921875 , train loss 14333.375390625, pcc = -0.010477601550519466\n",
      "epoch 71, EV = -132579.5913835492, val  loss = 13527.5943359375 , train loss 13925.56787109375, pcc = -0.008285817690193653\n",
      "epoch 72, EV = -126984.14093338816, val  loss = 13084.4740234375 , train loss 13373.7541015625, pcc = -0.00484791724011302\n",
      "epoch 73, EV = -121176.43978935375, val  loss = 12628.5921875 , train loss 12961.24580078125, pcc = -0.009682592935860157\n",
      "epoch 74, EV = -115095.4198940344, val  loss = 12185.1619140625 , train loss 12504.18232421875, pcc = -0.0036206848453730345\n",
      "epoch 75, EV = -109506.5611657929, val  loss = 11749.040234375 , train loss 11988.92890625, pcc = -0.0071329460479319096\n",
      "epoch 76, EV = -105233.98614394874, val  loss = 11366.65390625 , train loss 11544.62021484375, pcc = -0.007765364833176136\n",
      "epoch 77, EV = -101026.98261890077, val  loss = 10987.88671875 , train loss 11338.49208984375, pcc = -0.005433239974081516\n",
      "epoch 78, EV = -96142.53096409848, val  loss = 10615.05390625 , train loss 10927.8806640625, pcc = -0.006118507124483585\n",
      "epoch 79, EV = -91462.85962984855, val  loss = 10251.067578125 , train loss 10330.0, pcc = -0.0071886456571519375\n",
      "epoch 80, EV = -87485.72829075863, val  loss = 9904.7609375 , train loss 10155.92353515625, pcc = -0.004135253373533487\n",
      "epoch 81, EV = -83305.82441363418, val  loss = 9582.264453125 , train loss 9737.95517578125, pcc = -0.007000723388046026\n",
      "epoch 82, EV = -80595.63912482011, val  loss = 9291.6703125 , train loss 9453.701171875, pcc = -0.005812358111143112\n",
      "epoch 83, EV = -75918.75519227145, val  loss = 8950.0943359375 , train loss 9197.43837890625, pcc = 0.00010304469469701871\n",
      "epoch 84, EV = -73001.95227479098, val  loss = 8664.0205078125 , train loss 8832.211328125, pcc = -0.008513173088431358\n",
      "epoch 85, EV = -69703.50248102556, val  loss = 8383.4884765625 , train loss 8674.3603515625, pcc = -0.004418513737618923\n",
      "epoch 86, EV = -66218.86559844435, val  loss = 8111.993359375 , train loss 8304.606298828125, pcc = -0.0032672476954758167\n",
      "epoch 87, EV = -63295.874377869724, val  loss = 7844.2435546875 , train loss 7907.81103515625, pcc = -0.0035980616230517626\n",
      "epoch 88, EV = -60644.30060510468, val  loss = 7610.202734375 , train loss 7708.05927734375, pcc = -0.003340627998113632\n",
      "epoch 89, EV = -57865.804081967006, val  loss = 7352.68125 , train loss 7471.163232421875, pcc = -0.005039254669100046\n",
      "epoch 90, EV = -55544.62219345361, val  loss = 7124.1734375 , train loss 7305.00234375, pcc = -0.005245856940746307\n",
      "epoch 91, EV = -52609.36518190618, val  loss = 6890.3998046875 , train loss 6977.6296875, pcc = -0.007391403429210186\n",
      "epoch 92, EV = -50150.6077522144, val  loss = 6692.06806640625 , train loss 6857.053173828125, pcc = -0.005547617562115192\n",
      "epoch 93, EV = -48057.25204253615, val  loss = 6483.3498046875 , train loss 6577.6896484375, pcc = -0.0016789973014965653\n",
      "epoch 94, EV = -45921.479520563495, val  loss = 6284.88212890625 , train loss 6376.21162109375, pcc = -0.007385210134088993\n",
      "epoch 95, EV = -43844.11919844778, val  loss = 6101.01875 , train loss 6244.896728515625, pcc = -0.005231351591646671\n",
      "epoch 96, EV = -41679.52455005311, val  loss = 5915.12490234375 , train loss 6027.6673828125, pcc = -0.0023973356001079082\n",
      "epoch 97, EV = -39872.69068667763, val  loss = 5737.1375 , train loss 5818.227099609375, pcc = -0.004068627953529358\n",
      "epoch 98, EV = -37962.83022589432, val  loss = 5576.5677734375 , train loss 5686.09453125, pcc = 0.0033353723119944334\n",
      "epoch 99, EV = -36187.62641772889, val  loss = 5413.2263671875 , train loss 5495.47216796875, pcc = -0.0027665882371366024\n",
      "epoch 100, EV = -34738.880795662866, val  loss = 5262.46591796875 , train loss 5307.334423828125, pcc = -0.006964951753616333\n",
      "epoch 101, EV = -32982.27607967979, val  loss = 5106.0544921875 , train loss 5184.326708984375, pcc = -0.006826527416706085\n",
      "epoch 102, EV = -31508.671585618406, val  loss = 4968.7927734375 , train loss 5055.87568359375, pcc = -0.005196443758904934\n",
      "epoch 103, EV = -30114.459098949766, val  loss = 4832.34560546875 , train loss 4883.9392578125, pcc = -0.0014444369589909911\n",
      "epoch 104, EV = -28607.388552749366, val  loss = 4695.03330078125 , train loss 4764.172265625, pcc = -0.0024908571504056454\n",
      "epoch 105, EV = -27431.340620342053, val  loss = 4573.4875 , train loss 4616.88115234375, pcc = -0.005405266769230366\n",
      "epoch 106, EV = -26319.958828574734, val  loss = 4458.6173828125 , train loss 4495.8533203125, pcc = -0.008924711495637894\n",
      "epoch 107, EV = -25006.382949293704, val  loss = 4337.3849609375 , train loss 4378.71611328125, pcc = -0.00863108690828085\n",
      "epoch 108, EV = -23860.501702291924, val  loss = 4226.965625 , train loss 4295.2216796875, pcc = -0.001540209399536252\n",
      "epoch 109, EV = -22760.326176693565, val  loss = 4121.46708984375 , train loss 4154.434228515625, pcc = 0.0005795817705802619\n",
      "epoch 110, EV = -21735.389079579138, val  loss = 4018.4853515625 , train loss 4089.41142578125, pcc = -0.00103894027415663\n",
      "epoch 111, EV = -20713.13719016627, val  loss = 3920.914306640625 , train loss 3953.0067138671875, pcc = 0.004375441465526819\n",
      "epoch 112, EV = -19867.738496613085, val  loss = 3825.616259765625 , train loss 3888.0673095703123, pcc = 0.0007301039877347648\n",
      "epoch 113, EV = -18675.536397565877, val  loss = 3729.166650390625 , train loss 3760.618994140625, pcc = -0.0028216687496751547\n",
      "epoch 114, EV = -17973.472893430477, val  loss = 3640.864306640625 , train loss 3689.85732421875, pcc = -0.00661074835807085\n",
      "epoch 115, EV = -17030.3899716829, val  loss = 3551.83984375 , train loss 3595.3612060546875, pcc = -0.0013420537579804659\n",
      "epoch 116, EV = -16337.77386782462, val  loss = 3475.371240234375 , train loss 3488.926171875, pcc = 0.002569497562944889\n",
      "epoch 117, EV = -15499.986710130122, val  loss = 3394.201708984375 , train loss 3413.968994140625, pcc = -0.006286340765655041\n",
      "epoch 118, EV = -14736.886990731222, val  loss = 3317.1921875 , train loss 3345.9377685546874, pcc = -0.004803855903446674\n",
      "epoch 119, EV = -14193.226330807334, val  loss = 3248.44697265625 , train loss 3282.897705078125, pcc = -0.01072765327990055\n",
      "epoch 120, EV = -13528.695517021313, val  loss = 3176.995654296875 , train loss 3209.276416015625, pcc = 0.0028231744654476643\n",
      "epoch 121, EV = -12862.84987834462, val  loss = 3108.9666015625 , train loss 3144.157080078125, pcc = -0.0015193379949778318\n",
      "epoch 122, EV = -12325.73093019452, val  loss = 3044.78193359375 , train loss 3070.769140625, pcc = -0.001810312271118164\n",
      "epoch 123, EV = -11792.521035311514, val  loss = 2980.871826171875 , train loss 2991.081689453125, pcc = 0.004264306277036667\n",
      "epoch 124, EV = -11117.971865135327, val  loss = 2918.69091796875 , train loss 2932.254150390625, pcc = -0.0015277840429916978\n",
      "epoch 125, EV = -10567.848366252163, val  loss = 2861.20888671875 , train loss 2885.2426025390623, pcc = 0.008004982955753803\n",
      "epoch 126, EV = -10187.819835997465, val  loss = 2802.032373046875 , train loss 2834.6671875, pcc = 0.0036295242607593536\n",
      "epoch 127, EV = -9735.743653949938, val  loss = 2751.996240234375 , train loss 2774.1171142578123, pcc = 0.003956840373575687\n",
      "epoch 128, EV = -9184.839175575658, val  loss = 2696.32353515625 , train loss 2723.585986328125, pcc = 0.004303298890590668\n",
      "epoch 129, EV = -8814.973488924796, val  loss = 2645.516015625 , train loss 2670.6718505859376, pcc = 0.006327517330646515\n",
      "epoch 130, EV = -8372.428966354906, val  loss = 2595.628662109375 , train loss 2614.080908203125, pcc = -0.0025896935258060694\n",
      "epoch 131, EV = -7924.6479257951705, val  loss = 2544.00703125 , train loss 2559.6644775390623, pcc = -0.004801650531589985\n",
      "epoch 132, EV = -7674.655055430898, val  loss = 2501.752734375 , train loss 2515.9184814453124, pcc = 0.005669859237968922\n",
      "epoch 133, EV = -7134.951126968652, val  loss = 2452.529150390625 , train loss 2478.9206787109374, pcc = 0.0017891634488478303\n",
      "epoch 134, EV = -6853.283940265053, val  loss = 2409.521923828125 , train loss 2421.9524658203127, pcc = 0.0030513114761561155\n",
      "epoch 135, EV = -6581.044085887441, val  loss = 2368.50654296875 , train loss 2373.031982421875, pcc = -0.0009884476894512773\n",
      "epoch 136, EV = -6230.424741544221, val  loss = 2325.630419921875 , train loss 2332.8171630859374, pcc = 0.0005549822235479951\n",
      "epoch 137, EV = -6027.635458260252, val  loss = 2284.991845703125 , train loss 2304.222119140625, pcc = -0.0019190229941159487\n",
      "epoch 138, EV = -5645.578012934902, val  loss = 2245.018896484375 , train loss 2260.8705078125, pcc = -0.0010031354613602161\n",
      "epoch 139, EV = -5368.602411939387, val  loss = 2206.7978515625 , train loss 2210.962060546875, pcc = 0.009861961007118225\n",
      "epoch 140, EV = -5088.01082611084, val  loss = 2168.645751953125 , train loss 2186.90703125, pcc = -0.001254692324437201\n",
      "epoch 141, EV = -4868.707114671406, val  loss = 2132.296923828125 , train loss 2142.912060546875, pcc = 0.005640266928821802\n",
      "epoch 142, EV = -4673.807728148343, val  loss = 2097.8015625 , train loss 2109.8956298828125, pcc = 0.000549929856788367\n",
      "epoch 143, EV = -4400.487054624055, val  loss = 2062.64365234375 , train loss 2071.019091796875, pcc = 0.011154201813042164\n",
      "epoch 144, EV = -4200.040699038589, val  loss = 2030.1203369140626 , train loss 2039.7091796875, pcc = -4.575401544570923e-05\n",
      "epoch 145, EV = -3985.946124813013, val  loss = 1994.2715576171875 , train loss 2000.9668090820312, pcc = 0.0005608163774013519\n",
      "epoch 146, EV = -3798.557332758318, val  loss = 1963.52568359375 , train loss 1981.3400634765626, pcc = 0.00507836788892746\n",
      "epoch 147, EV = -3540.960860754314, val  loss = 1929.3359619140624 , train loss 1947.3053588867188, pcc = 0.006501609925180674\n",
      "epoch 148, EV = -3375.016621104458, val  loss = 1898.3564208984376 , train loss 1909.6765014648438, pcc = 0.007133313454687595\n",
      "epoch 149, EV = -3234.2906734985218, val  loss = 1868.24833984375 , train loss 1874.0957641601562, pcc = 0.008592283353209496\n",
      "epoch 150, EV = -3086.418187961244, val  loss = 1838.3277587890625 , train loss 1848.1812377929687, pcc = 0.00737015763297677\n",
      "epoch 151, EV = -2855.198316072163, val  loss = 1808.8400146484375 , train loss 1817.0172607421875, pcc = 0.00461074523627758\n",
      "epoch 152, EV = -2740.0417659826444, val  loss = 1779.8765869140625 , train loss 1787.2633911132812, pcc = -0.001819112105295062\n",
      "epoch 153, EV = -2613.9315944303544, val  loss = 1751.7440185546875 , train loss 1759.6619262695312, pcc = 0.008276653476059437\n",
      "epoch 154, EV = -2470.4850312617787, val  loss = 1723.0135498046875 , train loss 1730.4798828125, pcc = 2.4272874725284055e-05\n",
      "epoch 155, EV = -2341.351452392444, val  loss = 1695.504248046875 , train loss 1703.12080078125, pcc = -0.0004712488444056362\n",
      "epoch 156, EV = -2206.3882047753586, val  loss = 1668.048291015625 , train loss 1679.8917602539063, pcc = 0.016529982909560204\n",
      "epoch 157, EV = -2106.3868056054703, val  loss = 1641.6134033203125 , train loss 1648.4019775390625, pcc = 0.005570250563323498\n",
      "epoch 158, EV = -2004.3097631638511, val  loss = 1615.269189453125 , train loss 1623.6371215820313, pcc = 0.01016332395374775\n",
      "epoch 159, EV = -1871.1690398810204, val  loss = 1589.59111328125 , train loss 1600.4536743164062, pcc = 0.013444969430565834\n",
      "epoch 160, EV = -1771.18758288601, val  loss = 1564.0080810546874 , train loss 1572.7859985351563, pcc = 0.009681141003966331\n",
      "epoch 161, EV = -1681.486393347121, val  loss = 1538.61728515625 , train loss 1547.0562133789062, pcc = 0.010331702418625355\n",
      "epoch 162, EV = -1583.0208466094837, val  loss = 1512.9911376953125 , train loss 1523.2380737304688, pcc = 0.018443383276462555\n",
      "epoch 163, EV = -1513.6397052647774, val  loss = 1489.342236328125 , train loss 1495.1094360351562, pcc = 0.013871190138161182\n",
      "epoch 164, EV = -1441.8268444036182, val  loss = 1464.543896484375 , train loss 1470.9980224609376, pcc = 0.005768139846622944\n",
      "epoch 165, EV = -1321.3959978358787, val  loss = 1439.9410888671875 , train loss 1446.9907592773438, pcc = 0.017001917585730553\n",
      "epoch 166, EV = -1287.3467491467793, val  loss = 1416.7695068359376 , train loss 1420.0816040039062, pcc = 0.021032745018601418\n",
      "epoch 167, EV = -1196.5821050740126, val  loss = 1392.830859375 , train loss 1401.1302490234375, pcc = 0.009877253323793411\n",
      "epoch 168, EV = -1135.1502363953673, val  loss = 1369.50126953125 , train loss 1375.873876953125, pcc = 0.0036918551195412874\n",
      "epoch 169, EV = -1082.1866698787924, val  loss = 1346.6487548828125 , train loss 1350.083154296875, pcc = 0.0010651301126927137\n",
      "epoch 170, EV = -1012.1383399335962, val  loss = 1323.303857421875 , train loss 1330.23564453125, pcc = 0.016737986356019974\n",
      "epoch 171, EV = -942.8449153042676, val  loss = 1300.286474609375 , train loss 1305.678759765625, pcc = 0.021649297326803207\n",
      "epoch 172, EV = -895.3242461974161, val  loss = 1277.8204345703125 , train loss 1282.5198852539063, pcc = 0.02243855968117714\n",
      "epoch 173, EV = -847.1733961753678, val  loss = 1255.516064453125 , train loss 1260.5398193359374, pcc = 0.01876366138458252\n",
      "epoch 174, EV = -792.8184109685714, val  loss = 1233.5878173828125 , train loss 1240.4332641601563, pcc = 0.008651159703731537\n",
      "epoch 175, EV = -742.9537636303065, val  loss = 1211.0128173828125 , train loss 1216.483837890625, pcc = 0.01807211898267269\n",
      "epoch 176, EV = -691.4042185586795, val  loss = 1189.2046630859375 , train loss 1194.880224609375, pcc = 0.016393709927797318\n",
      "epoch 177, EV = -663.919814485207, val  loss = 1167.9397705078125 , train loss 1171.97421875, pcc = 0.013631828129291534\n",
      "epoch 178, EV = -627.59214376253, val  loss = 1146.522998046875 , train loss 1152.6424194335937, pcc = 0.018034134060144424\n",
      "epoch 179, EV = -580.7427130404271, val  loss = 1124.6200439453125 , train loss 1129.3950439453124, pcc = 0.01790723018348217\n",
      "epoch 180, EV = -554.2885192559477, val  loss = 1103.8586669921874 , train loss 1108.14521484375, pcc = 0.02102319896221161\n",
      "epoch 181, EV = -513.7122203425357, val  loss = 1082.6342041015625 , train loss 1087.4605224609375, pcc = 0.029909467324614525\n",
      "epoch 182, EV = -483.95363517602283, val  loss = 1061.714013671875 , train loss 1064.9834228515624, pcc = 0.015524995513260365\n",
      "epoch 183, EV = -459.42556663249667, val  loss = 1041.3621337890625 , train loss 1044.9177368164062, pcc = 0.010096357204020023\n",
      "epoch 184, EV = -439.82264534004946, val  loss = 1021.0371215820312 , train loss 1024.7298706054687, pcc = 0.023001154884696007\n",
      "epoch 185, EV = -404.2477063808525, val  loss = 1000.3821899414063 , train loss 1003.7808959960937, pcc = 0.01806546188890934\n",
      "epoch 186, EV = -381.5874790486537, val  loss = 980.1484985351562 , train loss 983.4610168457032, pcc = 0.02179003693163395\n",
      "epoch 187, EV = -362.7829175779694, val  loss = 960.1917602539063 , train loss 963.8761901855469, pcc = 0.024135421961545944\n",
      "epoch 188, EV = -332.5402797439642, val  loss = 939.9574096679687 , train loss 943.9652709960938, pcc = 0.013995466753840446\n",
      "epoch 189, EV = -318.9304338202142, val  loss = 920.7789916992188 , train loss 924.4830139160156, pcc = 0.017925385385751724\n",
      "epoch 190, EV = -304.5997942926591, val  loss = 901.1245361328125 , train loss 903.8570129394532, pcc = 0.011634346097707748\n",
      "epoch 191, EV = -278.0686185098531, val  loss = 881.5010375976562 , train loss 884.5646850585938, pcc = 0.022091643884778023\n",
      "epoch 192, EV = -259.78807784381667, val  loss = 862.3327270507813 , train loss 865.2987670898438, pcc = 0.025137554854154587\n",
      "epoch 193, EV = -243.91366285085678, val  loss = 843.17099609375 , train loss 846.9070373535156, pcc = 0.021781766787171364\n",
      "epoch 194, EV = -236.90000733680893, val  loss = 824.5662719726563 , train loss 827.993115234375, pcc = 0.018682856112718582\n",
      "epoch 195, EV = -270.74683126964067, val  loss = 808.2023193359375 , train loss 812.0203552246094, pcc = 0.02454483136534691\n",
      "epoch 196, EV = -247.49769466906264, val  loss = 788.7654174804687 , train loss 792.5788330078125, pcc = 0.03326103836297989\n",
      "epoch 197, EV = -212.3758052158774, val  loss = 769.6049194335938 , train loss 772.3768005371094, pcc = 0.025060921907424927\n",
      "epoch 198, EV = -179.94382679253295, val  loss = 750.7147338867187 , train loss 754.3092041015625, pcc = 0.039992555975914\n",
      "epoch 199, EV = -170.49462278265702, val  loss = 732.7549438476562 , train loss 735.9617553710938, pcc = 0.02247268334031105\n",
      "epoch 200, EV = -150.75926131219194, val  loss = 730.340673828125 , train loss 731.13359375, pcc = 0.03397470712661743\n",
      "epoch 201, EV = -147.5731427910035, val  loss = 728.446484375 , train loss 728.5549743652343, pcc = 0.03634455427527428\n",
      "epoch 202, EV = -146.54147123663049, val  loss = 726.5798828125 , train loss 726.8652282714844, pcc = 0.03454672917723656\n",
      "epoch 203, EV = -145.25077138658156, val  loss = 724.6719482421875 , train loss 725.0577270507813, pcc = 0.03650255128741264\n",
      "epoch 204, EV = -143.29045068903974, val  loss = 722.7103759765625 , train loss 723.1513122558594, pcc = 0.033379413187503815\n",
      "epoch 205, EV = -142.0398208383928, val  loss = 720.73095703125 , train loss 720.8005798339843, pcc = 0.035463497042655945\n",
      "epoch 206, EV = -140.65743288659212, val  loss = 718.7209716796875 , train loss 718.9104675292969, pcc = 0.034950096160173416\n",
      "epoch 207, EV = -139.5513526232619, val  loss = 716.659130859375 , train loss 717.2461608886719, pcc = 0.03298039361834526\n",
      "epoch 208, EV = -138.62658556720666, val  loss = 714.5879028320312 , train loss 715.0051635742187, pcc = 0.03379466384649277\n",
      "epoch 209, EV = -137.44421524332282, val  loss = 712.4534301757812 , train loss 712.8365783691406, pcc = 0.0356031097471714\n",
      "epoch 210, EV = -136.24057447073753, val  loss = 710.286328125 , train loss 710.7246704101562, pcc = 0.03408830240368843\n",
      "epoch 211, EV = -134.75073601174773, val  loss = 708.0788818359375 , train loss 708.0742065429688, pcc = 0.0367162711918354\n",
      "epoch 212, EV = -133.5957142647944, val  loss = 705.82822265625 , train loss 706.3331298828125, pcc = 0.03562911972403526\n",
      "epoch 213, EV = -132.5944941890867, val  loss = 703.5517822265625 , train loss 703.9292297363281, pcc = 0.035188429057598114\n",
      "epoch 214, EV = -131.29220998182632, val  loss = 701.2210815429687 , train loss 701.5740600585938, pcc = 0.03552901744842529\n",
      "epoch 215, EV = -129.95163718022798, val  loss = 698.8462036132812 , train loss 699.6171875, pcc = 0.03459659963846207\n",
      "epoch 216, EV = -129.06096347486763, val  loss = 696.444482421875 , train loss 696.7404846191406, pcc = 0.03546183556318283\n",
      "epoch 217, EV = -127.79862557586871, val  loss = 693.9763549804687 , train loss 694.3886047363281, pcc = 0.032366909086704254\n",
      "epoch 218, EV = -126.29374190903546, val  loss = 691.4754272460938 , train loss 691.9495361328125, pcc = 0.03368834778666496\n",
      "epoch 219, EV = -125.60324230737854, val  loss = 688.9482543945312 , train loss 689.4968139648438, pcc = 0.03734510764479637\n",
      "epoch 220, EV = -123.71850788697861, val  loss = 686.3469848632812 , train loss 687.0772094726562, pcc = 0.03760772943496704\n",
      "epoch 221, EV = -122.60739477580054, val  loss = 683.7180419921875 , train loss 683.7427551269532, pcc = 0.0380568653345108\n",
      "epoch 222, EV = -121.0324655426176, val  loss = 681.025732421875 , train loss 681.4700500488282, pcc = 0.035132624208927155\n",
      "epoch 223, EV = -119.68702103276001, val  loss = 678.3027709960937 , train loss 678.7862670898437, pcc = 0.038977306336164474\n",
      "epoch 224, EV = -118.7726803484716, val  loss = 675.536669921875 , train loss 675.8854858398438, pcc = 0.03428954631090164\n",
      "epoch 225, EV = -117.56618440360354, val  loss = 672.7337280273438 , train loss 672.91533203125, pcc = 0.03421706706285477\n",
      "epoch 226, EV = -116.31844726152588, val  loss = 669.86513671875 , train loss 670.387060546875, pcc = 0.03436734527349472\n",
      "epoch 227, EV = -114.4575981163142, val  loss = 666.9296752929688 , train loss 667.658251953125, pcc = 0.040381040424108505\n",
      "epoch 228, EV = -113.87141563913278, val  loss = 663.9833251953125 , train loss 664.4159362792968, pcc = 0.03645530343055725\n",
      "epoch 229, EV = -112.03317473959504, val  loss = 660.962548828125 , train loss 661.4559997558594, pcc = 0.03644636273384094\n",
      "epoch 230, EV = -110.01290051769791, val  loss = 657.8815185546875 , train loss 658.3336608886718, pcc = 0.0370434895157814\n",
      "epoch 231, EV = -108.87533144888125, val  loss = 654.7838623046875 , train loss 655.59296875, pcc = 0.03453011438250542\n",
      "epoch 232, EV = -107.81058818101883, val  loss = 651.619677734375 , train loss 652.2615295410156, pcc = 0.035479836165905\n",
      "epoch 233, EV = -106.78787403357656, val  loss = 648.4185302734375 , train loss 648.9614990234375, pcc = 0.034793198108673096\n",
      "epoch 234, EV = -104.74384666430323, val  loss = 645.1447143554688 , train loss 645.4213439941407, pcc = 0.03512013331055641\n",
      "epoch 235, EV = -103.19852039270233, val  loss = 641.8236206054687 , train loss 642.1910461425781, pcc = 0.041245199739933014\n",
      "epoch 236, EV = -101.47938803936306, val  loss = 638.4503295898437 , train loss 639.0775573730468, pcc = 0.036034874618053436\n",
      "epoch 237, EV = -100.5748020807902, val  loss = 635.0490478515625 , train loss 635.5040771484375, pcc = 0.036325279623270035\n",
      "epoch 238, EV = -99.57060182094574, val  loss = 631.5906005859375 , train loss 632.1247924804687, pcc = 0.04027213901281357\n",
      "epoch 239, EV = -97.64366079422466, val  loss = 628.048681640625 , train loss 628.6715759277344, pcc = 0.04198526218533516\n",
      "epoch 240, EV = -96.58953549150834, val  loss = 624.4790283203125 , train loss 625.3294738769531, pcc = 0.04117093235254288\n",
      "epoch 241, EV = -94.19409238873867, val  loss = 620.8343383789063 , train loss 621.5446594238281, pcc = 0.033940061926841736\n",
      "epoch 242, EV = -92.95432438139329, val  loss = 617.145849609375 , train loss 617.558837890625, pcc = 0.04389994964003563\n",
      "epoch 243, EV = -91.91126707875938, val  loss = 613.4265502929687 , train loss 614.0748718261718, pcc = 0.038527555763721466\n",
      "epoch 244, EV = -90.08024623728635, val  loss = 609.6232055664062 , train loss 610.2599792480469, pcc = 0.04406755417585373\n",
      "epoch 245, EV = -88.91498352142803, val  loss = 605.7931030273437 , train loss 606.4380493164062, pcc = 0.04290775582194328\n",
      "epoch 246, EV = -87.30236855410693, val  loss = 601.88720703125 , train loss 602.7112854003906, pcc = 0.03950010985136032\n",
      "epoch 247, EV = -85.91374358482528, val  loss = 597.9429809570313 , train loss 598.3677978515625, pcc = 0.04523763805627823\n",
      "epoch 248, EV = -84.51660760871151, val  loss = 593.9438720703125 , train loss 594.5950500488282, pcc = 0.0446927547454834\n",
      "epoch 249, EV = -83.358320717226, val  loss = 589.8997314453125 , train loss 590.5057312011719, pcc = 0.04678990691900253\n",
      "epoch 250, EV = -81.33465467837819, val  loss = 585.7662109375 , train loss 586.348388671875, pcc = 0.044142723083496094\n",
      "epoch 251, EV = -79.89325724672852, val  loss = 581.61064453125 , train loss 582.0462280273438, pcc = 0.046562083065509796\n",
      "epoch 252, EV = -78.40444822374143, val  loss = 577.3822998046875 , train loss 578.0927368164063, pcc = 0.04608523100614548\n",
      "epoch 253, EV = -77.21017113693973, val  loss = 573.1228271484375 , train loss 573.9994140625, pcc = 0.03978551924228668\n",
      "epoch 254, EV = -76.08905445692832, val  loss = 568.7926147460937 , train loss 569.5173828125, pcc = 0.04717295989394188\n",
      "epoch 255, EV = -73.7736957909768, val  loss = 564.391259765625 , train loss 565.1186096191407, pcc = 0.04437998682260513\n",
      "epoch 256, EV = -72.84093348708069, val  loss = 559.995263671875 , train loss 560.7392883300781, pcc = 0.043984025716781616\n",
      "epoch 257, EV = -71.38920185335895, val  loss = 555.5020629882813 , train loss 556.0176879882813, pcc = 0.04469235986471176\n",
      "epoch 258, EV = -69.66584513689342, val  loss = 550.9606811523438 , train loss 551.4624206542969, pcc = 0.04733549803495407\n",
      "epoch 259, EV = -68.41182896338012, val  loss = 546.3790283203125 , train loss 547.0234130859375, pcc = 0.05418996885418892\n",
      "epoch 260, EV = -66.64159168067731, val  loss = 541.7321655273438 , train loss 542.5091918945312, pcc = 0.04744253307580948\n",
      "epoch 261, EV = -65.63813836219018, val  loss = 537.0505981445312 , train loss 537.7051086425781, pcc = 0.04413602501153946\n",
      "epoch 262, EV = -64.06997064435691, val  loss = 532.3042358398437 , train loss 533.2040832519531, pcc = 0.04810366779565811\n",
      "epoch 263, EV = -62.45179393835235, val  loss = 527.5064208984375 , train loss 528.3724548339844, pcc = 0.05435454845428467\n",
      "epoch 264, EV = -61.502553604151075, val  loss = 522.6826049804688 , train loss 523.4205688476562, pcc = 0.04976017400622368\n",
      "epoch 265, EV = -60.12925903943547, val  loss = 517.792529296875 , train loss 518.7045471191407, pcc = 0.04841623827815056\n",
      "epoch 266, EV = -58.80717736064342, val  loss = 512.85302734375 , train loss 513.5557739257813, pcc = 0.0533820316195488\n",
      "epoch 267, EV = -56.995196551607364, val  loss = 507.8576416015625 , train loss 508.4856262207031, pcc = 0.05312914401292801\n",
      "epoch 268, EV = -56.00289065482323, val  loss = 502.8412780761719 , train loss 503.7916290283203, pcc = 0.04723624885082245\n",
      "epoch 269, EV = -54.178733236957015, val  loss = 497.7587646484375 , train loss 498.28374938964845, pcc = 0.05598227307200432\n",
      "epoch 270, EV = -53.23123024848469, val  loss = 492.64728393554685 , train loss 493.4064666748047, pcc = 0.06337617337703705\n",
      "epoch 271, EV = -51.75585174665117, val  loss = 487.48085327148436 , train loss 488.1092559814453, pcc = 0.0522921159863472\n",
      "epoch 272, EV = -50.24179628752825, val  loss = 482.2696228027344 , train loss 483.0344207763672, pcc = 0.05641701817512512\n",
      "epoch 273, EV = -48.93866643205023, val  loss = 477.0117919921875 , train loss 477.86570129394534, pcc = 0.05921139195561409\n",
      "epoch 274, EV = -47.6338684992832, val  loss = 471.725830078125 , train loss 472.5538299560547, pcc = 0.05323738604784012\n",
      "epoch 275, EV = -46.89961614891102, val  loss = 466.415087890625 , train loss 467.0933013916016, pcc = 0.056096889078617096\n",
      "epoch 276, EV = -45.72098530332247, val  loss = 461.0484619140625 , train loss 461.78224487304686, pcc = 0.053726691752672195\n",
      "epoch 277, EV = -44.12132861478287, val  loss = 455.6398498535156 , train loss 456.46983337402344, pcc = 0.05780230090022087\n",
      "epoch 278, EV = -43.058729614604985, val  loss = 450.20390625 , train loss 450.91502380371094, pcc = 0.06464600563049316\n",
      "epoch 279, EV = -41.77717045315525, val  loss = 444.7273864746094 , train loss 445.5228759765625, pcc = 0.05940110608935356\n",
      "epoch 280, EV = -40.34536361432912, val  loss = 439.21697998046875 , train loss 440.00901794433594, pcc = 0.057464875280857086\n",
      "epoch 281, EV = -39.58233068543568, val  loss = 433.69525146484375 , train loss 434.4894561767578, pcc = 0.06838402152061462\n",
      "epoch 282, EV = -37.88940284178968, val  loss = 428.1199462890625 , train loss 428.901806640625, pcc = 0.06624992936849594\n",
      "epoch 283, EV = -36.83581697574833, val  loss = 422.516357421875 , train loss 423.31678466796876, pcc = 0.06320623308420181\n",
      "epoch 284, EV = -36.27469616710094, val  loss = 416.91840209960935 , train loss 417.66350708007815, pcc = 0.056487709283828735\n",
      "epoch 285, EV = -34.74788110036599, val  loss = 411.2591613769531 , train loss 412.0251068115234, pcc = 0.06801828742027283\n",
      "epoch 286, EV = -33.74104959400076, val  loss = 405.5937438964844 , train loss 406.31853637695315, pcc = 0.0675591304898262\n",
      "epoch 287, EV = -32.82155172396124, val  loss = 399.9078857421875 , train loss 400.66148986816404, pcc = 0.06412813067436218\n",
      "epoch 288, EV = -31.638422940383876, val  loss = 394.1933227539063 , train loss 395.02892150878904, pcc = 0.06995954364538193\n",
      "epoch 289, EV = -30.992803068014613, val  loss = 388.4851501464844 , train loss 389.3295379638672, pcc = 0.06849420070648193\n",
      "epoch 290, EV = -29.608925382296245, val  loss = 382.7351501464844 , train loss 383.6350921630859, pcc = 0.06393938511610031\n",
      "epoch 291, EV = -29.18706372955389, val  loss = 376.99445190429685 , train loss 377.7714874267578, pcc = 0.07126319408416748\n",
      "epoch 292, EV = -28.263665101507254, val  loss = 371.2352233886719 , train loss 372.07029113769534, pcc = 0.07565055042505264\n",
      "epoch 293, EV = -27.013176421324413, val  loss = 365.4448303222656 , train loss 366.22467041015625, pcc = 0.07450947910547256\n",
      "epoch 294, EV = -26.330086208226387, val  loss = 359.66986694335935 , train loss 360.3664611816406, pcc = 0.07662333548069\n",
      "epoch 295, EV = -25.074957258345787, val  loss = 353.86821899414065 , train loss 354.64288330078125, pcc = 0.07626739144325256\n",
      "epoch 296, EV = -24.361748310557584, val  loss = 348.0880126953125 , train loss 348.8571472167969, pcc = 0.07789525389671326\n",
      "epoch 297, EV = -23.49455365590882, val  loss = 342.29254150390625 , train loss 343.0549285888672, pcc = 0.08117900788784027\n",
      "epoch 298, EV = -22.880606819140283, val  loss = 336.5107116699219 , train loss 337.2374969482422, pcc = 0.09190616011619568\n",
      "epoch 299, EV = -21.57792048809821, val  loss = 330.7090209960937 , train loss 331.523291015625, pcc = 0.08331380784511566\n",
      "epoch 300, EV = -21.088348290899344, val  loss = 324.941162109375 , train loss 325.7408050537109, pcc = 0.0880114883184433\n",
      "epoch 301, EV = -20.27092239835806, val  loss = 319.1672119140625 , train loss 319.87962951660154, pcc = 0.07793086767196655\n",
      "epoch 302, EV = -19.69310526732813, val  loss = 313.4055969238281 , train loss 314.16310424804686, pcc = 0.078990638256073\n",
      "epoch 303, EV = -19.081108425792895, val  loss = 307.6579895019531 , train loss 308.3793151855469, pcc = 0.0882873684167862\n",
      "epoch 304, EV = -18.202072810185584, val  loss = 301.9221252441406 , train loss 302.6924224853516, pcc = 0.0891856700181961\n",
      "epoch 305, EV = -17.41774035702672, val  loss = 296.19989013671875 , train loss 296.9897430419922, pcc = 0.09575442969799042\n",
      "epoch 306, EV = -16.385899095158827, val  loss = 290.49363403320314 , train loss 291.2819030761719, pcc = 0.10439221560955048\n",
      "epoch 307, EV = -15.848871106641335, val  loss = 284.82464599609375 , train loss 285.5388549804687, pcc = 0.09592925012111664\n",
      "epoch 308, EV = -15.305671962729672, val  loss = 279.1704040527344 , train loss 279.9442962646484, pcc = 0.09573527425527573\n",
      "epoch 309, EV = -14.947485870436617, val  loss = 273.5583801269531 , train loss 274.30517578125, pcc = 0.1078205332159996\n",
      "epoch 310, EV = -14.211344719455953, val  loss = 267.95555419921874 , train loss 268.70400085449216, pcc = 0.09014073759317398\n",
      "epoch 311, EV = -13.442110155758105, val  loss = 262.38161010742186 , train loss 263.1601226806641, pcc = 0.10336973518133163\n",
      "epoch 312, EV = -13.106541880912948, val  loss = 256.85894775390625 , train loss 257.6177185058594, pcc = 0.10505332052707672\n",
      "epoch 313, EV = -13.108963164844011, val  loss = 251.3800048828125 , train loss 252.08663482666014, pcc = 0.09963711351156235\n",
      "epoch 314, EV = -11.912017369479464, val  loss = 245.9045654296875 , train loss 246.65179290771485, pcc = 0.10734017938375473\n",
      "epoch 315, EV = -11.809932350589518, val  loss = 240.49578247070312 , train loss 241.20489349365235, pcc = 0.11327644437551498\n",
      "epoch 316, EV = -11.265249543545538, val  loss = 235.11788330078124 , train loss 235.8033233642578, pcc = 0.10904683917760849\n",
      "epoch 317, EV = -10.432902862628302, val  loss = 229.77420959472656 , train loss 230.47884674072264, pcc = 0.11677588522434235\n",
      "epoch 318, EV = -9.988850600886764, val  loss = 224.4889129638672 , train loss 225.18372802734376, pcc = 0.10733132064342499\n",
      "epoch 319, EV = -10.169842054969386, val  loss = 219.2739501953125 , train loss 220.00765533447264, pcc = 0.12048597633838654\n",
      "epoch 320, EV = -9.824625244266109, val  loss = 214.09035949707032 , train loss 214.74561767578126, pcc = 0.12058607488870621\n",
      "epoch 321, EV = -8.833690125691263, val  loss = 208.93168029785156 , train loss 209.57852020263672, pcc = 0.11307765543460846\n",
      "epoch 322, EV = -8.677271588329683, val  loss = 203.8599365234375 , train loss 204.48296661376952, pcc = 0.12637636065483093\n",
      "epoch 323, EV = -7.972204003940549, val  loss = 198.81940612792968 , train loss 199.52730560302734, pcc = 0.14436563849449158\n",
      "epoch 324, EV = -7.66711564283622, val  loss = 193.85330200195312 , train loss 194.4793670654297, pcc = 0.1422233283519745\n",
      "epoch 325, EV = -7.218805238865969, val  loss = 188.944091796875 , train loss 189.57565307617188, pcc = 0.14849698543548584\n",
      "epoch 326, EV = -6.934051729085152, val  loss = 184.098388671875 , train loss 184.7290069580078, pcc = 0.13223828375339508\n",
      "epoch 327, EV = -7.036143006985648, val  loss = 179.32583618164062 , train loss 179.96505584716797, pcc = 0.13211888074874878\n",
      "epoch 328, EV = -6.693681330534449, val  loss = 174.60465087890626 , train loss 175.2178695678711, pcc = 0.13902047276496887\n",
      "epoch 329, EV = -6.108961878115671, val  loss = 169.93673095703124 , train loss 170.53292999267578, pcc = 0.16364170610904694\n",
      "epoch 330, EV = -6.150636169994087, val  loss = 165.35535583496093 , train loss 165.9434356689453, pcc = 0.16175660490989685\n",
      "epoch 331, EV = -5.500470467826776, val  loss = 160.82823791503907 , train loss 161.40668640136718, pcc = 0.1522059142589569\n",
      "epoch 332, EV = -5.231540461381276, val  loss = 156.38096923828124 , train loss 156.96182861328126, pcc = 0.1830550730228424\n",
      "epoch 333, EV = -5.007232801956043, val  loss = 151.99065551757812 , train loss 152.54417266845704, pcc = 0.16341431438922882\n",
      "epoch 334, EV = -4.989415666513276, val  loss = 147.68758850097657 , train loss 148.24926147460937, pcc = 0.14483776688575745\n",
      "epoch 335, EV = -4.767907213746455, val  loss = 143.45612487792968 , train loss 144.0019989013672, pcc = 0.14734706282615662\n",
      "epoch 336, EV = -4.668798560113237, val  loss = 139.28661193847657 , train loss 139.84057006835937, pcc = 0.16236668825149536\n",
      "epoch 337, EV = -4.239032834768295, val  loss = 135.1914520263672 , train loss 135.7269485473633, pcc = 0.14969195425510406\n",
      "epoch 338, EV = -3.9314248007640504, val  loss = 131.1652801513672 , train loss 131.66512756347657, pcc = 0.16840697824954987\n",
      "epoch 339, EV = -3.836609107883353, val  loss = 127.2266632080078 , train loss 127.72679443359375, pcc = 0.17432847619056702\n",
      "epoch 340, EV = -3.4781663088422072, val  loss = 123.34272003173828 , train loss 123.85102691650391, pcc = 0.19132331013679504\n",
      "epoch 341, EV = -3.411030753139864, val  loss = 119.55835723876953 , train loss 120.02968292236328, pcc = 0.19863703846931458\n",
      "epoch 342, EV = -3.1099551445559452, val  loss = 115.83505554199219 , train loss 116.32308654785156, pcc = 0.17971093952655792\n",
      "epoch 343, EV = -3.2844345914690116, val  loss = 112.20862121582032 , train loss 112.6674186706543, pcc = 0.17623014748096466\n",
      "epoch 344, EV = -2.801110149475566, val  loss = 108.63280487060547 , train loss 109.07748031616211, pcc = 0.18285995721817017\n",
      "epoch 345, EV = -2.61977997735927, val  loss = 105.13758239746093 , train loss 105.58937072753906, pcc = 0.22770436108112335\n",
      "epoch 346, EV = -2.593117584784826, val  loss = 101.72767486572266 , train loss 102.17209396362304, pcc = 0.21108552813529968\n",
      "epoch 347, EV = -2.306436844562229, val  loss = 98.38801422119141 , train loss 98.81825714111328, pcc = 0.2049858123064041\n",
      "epoch 348, EV = -2.3145691553751626, val  loss = 95.13329620361328 , train loss 95.52910232543945, pcc = 0.21184667944908142\n",
      "epoch 349, EV = -2.0446667143127373, val  loss = 91.94764862060546 , train loss 92.3533546447754, pcc = 0.21465763449668884\n",
      "epoch 350, EV = -1.9204812091693544, val  loss = 88.84110412597656 , train loss 89.23378448486328, pcc = 0.22279593348503113\n",
      "epoch 351, EV = -1.8083804453674115, val  loss = 85.8101318359375 , train loss 86.1828498840332, pcc = 0.2389817237854004\n",
      "epoch 352, EV = -1.7706055165382855, val  loss = 82.8532485961914 , train loss 83.21982498168946, pcc = 0.24072475731372833\n",
      "epoch 353, EV = -1.6806194986167706, val  loss = 79.97088012695312 , train loss 80.34265747070313, pcc = 0.2533819377422333\n",
      "epoch 354, EV = -1.535416460873788, val  loss = 77.165234375 , train loss 77.5163703918457, pcc = 0.24935832619667053\n",
      "epoch 355, EV = -1.7562078352560078, val  loss = 74.43971557617188 , train loss 74.79179306030274, pcc = 0.22250588238239288\n",
      "epoch 356, EV = -1.5902361425391414, val  loss = 71.77386627197265 , train loss 72.12055892944336, pcc = 0.23605164885520935\n",
      "epoch 357, EV = -1.3806038159027434, val  loss = 69.1778793334961 , train loss 69.51878204345704, pcc = 0.2524479031562805\n",
      "epoch 358, EV = -1.2895138221874571, val  loss = 66.65814361572265 , train loss 66.9753433227539, pcc = 0.24016471207141876\n",
      "epoch 359, EV = -1.1728581048940356, val  loss = 64.20609130859376 , train loss 64.51293716430663, pcc = 0.24587607383728027\n",
      "epoch 360, EV = -1.1786859213260181, val  loss = 61.82805480957031 , train loss 62.12742195129395, pcc = 0.2587307095527649\n",
      "epoch 361, EV = -0.9923324428106609, val  loss = 59.51094284057617 , train loss 59.80180740356445, pcc = 0.2771037817001343\n",
      "epoch 362, EV = -1.0029549122902386, val  loss = 57.27019424438477 , train loss 57.54958801269531, pcc = 0.28043022751808167\n",
      "epoch 363, EV = -0.9190947167706072, val  loss = 55.08681640625 , train loss 55.36908645629883, pcc = 0.2904331088066101\n",
      "epoch 364, EV = -0.8874696508834237, val  loss = 52.974701690673825 , train loss 53.2382999420166, pcc = 0.3076372742652893\n",
      "epoch 365, EV = -0.9174749584574449, val  loss = 50.92855224609375 , train loss 51.175431442260745, pcc = 0.2925139367580414\n",
      "epoch 366, EV = -0.8214055864434493, val  loss = 48.945519256591794 , train loss 49.19174690246582, pcc = 0.3002031147480011\n",
      "epoch 367, EV = -0.7861394343668955, val  loss = 47.02310791015625 , train loss 47.2708194732666, pcc = 0.2847934365272522\n",
      "epoch 368, EV = -0.8160972642271143, val  loss = 45.163290405273436 , train loss 45.40906028747558, pcc = 0.2800162732601166\n",
      "epoch 369, EV = -0.720392972753759, val  loss = 43.3538215637207 , train loss 43.592817306518555, pcc = 0.2884257733821869\n",
      "epoch 370, EV = -0.6815439263979594, val  loss = 41.60625305175781 , train loss 41.82852363586426, pcc = 0.2819991111755371\n",
      "epoch 371, EV = -0.5701006551583608, val  loss = 39.914458465576175 , train loss 40.13131141662598, pcc = 0.30423083901405334\n",
      "epoch 372, EV = -0.5229052681671945, val  loss = 38.28287124633789 , train loss 38.48787422180176, pcc = 0.3186561167240143\n",
      "epoch 373, EV = -0.4759440082207061, val  loss = 36.702562713623045 , train loss 36.90199279785156, pcc = 0.33483225107192993\n",
      "epoch 374, EV = -0.46541142097690646, val  loss = 35.179074096679685 , train loss 35.370432662963864, pcc = 0.32978641986846924\n",
      "epoch 375, EV = -0.4487398779183103, val  loss = 33.70802001953125 , train loss 33.886051940917966, pcc = 0.3421992361545563\n",
      "epoch 376, EV = -0.4849133841824113, val  loss = 32.29059982299805 , train loss 32.467384338378906, pcc = 0.3130391836166382\n",
      "epoch 377, EV = -0.400750876518718, val  loss = 30.91365280151367 , train loss 31.091963958740234, pcc = 0.34450629353523254\n",
      "epoch 378, EV = -0.3654733525033583, val  loss = 29.590909957885742 , train loss 29.751129150390625, pcc = 0.34865498542785645\n",
      "epoch 379, EV = -0.3541025054036525, val  loss = 28.315416717529295 , train loss 28.475792694091798, pcc = 0.3605212867259979\n",
      "epoch 380, EV = -0.29705707336726944, val  loss = 27.084963226318358 , train loss 27.240574836730957, pcc = 0.35202035307884216\n",
      "epoch 381, EV = -0.4773043822823909, val  loss = 25.908686447143555 , train loss 26.06241569519043, pcc = 0.33104950189590454\n",
      "epoch 382, EV = -0.40153320002974124, val  loss = 24.76729850769043 , train loss 24.909923934936522, pcc = 0.3397618234157562\n",
      "epoch 383, EV = -0.3369742475057903, val  loss = 23.662815856933594 , train loss 23.809409141540527, pcc = 0.34361720085144043\n",
      "epoch 384, EV = -0.2744043260289912, val  loss = 22.604054641723632 , train loss 22.736236572265625, pcc = 0.36507993936538696\n",
      "epoch 385, EV = -0.3265716799518518, val  loss = 21.590079879760744 , train loss 21.724688720703124, pcc = 0.3430168628692627\n",
      "epoch 386, EV = -0.350461272816909, val  loss = 20.614496612548827 , train loss 20.735720062255858, pcc = 0.3244662880897522\n",
      "epoch 387, EV = -0.27133206892431827, val  loss = 19.670591354370117 , train loss 19.78986225128174, pcc = 0.3355945646762848\n",
      "epoch 388, EV = -0.1962725833842629, val  loss = 18.76071434020996 , train loss 18.87679386138916, pcc = 0.3746914863586426\n",
      "epoch 389, EV = -0.2007515775529962, val  loss = 17.891120529174806 , train loss 18.004576873779296, pcc = 0.3934021294116974\n",
      "epoch 390, EV = -0.19995893720994917, val  loss = 17.05826187133789 , train loss 17.16430492401123, pcc = 0.3773292303085327\n",
      "epoch 391, EV = -0.253189015806767, val  loss = 16.25816192626953 , train loss 16.3639404296875, pcc = 0.3751831650733948\n",
      "epoch 392, EV = -0.2120414648139686, val  loss = 15.486787796020508 , train loss 15.591065883636475, pcc = 0.3748962879180908\n",
      "epoch 393, EV = -0.1650433409632298, val  loss = 14.74609832763672 , train loss 14.841472816467284, pcc = 0.40633535385131836\n",
      "epoch 394, EV = -0.15681124726931253, val  loss = 14.03834800720215 , train loss 14.128554630279542, pcc = 0.40781575441360474\n",
      "epoch 395, EV = -0.14103855898505763, val  loss = 13.357568359375 , train loss 13.442776870727538, pcc = 0.41690969467163086\n",
      "epoch 396, EV = -0.12674582318255775, val  loss = 12.707256698608399 , train loss 12.790243434906007, pcc = 0.4132794439792633\n",
      "epoch 397, EV = -0.12198098134576228, val  loss = 12.0866117477417 , train loss 12.16476812362671, pcc = 0.38830819725990295\n",
      "epoch 398, EV = -0.11043942288348549, val  loss = 11.485858917236328 , train loss 11.565519428253173, pcc = 0.42747586965560913\n",
      "epoch 399, EV = -0.10494705250388697, val  loss = 10.914033126831054 , train loss 10.990440177917481, pcc = 0.41735321283340454\n",
      "epoch 400, EV = -0.07921215473559864, val  loss = 10.855329322814942 , train loss 10.8692120552063, pcc = 0.4538182318210602\n",
      "epoch 401, EV = -0.07047693457519799, val  loss = 10.798214530944824 , train loss 10.810789585113525, pcc = 0.45992153882980347\n",
      "epoch 402, EV = -0.07064070251950047, val  loss = 10.740390968322753 , train loss 10.751058387756348, pcc = 0.45901885628700256\n",
      "epoch 403, EV = -0.0679398236567514, val  loss = 10.68169059753418 , train loss 10.69355525970459, pcc = 0.4631660580635071\n",
      "epoch 404, EV = -0.06852434497130544, val  loss = 10.622258186340332 , train loss 10.627869796752929, pcc = 0.4627651274204254\n",
      "epoch 405, EV = -0.06572400896172774, val  loss = 10.561944007873535 , train loss 10.573249816894531, pcc = 0.46265172958374023\n",
      "epoch 406, EV = -0.06586204181637681, val  loss = 10.500776481628417 , train loss 10.511047267913819, pcc = 0.4632623791694641\n",
      "epoch 407, EV = -0.06818133906314247, val  loss = 10.438683319091798 , train loss 10.442764472961425, pcc = 0.46370935440063477\n",
      "epoch 408, EV = -0.06812867208531029, val  loss = 10.375809288024902 , train loss 10.387341785430909, pcc = 0.4636372923851013\n",
      "epoch 409, EV = -0.06755683139750832, val  loss = 10.312040901184082 , train loss 10.320726776123047, pcc = 0.4635075628757477\n",
      "epoch 410, EV = -0.06636558760676467, val  loss = 10.247336387634277 , train loss 10.2574031829834, pcc = 0.4629763662815094\n",
      "epoch 411, EV = -0.0651222822958963, val  loss = 10.181789970397949 , train loss 10.191827774047852, pcc = 0.4657442569732666\n",
      "epoch 412, EV = -0.06478224198023479, val  loss = 10.115632820129395 , train loss 10.128445434570313, pcc = 0.46265071630477905\n",
      "epoch 413, EV = -0.06672096775289167, val  loss = 10.048293685913086 , train loss 10.0570631980896, pcc = 0.4658976197242737\n",
      "epoch 414, EV = -0.0639821123658565, val  loss = 9.98025722503662 , train loss 9.988857078552247, pcc = 0.46200698614120483\n",
      "epoch 415, EV = -0.06409601475063123, val  loss = 9.911354637145996 , train loss 9.924619102478028, pcc = 0.4642297327518463\n",
      "epoch 416, EV = -0.06439425443348132, val  loss = 9.841559600830077 , train loss 9.856223487854004, pcc = 0.464242160320282\n",
      "epoch 417, EV = -0.06445933381716411, val  loss = 9.77097816467285 , train loss 9.787736797332764, pcc = 0.46188920736312866\n",
      "epoch 418, EV = -0.06444246099706281, val  loss = 9.699510955810547 , train loss 9.715857696533202, pcc = 0.46917596459388733\n",
      "epoch 419, EV = -0.06276334691465947, val  loss = 9.627322769165039 , train loss 9.63869800567627, pcc = 0.4647446572780609\n",
      "epoch 420, EV = -0.06384611182045519, val  loss = 9.554331588745118 , train loss 9.563581848144532, pcc = 0.4658530354499817\n",
      "epoch 421, EV = -0.06333634361886141, val  loss = 9.480306434631348 , train loss 9.490158557891846, pcc = 0.4661659598350525\n",
      "epoch 422, EV = -0.060476561387379967, val  loss = 9.405707550048827 , train loss 9.415821552276611, pcc = 0.4654706120491028\n",
      "epoch 423, EV = -0.061388029863959866, val  loss = 9.330403137207032 , train loss 9.34078016281128, pcc = 0.46509677171707153\n",
      "epoch 424, EV = -0.06091933919672381, val  loss = 9.25394515991211 , train loss 9.267863178253174, pcc = 0.46787768602371216\n",
      "epoch 425, EV = -0.0582077795999092, val  loss = 9.176880836486816 , train loss 9.191550350189209, pcc = 0.46863478422164917\n",
      "epoch 426, EV = -0.06306950878678706, val  loss = 9.099121856689454 , train loss 9.114661502838135, pcc = 0.4684170186519623\n",
      "epoch 427, EV = -0.06223896407244498, val  loss = 9.0205659866333 , train loss 9.032514667510986, pcc = 0.46741318702697754\n",
      "epoch 428, EV = -0.05863556213546218, val  loss = 8.941397666931152 , train loss 8.95457935333252, pcc = 0.46812278032302856\n",
      "epoch 429, EV = -0.0605813113220951, val  loss = 8.861536979675293 , train loss 8.871254920959473, pcc = 0.462897390127182\n",
      "epoch 430, EV = -0.05947323587902805, val  loss = 8.780725479125977 , train loss 8.796430015563965, pcc = 0.4674469828605652\n",
      "epoch 431, EV = -0.05655292199369062, val  loss = 8.69945125579834 , train loss 8.71659803390503, pcc = 0.4643060266971588\n",
      "epoch 432, EV = -0.056243290503819786, val  loss = 8.617466735839844 , train loss 8.63378553390503, pcc = 0.46527600288391113\n",
      "epoch 433, EV = -0.056240138777515346, val  loss = 8.534773254394532 , train loss 8.5468900680542, pcc = 0.4663183093070984\n",
      "epoch 434, EV = -0.05769641618979605, val  loss = 8.45140266418457 , train loss 8.468280696868897, pcc = 0.46739691495895386\n",
      "epoch 435, EV = -0.056352286485203525, val  loss = 8.367563056945801 , train loss 8.383524227142335, pcc = 0.46765953302383423\n",
      "epoch 436, EV = -0.055083500188693665, val  loss = 8.283140182495117 , train loss 8.292750549316406, pcc = 0.46803346276283264\n",
      "epoch 437, EV = -0.05485925078392029, val  loss = 8.197901153564453 , train loss 8.212893199920654, pcc = 0.4724743962287903\n",
      "epoch 438, EV = -0.05616965576222068, val  loss = 8.112548065185546 , train loss 8.126407241821289, pcc = 0.46575674414634705\n",
      "epoch 439, EV = -0.05549721289099308, val  loss = 8.026516914367676 , train loss 8.039130115509034, pcc = 0.46615734696388245\n",
      "epoch 440, EV = -0.0563719696120212, val  loss = 7.939648914337158 , train loss 7.956100034713745, pcc = 0.4715500771999359\n",
      "epoch 441, EV = -0.054732525034954675, val  loss = 7.852618789672851 , train loss 7.866731595993042, pcc = 0.4673593044281006\n",
      "epoch 442, EV = -0.05562799332434671, val  loss = 7.76536226272583 , train loss 7.781170082092285, pcc = 0.4627199172973633\n",
      "epoch 443, EV = -0.054752307502846966, val  loss = 7.677446556091309 , train loss 7.692938995361328, pcc = 0.4653760492801666\n",
      "epoch 444, EV = -0.054388086001078285, val  loss = 7.5891508102417 , train loss 7.6077837467193605, pcc = 0.46136683225631714\n",
      "epoch 445, EV = -0.05436044826842191, val  loss = 7.500440311431885 , train loss 7.517233753204346, pcc = 0.4628599286079407\n",
      "epoch 446, EV = -0.05254153619732773, val  loss = 7.411294078826904 , train loss 7.423682689666748, pcc = 0.4650951027870178\n",
      "epoch 447, EV = -0.054083001718186495, val  loss = 7.3217779159545895 , train loss 7.340384817123413, pcc = 0.4695407748222351\n",
      "epoch 448, EV = -0.05018517165853266, val  loss = 7.231971645355225 , train loss 7.2475724697113035, pcc = 0.47437983751296997\n",
      "epoch 449, EV = -0.05141045702131171, val  loss = 7.142391586303711 , train loss 7.153166723251343, pcc = 0.46700960397720337\n",
      "epoch 450, EV = -0.052376691709484974, val  loss = 7.05211992263794 , train loss 7.0697417736053465, pcc = 0.4700213074684143\n",
      "epoch 451, EV = -0.05399737441748904, val  loss = 6.961838912963867 , train loss 6.977943706512451, pcc = 0.47121086716651917\n",
      "epoch 452, EV = -0.053647747165278384, val  loss = 6.871354675292968 , train loss 6.880338144302368, pcc = 0.47091665863990784\n",
      "epoch 453, EV = -0.048995204139174076, val  loss = 6.781087207794189 , train loss 6.790695476531982, pcc = 0.46704158186912537\n",
      "epoch 454, EV = -0.052876334441335576, val  loss = 6.690034198760986 , train loss 6.706593561172485, pcc = 0.47060197591781616\n",
      "epoch 455, EV = -0.05622911296392742, val  loss = 6.5993577003479 , train loss 6.614569854736328, pcc = 0.46767663955688477\n",
      "epoch 456, EV = -0.048715842397589436, val  loss = 6.508656978607178 , train loss 6.524270153045654, pcc = 0.4688548147678375\n",
      "epoch 457, EV = -0.04692447237801133, val  loss = 6.418072032928467 , train loss 6.432673120498658, pcc = 0.4663867950439453\n",
      "epoch 458, EV = -0.05193661179458886, val  loss = 6.327053165435791 , train loss 6.344568681716919, pcc = 0.47726088762283325\n",
      "epoch 459, EV = -0.04997986398245159, val  loss = 6.236524772644043 , train loss 6.255747365951538, pcc = 0.46927517652511597\n",
      "epoch 460, EV = -0.05075764760636447, val  loss = 6.145897483825683 , train loss 6.161924839019775, pcc = 0.4691833555698395\n",
      "epoch 461, EV = -0.046865576191952356, val  loss = 6.055860805511474 , train loss 6.067993927001953, pcc = 0.4653823971748352\n",
      "epoch 462, EV = -0.043896790659218504, val  loss = 5.965427494049072 , train loss 5.977958631515503, pcc = 0.4683287739753723\n",
      "epoch 463, EV = -0.04911529756428903, val  loss = 5.875028610229492 , train loss 5.889549636840821, pcc = 0.4691734313964844\n",
      "epoch 464, EV = -0.050555913071883354, val  loss = 5.785235500335693 , train loss 5.7995504379272464, pcc = 0.47408056259155273\n",
      "epoch 465, EV = -0.04594595756447106, val  loss = 5.695612907409668 , train loss 5.714414405822754, pcc = 0.47571268677711487\n",
      "epoch 466, EV = -0.052845336366118045, val  loss = 5.606475353240967 , train loss 5.619354677200318, pcc = 0.4696696400642395\n",
      "epoch 467, EV = -0.045736743169918395, val  loss = 5.517660999298096 , train loss 5.534532070159912, pcc = 0.470933198928833\n",
      "epoch 468, EV = -0.0481927075929809, val  loss = 5.428580093383789 , train loss 5.446746301651001, pcc = 0.47709760069847107\n",
      "epoch 469, EV = -0.04922147121345788, val  loss = 5.340553092956543 , train loss 5.357547283172607, pcc = 0.4715500771999359\n",
      "epoch 470, EV = -0.04247890909512838, val  loss = 5.25290355682373 , train loss 5.2680192470550535, pcc = 0.47445836663246155\n",
      "epoch 471, EV = -0.050561915887029546, val  loss = 5.165517520904541 , train loss 5.182109212875366, pcc = 0.4739413857460022\n",
      "epoch 472, EV = -0.0455978042200992, val  loss = 5.078471088409424 , train loss 5.094460487365723, pcc = 0.4764280915260315\n",
      "epoch 473, EV = -0.04356973108492399, val  loss = 4.992236137390137 , train loss 5.002804946899414, pcc = 0.4746595323085785\n",
      "epoch 474, EV = -0.04474787231077228, val  loss = 4.906411933898926 , train loss 4.920321941375732, pcc = 0.4737214148044586\n",
      "epoch 475, EV = -0.04228046536445618, val  loss = 4.8210272789001465 , train loss 4.833227825164795, pcc = 0.47424736618995667\n",
      "epoch 476, EV = -0.0447068360813877, val  loss = 4.73625545501709 , train loss 4.747647285461426, pcc = 0.48056760430336\n",
      "epoch 477, EV = -0.04242241487168429, val  loss = 4.652436637878418 , train loss 4.669860076904297, pcc = 0.47442832589149475\n",
      "epoch 478, EV = -0.04831546806452567, val  loss = 4.569113445281983 , train loss 4.58180046081543, pcc = 0.4708666205406189\n",
      "epoch 479, EV = -0.04342696488949291, val  loss = 4.486364841461182 , train loss 4.499578046798706, pcc = 0.47495007514953613\n",
      "epoch 480, EV = -0.0430543255387691, val  loss = 4.404296779632569 , train loss 4.419036531448365, pcc = 0.4737094044685364\n",
      "epoch 481, EV = -0.04043288293637728, val  loss = 4.322821712493896 , train loss 4.338612985610962, pcc = 0.4771767258644104\n",
      "epoch 482, EV = -0.04557172195953235, val  loss = 4.242414283752441 , train loss 4.256960201263428, pcc = 0.471504271030426\n",
      "epoch 483, EV = -0.04052787025769552, val  loss = 4.162580394744873 , train loss 4.179167795181274, pcc = 0.4754793643951416\n",
      "epoch 484, EV = -0.041933935462382804, val  loss = 4.0834955215454105 , train loss 4.096088218688965, pcc = 0.4716884195804596\n",
      "epoch 485, EV = -0.04436799308709931, val  loss = 4.005313968658447 , train loss 4.021074295043945, pcc = 0.47868409752845764\n",
      "epoch 486, EV = -0.042389310765684696, val  loss = 3.9280235290527346 , train loss 3.9415157318115233, pcc = 0.47307100892066956\n",
      "epoch 487, EV = -0.04281366917124966, val  loss = 3.8520111560821535 , train loss 3.8667564153671266, pcc = 0.46393871307373047\n",
      "epoch 488, EV = -0.045710887825279906, val  loss = 3.7758789539337156 , train loss 3.79220073223114, pcc = 0.4771014153957367\n",
      "epoch 489, EV = -0.046466317093163206, val  loss = 3.7008455276489256 , train loss 3.7238702535629273, pcc = 0.47277283668518066\n",
      "epoch 490, EV = -0.04124853380939417, val  loss = 3.6269827365875242 , train loss 3.641546201705933, pcc = 0.470409095287323\n",
      "epoch 491, EV = -0.04043798436198318, val  loss = 3.553810691833496 , train loss 3.5708933591842653, pcc = 0.4688689708709717\n",
      "epoch 492, EV = -0.04841652878543787, val  loss = 3.4816133975982666 , train loss 3.490924048423767, pcc = 0.4753548502922058\n",
      "epoch 493, EV = -0.04391308520969592, val  loss = 3.410309362411499 , train loss 3.4253546476364134, pcc = 0.47807401418685913\n",
      "epoch 494, EV = -0.04089765956527308, val  loss = 3.339809942245483 , train loss 3.3552857398986817, pcc = 0.48126888275146484\n",
      "epoch 495, EV = -0.044501914266954386, val  loss = 3.271090364456177 , train loss 3.287262225151062, pcc = 0.4634421467781067\n",
      "epoch 496, EV = -0.04300713277699655, val  loss = 3.202222967147827 , train loss 3.212579679489136, pcc = 0.47868189215660095\n",
      "epoch 497, EV = -0.047137524475131115, val  loss = 3.135055351257324 , train loss 3.1513906955718993, pcc = 0.4685947299003601\n",
      "epoch 498, EV = -0.04151327515903272, val  loss = 3.0683945655822753 , train loss 3.0812347412109373, pcc = 0.48027777671813965\n",
      "epoch 499, EV = -0.03807852508728964, val  loss = 3.0030884742736816 , train loss 3.0115748405456544, pcc = 0.4686194062232971\n",
      "epoch 500, EV = -0.038671021398745085, val  loss = 2.938179540634155 , train loss 2.9537926197052, pcc = 0.4783785939216614\n",
      "epoch 501, EV = -0.0524289702114306, val  loss = 2.875459909439087 , train loss 2.8896623849868774, pcc = 0.4637377858161926\n",
      "epoch 502, EV = -0.03253743993608575, val  loss = 2.813059425354004 , train loss 2.827003574371338, pcc = 0.4655857980251312\n",
      "epoch 503, EV = -0.041104652902536225, val  loss = 2.750901126861572 , train loss 2.7646438598632814, pcc = 0.47883445024490356\n",
      "epoch 504, EV = -0.04479863559990598, val  loss = 2.690427827835083 , train loss 2.7049515724182127, pcc = 0.46879106760025024\n",
      "epoch 505, EV = -0.039063071472602975, val  loss = 2.630620765686035 , train loss 2.6406360387802126, pcc = 0.4778822064399719\n",
      "epoch 506, EV = -0.03556216076800698, val  loss = 2.571838617324829 , train loss 2.584733748435974, pcc = 0.4860110878944397\n",
      "epoch 507, EV = -0.038169529877210916, val  loss = 2.5142921924591066 , train loss 2.527687406539917, pcc = 0.4825821816921234\n",
      "epoch 508, EV = -0.03373178003127115, val  loss = 2.4578521728515623 , train loss 2.469986844062805, pcc = 0.4842432141304016\n",
      "epoch 509, EV = -0.03524122269530045, val  loss = 2.4023920059204102 , train loss 2.4154857873916624, pcc = 0.48478221893310547\n",
      "epoch 510, EV = -0.035522463028891046, val  loss = 2.3475882530212404 , train loss 2.3576569080352785, pcc = 0.47688722610473633\n",
      "epoch 511, EV = -0.03349676100831283, val  loss = 2.2940972328186033 , train loss 2.3068538427352907, pcc = 0.4792027473449707\n",
      "epoch 512, EV = -0.03371233009455497, val  loss = 2.241662073135376 , train loss 2.2569133281707763, pcc = 0.4722183346748352\n",
      "epoch 513, EV = -0.030425801611783213, val  loss = 2.189987564086914 , train loss 2.199917936325073, pcc = 0.4736579358577728\n",
      "epoch 514, EV = -0.03363203845526043, val  loss = 2.1396531581878664 , train loss 2.1491257667541506, pcc = 0.46497949957847595\n",
      "epoch 515, EV = -0.049208733596299827, val  loss = 2.0899403572082518 , train loss 2.0988751888275146, pcc = 0.46762815117836\n",
      "epoch 516, EV = -0.044974810721581444, val  loss = 2.040548801422119 , train loss 2.0514432907104494, pcc = 0.4767727255821228\n",
      "epoch 517, EV = -0.03406142456489697, val  loss = 1.992517113685608 , train loss 2.001790487766266, pcc = 0.4760963022708893\n",
      "epoch 518, EV = -0.03272550200161181, val  loss = 1.946140170097351 , train loss 1.9592873454093933, pcc = 0.47062402963638306\n",
      "epoch 519, EV = -0.03231246220438104, val  loss = 1.8996487855911255 , train loss 1.9142153859138489, pcc = 0.4847536087036133\n",
      "epoch 520, EV = -0.03385084867477417, val  loss = 1.8544160366058349 , train loss 1.8669561505317689, pcc = 0.48277145624160767\n",
      "epoch 521, EV = -0.029798950542483413, val  loss = 1.8104021310806275 , train loss 1.8246991395950318, pcc = 0.47875967621803284\n",
      "epoch 522, EV = -0.02839461648673342, val  loss = 1.7679993391036988 , train loss 1.7783811211585998, pcc = 0.46692490577697754\n",
      "epoch 523, EV = -0.03536743709915563, val  loss = 1.7245507955551147 , train loss 1.7375509023666382, pcc = 0.48465457558631897\n",
      "epoch 524, EV = -0.03669409009448269, val  loss = 1.683460283279419 , train loss 1.6966799736022948, pcc = 0.47958412766456604\n",
      "epoch 525, EV = -0.036474990217309246, val  loss = 1.6425897359848023 , train loss 1.6596917986869812, pcc = 0.48226290941238403\n",
      "epoch 526, EV = -0.030747228547146444, val  loss = 1.6029253959655763 , train loss 1.6138441205024718, pcc = 0.4818505644798279\n",
      "epoch 527, EV = -0.04226369240827728, val  loss = 1.5643000841140746 , train loss 1.5745501279830934, pcc = 0.4808274209499359\n",
      "epoch 528, EV = -0.03251351285399052, val  loss = 1.5260540008544923 , train loss 1.5390097737312316, pcc = 0.4850849211215973\n",
      "epoch 529, EV = -0.027522117422338118, val  loss = 1.4894617319107055 , train loss 1.4973289608955382, pcc = 0.4739214777946472\n",
      "epoch 530, EV = -0.03576919802448206, val  loss = 1.4524184226989747 , train loss 1.4686881184577942, pcc = 0.4808630347251892\n",
      "epoch 531, EV = -0.02751700292553818, val  loss = 1.416871190071106 , train loss 1.426021409034729, pcc = 0.4796597361564636\n",
      "epoch 532, EV = -0.03258765214367917, val  loss = 1.3821510076522827 , train loss 1.391272234916687, pcc = 0.48403459787368774\n",
      "epoch 533, EV = -0.029409328050780715, val  loss = 1.3483552932739258 , train loss 1.360304832458496, pcc = 0.47587329149246216\n",
      "epoch 534, EV = -0.02545786269924097, val  loss = 1.3156652450561523 , train loss 1.326660966873169, pcc = 0.4659520983695984\n",
      "epoch 535, EV = -0.04039332375191806, val  loss = 1.282518982887268 , train loss 1.2897467851638793, pcc = 0.4848119616508484\n",
      "epoch 536, EV = -0.03193753382615876, val  loss = 1.250662350654602 , train loss 1.259931755065918, pcc = 0.4827609658241272\n",
      "epoch 537, EV = -0.03452823664012708, val  loss = 1.2195654392242432 , train loss 1.2280960202217102, pcc = 0.486661821603775\n",
      "epoch 538, EV = -0.03119179606437683, val  loss = 1.1892687559127808 , train loss 1.2053138256072997, pcc = 0.4806530475616455\n",
      "epoch 539, EV = -0.02996988672959177, val  loss = 1.159805417060852 , train loss 1.1731520295143127, pcc = 0.48703432083129883\n",
      "epoch 540, EV = -0.03184567993147332, val  loss = 1.130936050415039 , train loss 1.1372171878814696, pcc = 0.47899913787841797\n",
      "epoch 541, EV = -0.029382117484745226, val  loss = 1.1026346683502197 , train loss 1.1131072402000428, pcc = 0.4884103834629059\n",
      "epoch 542, EV = -0.025139916884271724, val  loss = 1.0756285429000854 , train loss 1.0899854063987733, pcc = 0.4771714210510254\n",
      "epoch 543, EV = -0.030732061779289915, val  loss = 1.0486057996749878 , train loss 1.056955921649933, pcc = 0.48354044556617737\n",
      "epoch 544, EV = -0.0261952024802827, val  loss = 1.0223199129104614 , train loss 1.035504198074341, pcc = 0.4814964234828949\n",
      "epoch 545, EV = -0.025662397083483245, val  loss = 0.9972495675086975 , train loss 1.002574807405472, pcc = 0.47906917333602905\n",
      "epoch 546, EV = -0.02384755851929648, val  loss = 0.9732875823974609 , train loss 0.9853696882724762, pcc = 0.45973315834999084\n",
      "epoch 547, EV = -0.0327556196011995, val  loss = 0.9474841713905334 , train loss 0.9585072457790375, pcc = 0.4900704324245453\n",
      "epoch 548, EV = -0.03763667899265624, val  loss = 0.9241998314857482 , train loss 0.9324610948562622, pcc = 0.4785727560520172\n",
      "epoch 549, EV = -0.02797113332832069, val  loss = 0.9007713794708252 , train loss 0.9081430852413177, pcc = 0.48629236221313477\n",
      "epoch 550, EV = -0.028579257036510267, val  loss = 0.8787002801895142 , train loss 0.8890811562538147, pcc = 0.4741548001766205\n",
      "epoch 551, EV = -0.024050556776816386, val  loss = 0.85616375207901 , train loss 0.8646193683147431, pcc = 0.49160853028297424\n",
      "epoch 552, EV = -0.03347168104690418, val  loss = 0.8348453283309937 , train loss 0.8421248853206634, pcc = 0.4822600781917572\n",
      "epoch 553, EV = -0.025004023522661442, val  loss = 0.8137272357940674 , train loss 0.8207171618938446, pcc = 0.49017414450645447\n",
      "epoch 554, EV = -0.022772138578849927, val  loss = 0.7938357472419739 , train loss 0.7956674754619598, pcc = 0.4876982271671295\n",
      "epoch 555, EV = -0.03063738136960749, val  loss = 0.7736025452613831 , train loss 0.7809463679790497, pcc = 0.48934054374694824\n",
      "epoch 556, EV = -0.028203889989016347, val  loss = 0.7542958617210388 , train loss 0.7631824970245361, pcc = 0.4855690002441406\n",
      "epoch 557, EV = -0.02878149298199436, val  loss = 0.7355596423149109 , train loss 0.7470469176769257, pcc = 0.48476219177246094\n",
      "epoch 558, EV = -0.022600439034010236, val  loss = 0.7174859881401062 , train loss 0.7239651024341583, pcc = 0.47875720262527466\n",
      "epoch 559, EV = -0.020713054297263164, val  loss = 0.699719774723053 , train loss 0.7113553345203399, pcc = 0.47782546281814575\n",
      "epoch 560, EV = -0.022837477817870024, val  loss = 0.6822231650352478 , train loss 0.688286691904068, pcc = 0.48664745688438416\n",
      "epoch 561, EV = -0.031176865100860596, val  loss = 0.6650266289710999 , train loss 0.6772025108337403, pcc = 0.49013933539390564\n",
      "epoch 562, EV = -0.023156082421018367, val  loss = 0.648719847202301 , train loss 0.6571944892406464, pcc = 0.48660722374916077\n",
      "epoch 563, EV = -0.030609440908097384, val  loss = 0.6326250314712525 , train loss 0.6365474224090576, pcc = 0.48640871047973633\n",
      "epoch 564, EV = -0.02883214176746837, val  loss = 0.6168927311897278 , train loss 0.6208601057529449, pcc = 0.48743242025375366\n",
      "epoch 565, EV = -0.02823242179134436, val  loss = 0.6017270445823669 , train loss 0.6076996862888336, pcc = 0.485507071018219\n",
      "epoch 566, EV = -0.0303964144305179, val  loss = 0.5868569374084472 , train loss 0.5987554252147674, pcc = 0.48702818155288696\n",
      "epoch 567, EV = -0.02017332221332349, val  loss = 0.5725798368453979 , train loss 0.5790418446063995, pcc = 0.48692822456359863\n",
      "epoch 568, EV = -0.021191439607687164, val  loss = 0.5584503412246704 , train loss 0.5658937692642212, pcc = 0.48429712653160095\n",
      "epoch 569, EV = -0.027049921583711056, val  loss = 0.5446202158927917 , train loss 0.5544474065303803, pcc = 0.4873403012752533\n",
      "epoch 570, EV = -0.024320952202144423, val  loss = 0.531233274936676 , train loss 0.5440456151962281, pcc = 0.4933948516845703\n",
      "epoch 571, EV = -0.02679670066164251, val  loss = 0.5184796690940857 , train loss 0.5263464987277985, pcc = 0.48643413186073303\n",
      "epoch 572, EV = -0.01968522971136528, val  loss = 0.5064846515655518 , train loss 0.514076191186905, pcc = 0.4819651246070862\n",
      "epoch 573, EV = -0.026853648194095546, val  loss = 0.4936309397220612 , train loss 0.5065795421600342, pcc = 0.49244484305381775\n",
      "epoch 574, EV = -0.030070243174569647, val  loss = 0.48167450428009034 , train loss 0.48769989013671877, pcc = 0.49252238869667053\n",
      "epoch 575, EV = -0.02495070873645314, val  loss = 0.4700534164905548 , train loss 0.47858827710151675, pcc = 0.48974594473838806\n",
      "epoch 576, EV = -0.026700796265351146, val  loss = 0.4587725102901459 , train loss 0.4660586804151535, pcc = 0.4890245795249939\n",
      "epoch 577, EV = -0.014268760618410613, val  loss = 0.44840604066848755 , train loss 0.46202991604804994, pcc = 0.48391491174697876\n",
      "epoch 578, EV = -0.035124199955086956, val  loss = 0.43771663308143616 , train loss 0.44685608744621275, pcc = 0.4750848710536957\n",
      "epoch 579, EV = -0.021874735752741497, val  loss = 0.42695499658584596 , train loss 0.4391172528266907, pcc = 0.4930424690246582\n",
      "epoch 580, EV = -0.02520849725656342, val  loss = 0.4168174684047699 , train loss 0.4280684679746628, pcc = 0.48911231756210327\n",
      "epoch 581, EV = -0.02596263979610644, val  loss = 0.40685907006263733 , train loss 0.4168659567832947, pcc = 0.49058571457862854\n",
      "epoch 582, EV = -0.01817446005971808, val  loss = 0.3977455019950867 , train loss 0.40320132076740267, pcc = 0.4879859387874603\n",
      "epoch 583, EV = -0.03474864363670349, val  loss = 0.3883305490016937 , train loss 0.39264496266841886, pcc = 0.49188584089279175\n",
      "epoch 584, EV = -0.021940723322985464, val  loss = 0.37918444275856017 , train loss 0.3813642531633377, pcc = 0.493161141872406\n",
      "epoch 585, EV = -0.02209932344001636, val  loss = 0.3705391466617584 , train loss 0.3758653372526169, pcc = 0.4878873825073242\n",
      "epoch 586, EV = -0.03154795211658143, val  loss = 0.3619180679321289 , train loss 0.37036985754966734, pcc = 0.489225298166275\n",
      "epoch 587, EV = -0.02407401015883998, val  loss = 0.35355414152145387 , train loss 0.3572820633649826, pcc = 0.4920225143432617\n",
      "epoch 588, EV = -0.01571203532971834, val  loss = 0.3458038687705994 , train loss 0.3514315217733383, pcc = 0.4877714216709137\n",
      "epoch 589, EV = -0.024168841148677626, val  loss = 0.33757391571998596 , train loss 0.34461465775966643, pcc = 0.49150270223617554\n",
      "epoch 590, EV = -0.020809861651638097, val  loss = 0.3301417589187622 , train loss 0.3340836435556412, pcc = 0.48800936341285706\n",
      "epoch 591, EV = -0.027464740109025387, val  loss = 0.3228291094303131 , train loss 0.3343019366264343, pcc = 0.48034435510635376\n",
      "epoch 592, EV = -0.017110564729623627, val  loss = 0.3154782593250275 , train loss 0.33249554634094236, pcc = 0.49034208059310913\n",
      "epoch 593, EV = -0.019276868878749378, val  loss = 0.30839107632637025 , train loss 0.3127161651849747, pcc = 0.49357420206069946\n",
      "epoch 594, EV = -0.022374426586586133, val  loss = 0.3017275869846344 , train loss 0.3093538761138916, pcc = 0.4866297245025635\n",
      "epoch 595, EV = -0.018724937710845678, val  loss = 0.29490508437156676 , train loss 0.302792426943779, pcc = 0.4924260973930359\n",
      "epoch 596, EV = -0.016197379220995987, val  loss = 0.2888055324554443 , train loss 0.2957357883453369, pcc = 0.48217397928237915\n",
      "epoch 597, EV = -0.019222669434129147, val  loss = 0.28239613175392153 , train loss 0.2870639741420746, pcc = 0.49014878273010254\n",
      "epoch 598, EV = -0.02270629688313133, val  loss = 0.2761102318763733 , train loss 0.28437240421772003, pcc = 0.4951249957084656\n",
      "epoch 599, EV = -0.021394801244401095, val  loss = 0.2701882541179657 , train loss 0.2814483493566513, pcc = 0.49153590202331543\n",
      "epoch 600, EV = -0.01737155464657566, val  loss = 0.26961973309516907 , train loss 0.27729871571063996, pcc = 0.4937947392463684\n",
      "epoch 601, EV = -0.018494242116024618, val  loss = 0.26894343495368955 , train loss 0.27387265861034393, pcc = 0.4946073591709137\n",
      "epoch 602, EV = -0.020447196144806713, val  loss = 0.26830960512161256 , train loss 0.27823140025138854, pcc = 0.4955473840236664\n",
      "epoch 603, EV = -0.01851096749305725, val  loss = 0.26774842739105226 , train loss 0.27898763716220853, pcc = 0.495421826839447\n",
      "epoch 604, EV = -0.016909481663452953, val  loss = 0.26720046401023867 , train loss 0.2745081216096878, pcc = 0.4941073954105377\n",
      "epoch 605, EV = -0.01727158406324554, val  loss = 0.26658477783203127 , train loss 0.2657673329114914, pcc = 0.4957209527492523\n",
      "epoch 606, EV = -0.018673023110941836, val  loss = 0.26590721011161805 , train loss 0.270694363117218, pcc = 0.49616819620132446\n",
      "epoch 607, EV = -0.01789566194802, val  loss = 0.26530311703681947 , train loss 0.2731238842010498, pcc = 0.4959394037723541\n",
      "epoch 608, EV = -0.016851377068904407, val  loss = 0.2647515058517456 , train loss 0.27367706000804903, pcc = 0.49415501952171326\n",
      "epoch 609, EV = -0.018052542418764347, val  loss = 0.26404566168785093 , train loss 0.26814980804920197, pcc = 0.4957861006259918\n",
      "epoch 610, EV = -0.01915766011204636, val  loss = 0.2634125709533691 , train loss 0.2663943827152252, pcc = 0.4962632656097412\n",
      "epoch 611, EV = -0.016766818991878575, val  loss = 0.2628576934337616 , train loss 0.2704728990793228, pcc = 0.4954226016998291\n",
      "epoch 612, EV = -0.018615280327044036, val  loss = 0.26217964887619016 , train loss 0.2647783488035202, pcc = 0.4962637424468994\n",
      "epoch 613, EV = -0.01698930901393556, val  loss = 0.26159287691116334 , train loss 0.27141850888729097, pcc = 0.495513916015625\n",
      "epoch 614, EV = -0.02031041289630689, val  loss = 0.26087366938591006 , train loss 0.26697376668453215, pcc = 0.4967430531978607\n",
      "epoch 615, EV = -0.02127006859110113, val  loss = 0.2602671027183533 , train loss 0.2693809479475021, pcc = 0.49583402276039124\n",
      "epoch 616, EV = -0.019207992574624848, val  loss = 0.2595736593008041 , train loss 0.265580815076828, pcc = 0.497715562582016\n",
      "epoch 617, EV = -0.017820230701513458, val  loss = 0.25897863507270813 , train loss 0.2594891458749771, pcc = 0.4959529936313629\n",
      "epoch 618, EV = -0.018378505581303647, val  loss = 0.25829375982284547 , train loss 0.26478455066680906, pcc = 0.49703139066696167\n",
      "epoch 619, EV = -0.02096445884620934, val  loss = 0.2576420783996582 , train loss 0.26322761327028277, pcc = 0.49726954102516174\n",
      "epoch 620, EV = -0.01937664496271234, val  loss = 0.2569776177406311 , train loss 0.2670115366578102, pcc = 0.4969630241394043\n",
      "epoch 621, EV = -0.016838849636546353, val  loss = 0.2564020216464996 , train loss 0.26287844628095625, pcc = 0.49589866399765015\n",
      "epoch 622, EV = -0.01702681340669331, val  loss = 0.2557461529970169 , train loss 0.26038212329149246, pcc = 0.4969726502895355\n",
      "epoch 623, EV = -0.019090901864202398, val  loss = 0.255029159784317 , train loss 0.25965303778648374, pcc = 0.49762874841690063\n",
      "epoch 624, EV = -0.017226671440559522, val  loss = 0.25440987646579744 , train loss 0.25984596461057663, pcc = 0.49548664689064026\n",
      "epoch 625, EV = -0.01976513182907774, val  loss = 0.2536757975816727 , train loss 0.2563175320625305, pcc = 0.49719947576522827\n",
      "epoch 626, EV = -0.018927870612395436, val  loss = 0.25304287374019624 , train loss 0.2548001378774643, pcc = 0.496123731136322\n",
      "epoch 627, EV = -0.016845397781907467, val  loss = 0.25241161584854127 , train loss 0.253082649409771, pcc = 0.4958149492740631\n",
      "epoch 628, EV = -0.015591289390597427, val  loss = 0.2518663227558136 , train loss 0.25843982249498365, pcc = 0.49308356642723083\n",
      "epoch 629, EV = -0.018100087579927947, val  loss = 0.25111559629440305 , train loss 0.2580633357167244, pcc = 0.4948316514492035\n",
      "epoch 630, EV = -0.018288681904474895, val  loss = 0.2503668576478958 , train loss 0.2596258208155632, pcc = 0.49694880843162537\n",
      "epoch 631, EV = -0.019881241154252438, val  loss = 0.24969303011894226 , train loss 0.2535589084029198, pcc = 0.49676695466041565\n",
      "epoch 632, EV = -0.01846632099988168, val  loss = 0.24902417063713073 , train loss 0.25359862446784975, pcc = 0.4962373375892639\n",
      "epoch 633, EV = -0.020925774908902354, val  loss = 0.24830907583236694 , train loss 0.25445697605609896, pcc = 0.4974195957183838\n",
      "epoch 634, EV = -0.017599735343665407, val  loss = 0.24768876731395723 , train loss 0.2541067138314247, pcc = 0.4962480068206787\n",
      "epoch 635, EV = -0.01660060359720598, val  loss = 0.24706103205680846 , train loss 0.2552312403917313, pcc = 0.4953235983848572\n",
      "epoch 636, EV = -0.01822911019910846, val  loss = 0.24636503756046296 , train loss 0.25096057802438737, pcc = 0.49535322189331055\n",
      "epoch 637, EV = -0.01763448066878737, val  loss = 0.24567690789699553 , train loss 0.25051259696483613, pcc = 0.4960414469242096\n",
      "epoch 638, EV = -0.019296727159567047, val  loss = 0.24497604966163636 , train loss 0.25050178617239, pcc = 0.49614399671554565\n",
      "epoch 639, EV = -0.018097524057354843, val  loss = 0.24431445598602294 , train loss 0.25073141753673556, pcc = 0.4964962899684906\n",
      "epoch 640, EV = -0.017717335307807253, val  loss = 0.24365475177764892 , train loss 0.25003473907709123, pcc = 0.49575337767601013\n",
      "epoch 641, EV = -0.017895840239106564, val  loss = 0.24296798706054687 , train loss 0.2533106207847595, pcc = 0.49656692147254944\n",
      "epoch 642, EV = -0.016875541001035457, val  loss = 0.2423449456691742 , train loss 0.24700606912374495, pcc = 0.4949605464935303\n",
      "epoch 643, EV = -0.017555710516477887, val  loss = 0.24163052141666413 , train loss 0.24430715292692184, pcc = 0.49722665548324585\n",
      "epoch 644, EV = -0.01776481302160966, val  loss = 0.2409605711698532 , train loss 0.24254321604967116, pcc = 0.4958213269710541\n",
      "epoch 645, EV = -0.020475277252364577, val  loss = 0.2402190685272217 , train loss 0.24880054146051406, pcc = 0.4972667694091797\n",
      "epoch 646, EV = -0.017045781800621433, val  loss = 0.23963671326637268 , train loss 0.24550230652093888, pcc = 0.4962612986564636\n",
      "epoch 647, EV = -0.016885377335966678, val  loss = 0.23899349570274353 , train loss 0.24460717290639877, pcc = 0.4959107041358948\n",
      "epoch 648, EV = -0.019149697140643473, val  loss = 0.2382104367017746 , train loss 0.2434625044465065, pcc = 0.4969917833805084\n",
      "epoch 649, EV = -0.017498955914848728, val  loss = 0.23757852613925934 , train loss 0.24383718967437745, pcc = 0.49665823578834534\n",
      "epoch 650, EV = -0.019202400717818945, val  loss = 0.2368692398071289 , train loss 0.24291523545980453, pcc = 0.4965627193450928\n",
      "epoch 651, EV = -0.017521916251433522, val  loss = 0.23623064756393433 , train loss 0.23929672092199325, pcc = 0.4975601136684418\n",
      "epoch 652, EV = -0.019799289473316127, val  loss = 0.2354869067668915 , train loss 0.24081920385360717, pcc = 0.4986056387424469\n",
      "epoch 653, EV = -0.019147017545867385, val  loss = 0.2348819613456726 , train loss 0.2390956848859787, pcc = 0.49667930603027344\n",
      "epoch 654, EV = -0.017792476373806335, val  loss = 0.23422249853610994 , train loss 0.24405227303504945, pcc = 0.4962754249572754\n",
      "epoch 655, EV = -0.018767543529209337, val  loss = 0.23357557356357575 , train loss 0.24488091021776198, pcc = 0.49591565132141113\n",
      "epoch 656, EV = -0.017799150002630132, val  loss = 0.23290827572345735 , train loss 0.2456981435418129, pcc = 0.4965897500514984\n",
      "epoch 657, EV = -0.01628829617249338, val  loss = 0.23232823312282563 , train loss 0.2369592934846878, pcc = 0.4938851296901703\n",
      "epoch 658, EV = -0.020111712447383946, val  loss = 0.23155737519264222 , train loss 0.23745802640914918, pcc = 0.4965626299381256\n",
      "epoch 659, EV = -0.017295204233704953, val  loss = 0.230987149477005 , train loss 0.24079191982746123, pcc = 0.49467039108276367\n",
      "epoch 660, EV = -0.018610705931981403, val  loss = 0.2302427589893341 , train loss 0.2358692094683647, pcc = 0.49662843346595764\n",
      "epoch 661, EV = -0.019326761626360708, val  loss = 0.22960939705371858 , train loss 0.23207734078168868, pcc = 0.4965095520019531\n",
      "epoch 662, EV = -0.016114014805408947, val  loss = 0.2290847420692444 , train loss 0.242013081908226, pcc = 0.49424371123313904\n",
      "epoch 663, EV = -0.018805102820982012, val  loss = 0.2283017009496689 , train loss 0.2352633610367775, pcc = 0.49718064069747925\n",
      "epoch 664, EV = -0.01714802154323511, val  loss = 0.22772165536880493 , train loss 0.2277713969349861, pcc = 0.4956391751766205\n",
      "epoch 665, EV = -0.019924150747165345, val  loss = 0.22697335481643677 , train loss 0.23642715513706208, pcc = 0.4960978031158447\n",
      "epoch 666, EV = -0.01462778553628085, val  loss = 0.22656029164791108 , train loss 0.23290736079216004, pcc = 0.4915500581264496\n",
      "epoch 667, EV = -0.017551855037086887, val  loss = 0.22574710845947266 , train loss 0.23217159509658813, pcc = 0.4950782358646393\n",
      "epoch 668, EV = -0.01702475861499184, val  loss = 0.22509608268737794 , train loss 0.23514287620782853, pcc = 0.4969107508659363\n",
      "epoch 669, EV = -0.0188438730281696, val  loss = 0.22443844079971315 , train loss 0.2300793558359146, pcc = 0.4958187937736511\n",
      "epoch 670, EV = -0.017070827776925604, val  loss = 0.2239176243543625 , train loss 0.22981551587581633, pcc = 0.49289369583129883\n",
      "epoch 671, EV = -0.018002665879433614, val  loss = 0.22321085631847382 , train loss 0.2274869978427887, pcc = 0.49612027406692505\n",
      "epoch 672, EV = -0.016226416094261304, val  loss = 0.22263184189796448 , train loss 0.2235411986708641, pcc = 0.49579867720603943\n",
      "epoch 673, EV = -0.02035224908276608, val  loss = 0.22188349366188048 , train loss 0.22350757718086242, pcc = 0.49807634949684143\n",
      "epoch 674, EV = -0.019263742785704762, val  loss = 0.22124169170856475 , train loss 0.22336835712194442, pcc = 0.4978337287902832\n",
      "epoch 675, EV = -0.01836050131864715, val  loss = 0.22064465284347534 , train loss 0.22581867575645448, pcc = 0.4962247908115387\n",
      "epoch 676, EV = -0.01699424992527878, val  loss = 0.2201276034116745 , train loss 0.2244771733880043, pcc = 0.4952406883239746\n",
      "epoch 677, EV = -0.016475677490234375, val  loss = 0.2194944828748703 , train loss 0.2196819618344307, pcc = 0.4959423542022705\n",
      "epoch 678, EV = -0.018034612400489942, val  loss = 0.21882243752479552 , train loss 0.22354312390089034, pcc = 0.49524998664855957\n",
      "epoch 679, EV = -0.015459658806784111, val  loss = 0.2183257281780243 , train loss 0.22024734914302826, pcc = 0.4937230050563812\n",
      "epoch 680, EV = -0.015609734413916604, val  loss = 0.2177405297756195 , train loss 0.226527664065361, pcc = 0.4928978979587555\n",
      "epoch 681, EV = -0.017884663845363417, val  loss = 0.2170090913772583 , train loss 0.22249422669410707, pcc = 0.4953920245170593\n",
      "epoch 682, EV = -0.019452765322568125, val  loss = 0.21631680727005004 , train loss 0.22900085151195526, pcc = 0.49777477979660034\n",
      "epoch 683, EV = -0.0182061749592162, val  loss = 0.21576571166515351 , train loss 0.2230255350470543, pcc = 0.4965790808200836\n",
      "epoch 684, EV = -0.017398509540055927, val  loss = 0.21520239412784575 , train loss 0.21843046844005584, pcc = 0.4947214722633362\n",
      "epoch 685, EV = -0.02009733413395129, val  loss = 0.21453585028648375 , train loss 0.2231494665145874, pcc = 0.496878057718277\n",
      "epoch 686, EV = -0.017927566118407668, val  loss = 0.21395555436611174 , train loss 0.22190254032611847, pcc = 0.4975283741950989\n",
      "epoch 687, EV = -0.017915846485840648, val  loss = 0.21336210668087005 , train loss 0.2205003708600998, pcc = 0.4973575472831726\n",
      "epoch 688, EV = -0.019289740344934296, val  loss = 0.21274588704109193 , train loss 0.21792291551828386, pcc = 0.49760445952415466\n",
      "epoch 689, EV = -0.017444456355613574, val  loss = 0.21227555572986603 , train loss 0.21784460842609404, pcc = 0.4947710931301117\n",
      "epoch 690, EV = -0.01832863636184157, val  loss = 0.21166584491729737 , train loss 0.2176076963543892, pcc = 0.4955673813819885\n",
      "epoch 691, EV = -0.015801997561203807, val  loss = 0.21113260388374328 , train loss 0.2174480065703392, pcc = 0.4930395483970642\n",
      "epoch 692, EV = -0.016934137595327275, val  loss = 0.21048617064952851 , train loss 0.21780854910612107, pcc = 0.49577346444129944\n",
      "epoch 693, EV = -0.01955612866502059, val  loss = 0.2098925828933716 , train loss 0.21458670645952224, pcc = 0.4947642385959625\n",
      "epoch 694, EV = -0.019787098232068513, val  loss = 0.2092682808637619 , train loss 0.21420053094625474, pcc = 0.4973461627960205\n",
      "epoch 695, EV = -0.01677169454725165, val  loss = 0.20878754556179047 , train loss 0.21640172004699706, pcc = 0.4948606491088867\n",
      "epoch 696, EV = -0.017224992053550586, val  loss = 0.20820534229278564 , train loss 0.21479131579399108, pcc = 0.49461373686790466\n",
      "epoch 697, EV = -0.019313389794868335, val  loss = 0.20755251348018647 , train loss 0.21596465706825257, pcc = 0.49630099534988403\n",
      "epoch 698, EV = -0.016395056456850285, val  loss = 0.20707494914531707 , train loss 0.21312488466501237, pcc = 0.49632248282432556\n",
      "epoch 699, EV = -0.016523639883911402, val  loss = 0.20653509795665742 , train loss 0.2084337741136551, pcc = 0.4949764609336853\n",
      "epoch 700, EV = -0.019916852315266926, val  loss = 0.20591083765029908 , train loss 0.208100588619709, pcc = 0.49604660272598267\n",
      "epoch 701, EV = -0.018745014019179763, val  loss = 0.20532744228839875 , train loss 0.21158839464187623, pcc = 0.4968486726284027\n",
      "epoch 702, EV = -0.015688057531390274, val  loss = 0.20496813654899598 , train loss 0.21083047688007356, pcc = 0.49145403504371643\n",
      "epoch 703, EV = -0.020048803404757853, val  loss = 0.20424464643001555 , train loss 0.20956573486328126, pcc = 0.4964706301689148\n",
      "epoch 704, EV = -0.018467349441427933, val  loss = 0.203697806596756 , train loss 0.21460011303424836, pcc = 0.4967934191226959\n",
      "epoch 705, EV = -0.019308150337453475, val  loss = 0.2031496912240982 , train loss 0.205864217877388, pcc = 0.4964224398136139\n",
      "epoch 706, EV = -0.018456005213553447, val  loss = 0.20259287655353547 , train loss 0.2076561614871025, pcc = 0.4968334138393402\n",
      "epoch 707, EV = -0.017060827267797368, val  loss = 0.20211573243141173 , train loss 0.2055842325091362, pcc = 0.4951063096523285\n",
      "epoch 708, EV = -0.019103157415724638, val  loss = 0.20160202980041503 , train loss 0.204254087805748, pcc = 0.4953538775444031\n",
      "epoch 709, EV = -0.019868758163954083, val  loss = 0.20096466541290284 , train loss 0.2037598431110382, pcc = 0.4963759779930115\n",
      "epoch 710, EV = -0.020174921081777205, val  loss = 0.20047472715377807 , train loss 0.208258718252182, pcc = 0.49540776014328003\n",
      "epoch 711, EV = -0.01618256276113945, val  loss = 0.20002118945121766 , train loss 0.20718820244073868, pcc = 0.49381256103515625\n",
      "epoch 712, EV = -0.0183080277944866, val  loss = 0.19940997362136842 , train loss 0.2067703127861023, pcc = 0.49514445662498474\n",
      "epoch 713, EV = -0.018560166944537246, val  loss = 0.19889146387577056 , train loss 0.2076295867562294, pcc = 0.4938827455043793\n",
      "epoch 714, EV = -0.01934379496072468, val  loss = 0.19833193421363832 , train loss 0.20341919362545013, pcc = 0.4957670271396637\n",
      "epoch 715, EV = -0.01533726223728113, val  loss = 0.19802038967609406 , train loss 0.20046185106039047, pcc = 0.4910067915916443\n",
      "epoch 716, EV = -0.016406166448927763, val  loss = 0.19746506512165068 , train loss 0.20099723786115647, pcc = 0.4927919805049896\n",
      "epoch 717, EV = -0.018973893763726216, val  loss = 0.19675137400627135 , train loss 0.2056845799088478, pcc = 0.4977245330810547\n",
      "epoch 718, EV = -0.019677241643269856, val  loss = 0.19631375074386598 , train loss 0.2003715768456459, pcc = 0.4939918518066406\n",
      "epoch 719, EV = -0.016767049044893498, val  loss = 0.19581914246082305 , train loss 0.20614107847213745, pcc = 0.49509698152542114\n",
      "epoch 720, EV = -0.017004496695702535, val  loss = 0.19530956745147704 , train loss 0.19928079545497895, pcc = 0.4951651692390442\n",
      "epoch 721, EV = -0.018479685511505396, val  loss = 0.1947846919298172 , train loss 0.19744801968336106, pcc = 0.4958210587501526\n",
      "epoch 722, EV = -0.01739460177588881, val  loss = 0.1942995846271515 , train loss 0.20447517335414886, pcc = 0.49408769607543945\n",
      "epoch 723, EV = -0.0179486238119895, val  loss = 0.19373439252376556 , train loss 0.2005789801478386, pcc = 0.4965716302394867\n",
      "epoch 724, EV = -0.01859459594676369, val  loss = 0.19322031140327453 , train loss 0.19500425308942795, pcc = 0.49724632501602173\n",
      "epoch 725, EV = -0.018635530743682592, val  loss = 0.19277983009815217 , train loss 0.19834225475788117, pcc = 0.4960536062717438\n",
      "epoch 726, EV = -0.018171846343759904, val  loss = 0.1922515332698822 , train loss 0.19358746856451034, pcc = 0.49743422865867615\n",
      "epoch 727, EV = -0.019365696007745306, val  loss = 0.19172584712505342 , train loss 0.19724587947130204, pcc = 0.4969698488712311\n",
      "epoch 728, EV = -0.017504067797409862, val  loss = 0.19134514927864074 , train loss 0.19439531713724137, pcc = 0.494988352060318\n",
      "epoch 729, EV = -0.016822271702582377, val  loss = 0.19090572297573088 , train loss 0.19459018856287003, pcc = 0.4936721920967102\n",
      "epoch 730, EV = -0.01918177332794457, val  loss = 0.19026866257190705 , train loss 0.1953279674053192, pcc = 0.49668222665786743\n",
      "epoch 731, EV = -0.016689889786536235, val  loss = 0.18989917635917664 , train loss 0.19794888496398927, pcc = 0.4935974180698395\n",
      "epoch 732, EV = -0.017709812574219285, val  loss = 0.1893604427576065 , train loss 0.1931345656514168, pcc = 0.49551495909690857\n",
      "epoch 733, EV = -0.015444564191918624, val  loss = 0.18901985585689546 , train loss 0.19466855078935624, pcc = 0.4922729432582855\n",
      "epoch 734, EV = -0.019033295020722506, val  loss = 0.18834785521030425 , train loss 0.19693876057863235, pcc = 0.4967666566371918\n",
      "epoch 735, EV = -0.01728851596514384, val  loss = 0.1879247933626175 , train loss 0.19107310324907303, pcc = 0.4969400465488434\n",
      "epoch 736, EV = -0.01563479555280585, val  loss = 0.18759076595306395 , train loss 0.19608260840177535, pcc = 0.4923054277896881\n",
      "epoch 737, EV = -0.018403480450312298, val  loss = 0.18702429234981538 , train loss 0.1910927787423134, pcc = 0.4951387345790863\n",
      "epoch 738, EV = -0.01626774930117423, val  loss = 0.1866179883480072 , train loss 0.19153202027082444, pcc = 0.4949675500392914\n",
      "epoch 739, EV = -0.01714981804814255, val  loss = 0.18607966005802154 , train loss 0.19098722636699678, pcc = 0.4939134120941162\n",
      "epoch 740, EV = -0.019276890315507587, val  loss = 0.18555619120597838 , train loss 0.19152920693159103, pcc = 0.496534526348114\n",
      "epoch 741, EV = -0.01802519538946319, val  loss = 0.18513672947883605 , train loss 0.19642731845378875, pcc = 0.49480730295181274\n",
      "epoch 742, EV = -0.016716991600237395, val  loss = 0.1846846044063568 , train loss 0.18701963871717453, pcc = 0.4948040843009949\n",
      "epoch 743, EV = -0.018508966031827424, val  loss = 0.18424567878246306 , train loss 0.18602863550186158, pcc = 0.4933982789516449\n",
      "epoch 744, EV = -0.019185415485449005, val  loss = 0.18373202979564668 , train loss 0.19670167267322541, pcc = 0.49647942185401917\n",
      "epoch 745, EV = -0.01799764026675308, val  loss = 0.1833108365535736 , train loss 0.19152743369340897, pcc = 0.49535244703292847\n",
      "epoch 746, EV = -0.0161785436303992, val  loss = 0.18296574652194977 , train loss 0.18647759407758713, pcc = 0.4936390519142151\n",
      "epoch 747, EV = -0.020223809961687055, val  loss = 0.18233889043331147 , train loss 0.1887890562415123, pcc = 0.49746084213256836\n",
      "epoch 748, EV = -0.01943289658479523, val  loss = 0.1819185733795166 , train loss 0.18767150193452836, pcc = 0.49693888425827026\n",
      "epoch 749, EV = -0.01854310380785089, val  loss = 0.18147756159305573 , train loss 0.1869912713766098, pcc = 0.4975101947784424\n",
      "epoch 750, EV = -0.015829717380958692, val  loss = 0.18115985989570618 , train loss 0.1940370559692383, pcc = 0.4939708709716797\n",
      "epoch 751, EV = -0.018305348722558273, val  loss = 0.18062290251255037 , train loss 0.18818599730730057, pcc = 0.49537819623947144\n",
      "epoch 752, EV = -0.018383205459828963, val  loss = 0.18013937175273895 , train loss 0.18622058629989624, pcc = 0.49814143776893616\n",
      "epoch 753, EV = -0.01680522686556766, val  loss = 0.17979556918144227 , train loss 0.18918847441673278, pcc = 0.4947340488433838\n",
      "epoch 754, EV = -0.01836959207267092, val  loss = 0.1792762905359268 , train loss 0.18885647058486937, pcc = 0.4972022473812103\n",
      "epoch 755, EV = -0.016759297826833892, val  loss = 0.17890035212039948 , train loss 0.18624932914972306, pcc = 0.4946911931037903\n",
      "epoch 756, EV = -0.019577920959706892, val  loss = 0.17836996018886567 , train loss 0.18351449966430664, pcc = 0.4979628622531891\n",
      "epoch 757, EV = -0.015748121759347748, val  loss = 0.1781398594379425 , train loss 0.18377015590667725, pcc = 0.4939846098423004\n",
      "epoch 758, EV = -0.01902854285742107, val  loss = 0.17757514715194703 , train loss 0.18035470098257064, pcc = 0.49594154953956604\n",
      "epoch 759, EV = -0.017430241693530166, val  loss = 0.17716208398342131 , train loss 0.18787222504615783, pcc = 0.49573206901550293\n",
      "epoch 760, EV = -0.01852619857118841, val  loss = 0.17668436765670775 , train loss 0.18062746971845628, pcc = 0.4974099099636078\n",
      "epoch 761, EV = -0.018480684150729263, val  loss = 0.1763137549161911 , train loss 0.1793854907155037, pcc = 0.495462030172348\n",
      "epoch 762, EV = -0.021008827707223725, val  loss = 0.1758040428161621 , train loss 0.18373430520296097, pcc = 0.49906760454177856\n",
      "epoch 763, EV = -0.015926273245560497, val  loss = 0.17556430995464326 , train loss 0.1827564939856529, pcc = 0.49378353357315063\n",
      "epoch 764, EV = -0.018753656169824433, val  loss = 0.17505475580692292 , train loss 0.18122258186340331, pcc = 0.4949633479118347\n",
      "epoch 765, EV = -0.017470813111255045, val  loss = 0.174670073390007 , train loss 0.1834772825241089, pcc = 0.4945451319217682\n",
      "epoch 766, EV = -0.01937809952518396, val  loss = 0.17417107224464418 , train loss 0.18008933365345, pcc = 0.4971277713775635\n",
      "epoch 767, EV = -0.01876612190614667, val  loss = 0.17377031743526458 , train loss 0.17648539394140245, pcc = 0.49729403853416443\n",
      "epoch 768, EV = -0.017791344408403364, val  loss = 0.17341829240322112 , train loss 0.18258856236934662, pcc = 0.4947086274623871\n",
      "epoch 769, EV = -0.020038552974399767, val  loss = 0.17296184003353118 , train loss 0.176001138985157, pcc = 0.49681878089904785\n",
      "epoch 770, EV = -0.01813250884675143, val  loss = 0.1725861459970474 , train loss 0.17632533758878707, pcc = 0.4974598288536072\n",
      "epoch 771, EV = -0.019059799742280392, val  loss = 0.17213935852050782 , train loss 0.17867531329393388, pcc = 0.4986303448677063\n",
      "epoch 772, EV = -0.018062890098806014, val  loss = 0.17176916003227233 , train loss 0.17759411484003068, pcc = 0.49644285440444946\n",
      "epoch 773, EV = -0.01636094796030145, val  loss = 0.17144160866737365 , train loss 0.17824739962816238, pcc = 0.49542373418807983\n",
      "epoch 774, EV = -0.01747531535332663, val  loss = 0.1709737539291382 , train loss 0.1720712661743164, pcc = 0.4950709342956543\n",
      "epoch 775, EV = -0.01865516367711519, val  loss = 0.17055141031742097 , train loss 0.18031997829675675, pcc = 0.4967074394226074\n",
      "epoch 776, EV = -0.016305452898928995, val  loss = 0.17023752629756927 , train loss 0.17335097789764403, pcc = 0.4949246048927307\n",
      "epoch 777, EV = -0.019673616739741544, val  loss = 0.16974103450775146 , train loss 0.174222032725811, pcc = 0.4976147711277008\n",
      "epoch 778, EV = -0.01605252372591119, val  loss = 0.16946637034416198 , train loss 0.17481012642383575, pcc = 0.49387845396995544\n",
      "epoch 779, EV = -0.018677384184117903, val  loss = 0.168948757648468 , train loss 0.17215016335248948, pcc = 0.4968504309654236\n",
      "epoch 780, EV = -0.017652331214202076, val  loss = 0.16859129071235657 , train loss 0.17634243071079253, pcc = 0.4973452687263489\n",
      "epoch 781, EV = -0.016028950088902524, val  loss = 0.16825633943080903 , train loss 0.17704458832740783, pcc = 0.4960772395133972\n",
      "epoch 782, EV = -0.017503933948382996, val  loss = 0.16784596145153047 , train loss 0.17237799167633056, pcc = 0.4948839545249939\n",
      "epoch 783, EV = -0.01704321252672296, val  loss = 0.16748388409614562 , train loss 0.17144973427057267, pcc = 0.4955471158027649\n",
      "epoch 784, EV = -0.018114532295026277, val  loss = 0.1669849544763565 , train loss 0.17568579763174058, pcc = 0.498069703578949\n",
      "epoch 785, EV = -0.018812316028695358, val  loss = 0.1665870487689972 , train loss 0.17531168162822724, pcc = 0.4992886483669281\n",
      "epoch 786, EV = -0.017672963832554064, val  loss = 0.16625821888446807 , train loss 0.17717462480068208, pcc = 0.4972497522830963\n",
      "epoch 787, EV = -0.017776137381269222, val  loss = 0.16590383350849153 , train loss 0.17187100499868393, pcc = 0.4960590898990631\n",
      "epoch 788, EV = -0.016554402154788636, val  loss = 0.1655360996723175 , train loss 0.170196533203125, pcc = 0.4974592626094818\n",
      "epoch 789, EV = -0.01843807153534471, val  loss = 0.16511695683002472 , train loss 0.16662534475326538, pcc = 0.4982035756111145\n",
      "epoch 790, EV = -0.01327828513948541, val  loss = 0.16506246030330657 , train loss 0.1720690757036209, pcc = 0.4910582900047302\n",
      "epoch 791, EV = -0.017326622678522478, val  loss = 0.16439626216888428 , train loss 0.17172716408967972, pcc = 0.4953671991825104\n",
      "epoch 792, EV = -0.018492712263475385, val  loss = 0.16402013301849366 , train loss 0.16797783821821213, pcc = 0.49586981534957886\n",
      "epoch 793, EV = -0.017650600065264786, val  loss = 0.16370544731616973 , train loss 0.17693295180797577, pcc = 0.49421319365501404\n",
      "epoch 794, EV = -0.017878847686867965, val  loss = 0.16325887143611909 , train loss 0.1728157013654709, pcc = 0.49781686067581177\n",
      "epoch 795, EV = -0.016325754554648148, val  loss = 0.16296955943107605 , train loss 0.16700530648231507, pcc = 0.4959685802459717\n",
      "epoch 796, EV = -0.018164543206231634, val  loss = 0.1625057578086853 , train loss 0.1668485403060913, pcc = 0.4989583492279053\n",
      "epoch 797, EV = -0.020222113320702, val  loss = 0.1621071070432663 , train loss 0.16625225245952607, pcc = 0.49996185302734375\n",
      "epoch 798, EV = -0.017442197130437483, val  loss = 0.16177778840065002 , train loss 0.16653931140899658, pcc = 0.49814504384994507\n",
      "epoch 799, EV = -0.01757825257485373, val  loss = 0.16145441830158233 , train loss 0.16653211712837218, pcc = 0.498224675655365\n",
      "epoch 800, EV = -0.017049031299457215, val  loss = 0.1614197552204132 , train loss 0.1672595262527466, pcc = 0.4979715943336487\n",
      "epoch 801, EV = -0.017158237988488714, val  loss = 0.16138615012168883 , train loss 0.1640775591135025, pcc = 0.497549444437027\n",
      "epoch 802, EV = -0.01671722845027321, val  loss = 0.16136134266853333 , train loss 0.16297949999570846, pcc = 0.4970862865447998\n",
      "epoch 803, EV = -0.01666051985924704, val  loss = 0.1613345265388489 , train loss 0.16317178457975387, pcc = 0.49670839309692383\n",
      "epoch 804, EV = -0.016678366221879657, val  loss = 0.1612934559583664 , train loss 0.1633577361702919, pcc = 0.4970085024833679\n",
      "epoch 805, EV = -0.016894761930432236, val  loss = 0.16125044822692872 , train loss 0.16622748225927353, pcc = 0.49693217873573303\n",
      "epoch 806, EV = -0.016779593208379913, val  loss = 0.1612185537815094 , train loss 0.16635829359292983, pcc = 0.4969349801540375\n",
      "epoch 807, EV = -0.017027909295600756, val  loss = 0.16117398738861083 , train loss 0.16560017615556716, pcc = 0.49697932600975037\n",
      "epoch 808, EV = -0.017106255418375918, val  loss = 0.1611362725496292 , train loss 0.1708778589963913, pcc = 0.49702325463294983\n",
      "epoch 809, EV = -0.01699571076192354, val  loss = 0.16110222041606903 , train loss 0.1718778058886528, pcc = 0.4969728887081146\n",
      "epoch 810, EV = -0.01685888411705954, val  loss = 0.16107654869556426 , train loss 0.16650014221668244, pcc = 0.49648889899253845\n",
      "epoch 811, EV = -0.01699204873620418, val  loss = 0.1610309362411499 , train loss 0.16455836445093155, pcc = 0.49694982171058655\n",
      "epoch 812, EV = -0.0171519094391873, val  loss = 0.1609898626804352 , train loss 0.16614841669797897, pcc = 0.49696117639541626\n",
      "epoch 813, EV = -0.016677825074446827, val  loss = 0.160972660779953 , train loss 0.17346639633178712, pcc = 0.49654093384742737\n",
      "epoch 814, EV = -0.017338210030605917, val  loss = 0.16091233491897583 , train loss 0.16807484328746797, pcc = 0.49698829650878906\n",
      "epoch 815, EV = -0.016656756923909773, val  loss = 0.1608948290348053 , train loss 0.16453741490840912, pcc = 0.4965384006500244\n",
      "epoch 816, EV = -0.01684435679201494, val  loss = 0.16085584461688995 , train loss 0.16707912236452102, pcc = 0.49657660722732544\n",
      "epoch 817, EV = -0.016802099713107997, val  loss = 0.16081680357456207 , train loss 0.16476893275976182, pcc = 0.4966430068016052\n",
      "epoch 818, EV = -0.016830446427328546, val  loss = 0.16078196763992308 , train loss 0.1683468222618103, pcc = 0.49665719270706177\n",
      "epoch 819, EV = -0.016771258492218822, val  loss = 0.16074275374412536 , train loss 0.16565041989088058, pcc = 0.4967133104801178\n",
      "epoch 820, EV = -0.016887733810826352, val  loss = 0.1607102632522583 , train loss 0.16737359762191772, pcc = 0.496471107006073\n",
      "epoch 821, EV = -0.016997663075463812, val  loss = 0.16066847443580629 , train loss 0.16914954632520676, pcc = 0.4966951012611389\n",
      "epoch 822, EV = -0.01704649810205426, val  loss = 0.16063061356544495 , train loss 0.17073207646608352, pcc = 0.49667224287986755\n",
      "epoch 823, EV = -0.01681213577588399, val  loss = 0.160592982172966 , train loss 0.17081358730793, pcc = 0.4966896176338196\n",
      "epoch 824, EV = -0.016796294011567767, val  loss = 0.1605559766292572 , train loss 0.16151410192251206, pcc = 0.49672240018844604\n",
      "epoch 825, EV = -0.016845674368373136, val  loss = 0.16051921248435974 , train loss 0.1629372090101242, pcc = 0.4966614246368408\n",
      "epoch 826, EV = -0.017073271567361395, val  loss = 0.16047710180282593 , train loss 0.16798412501811982, pcc = 0.4968770146369934\n",
      "epoch 827, EV = -0.016968233543529845, val  loss = 0.16044149398803711 , train loss 0.16748160868883133, pcc = 0.49677222967147827\n",
      "epoch 828, EV = -0.017007630122335332, val  loss = 0.16040521264076232 , train loss 0.1633090928196907, pcc = 0.4967270791530609\n",
      "epoch 829, EV = -0.017114676927265367, val  loss = 0.16036397218704224 , train loss 0.16648097038269044, pcc = 0.49693137407302856\n",
      "epoch 830, EV = -0.017060829882036176, val  loss = 0.1603303909301758 , train loss 0.16188997328281401, pcc = 0.49674639105796814\n",
      "epoch 831, EV = -0.016749169220004165, val  loss = 0.1603001981973648 , train loss 0.16398519277572632, pcc = 0.4966401159763336\n",
      "epoch 832, EV = -0.017109453678131104, val  loss = 0.16025260388851165 , train loss 0.16656184196472168, pcc = 0.49688011407852173\n",
      "epoch 833, EV = -0.01669750872411226, val  loss = 0.1602266699075699 , train loss 0.16200639456510543, pcc = 0.4965916574001312\n",
      "epoch 834, EV = -0.016892296180390474, val  loss = 0.16018544435501098 , train loss 0.16468028128147125, pcc = 0.49672308564186096\n",
      "epoch 835, EV = -0.016919222317243878, val  loss = 0.16014550924301146 , train loss 0.16715555638074875, pcc = 0.49690356850624084\n",
      "epoch 836, EV = -0.01690449369581122, val  loss = 0.16011160016059875 , train loss 0.16285362392663955, pcc = 0.49665698409080505\n",
      "epoch 837, EV = -0.016844158632713453, val  loss = 0.1600740522146225 , train loss 0.1657596245408058, pcc = 0.4967536926269531\n",
      "epoch 838, EV = -0.01720367293608816, val  loss = 0.16002851724624634 , train loss 0.1670704111456871, pcc = 0.4969536364078522\n",
      "epoch 839, EV = -0.016780756544648556, val  loss = 0.16000309884548186 , train loss 0.1608910381793976, pcc = 0.4966293275356293\n",
      "epoch 840, EV = -0.017260227809872544, val  loss = 0.15995084047317504 , train loss 0.1629521518945694, pcc = 0.4968739449977875\n",
      "epoch 841, EV = -0.017149261738124647, val  loss = 0.15991703569889068 , train loss 0.1669035241007805, pcc = 0.49699416756629944\n",
      "epoch 842, EV = -0.016750021984702664, val  loss = 0.15988962948322297 , train loss 0.16511193364858628, pcc = 0.4967230260372162\n",
      "epoch 843, EV = -0.01670005656125253, val  loss = 0.15985641479492188 , train loss 0.1591353476047516, pcc = 0.4965464472770691\n",
      "epoch 844, EV = -0.016525135228508396, val  loss = 0.15983087420463563 , train loss 0.1641001284122467, pcc = 0.4963131546974182\n",
      "epoch 845, EV = -0.016877139346641406, val  loss = 0.15977507829666138 , train loss 0.17105536609888078, pcc = 0.4968129098415375\n",
      "epoch 846, EV = -0.016817600580683927, val  loss = 0.15973976850509644 , train loss 0.16555458158254624, pcc = 0.4967316687107086\n",
      "epoch 847, EV = -0.01700400469595926, val  loss = 0.15969133675098418 , train loss 0.1623564898967743, pcc = 0.49701809883117676\n",
      "epoch 848, EV = -0.016771745263484485, val  loss = 0.15966281592845916 , train loss 0.1687403976917267, pcc = 0.4969584047794342\n",
      "epoch 849, EV = -0.0167364802276879, val  loss = 0.1596323400735855 , train loss 0.16947181075811385, pcc = 0.4966351389884949\n",
      "epoch 850, EV = -0.016915325010031984, val  loss = 0.15958566069602967 , train loss 0.16384256184101104, pcc = 0.4968554377555847\n",
      "epoch 851, EV = -0.017182226243772004, val  loss = 0.1595419466495514 , train loss 0.16979035288095473, pcc = 0.4970954358577728\n",
      "epoch 852, EV = -0.016704370055282323, val  loss = 0.1595213681459427 , train loss 0.16622185707092285, pcc = 0.4966714382171631\n",
      "epoch 853, EV = -0.0169602982830583, val  loss = 0.1594678431749344 , train loss 0.15983422398567199, pcc = 0.4969118535518646\n",
      "epoch 854, EV = -0.01720987547907913, val  loss = 0.15942808389663696 , train loss 0.16316553503274916, pcc = 0.4971776604652405\n",
      "epoch 855, EV = -0.016667881555724563, val  loss = 0.1594078540802002 , train loss 0.16931038200855256, pcc = 0.49665775895118713\n",
      "epoch 856, EV = -0.016926723091225875, val  loss = 0.15936097204685212 , train loss 0.16249975562095642, pcc = 0.4969747066497803\n",
      "epoch 857, EV = -0.017046272231821428, val  loss = 0.15932532250881196 , train loss 0.16291542649269103, pcc = 0.4967811703681946\n",
      "epoch 858, EV = -0.016888762252372607, val  loss = 0.15928503274917602 , train loss 0.16454981863498688, pcc = 0.49713030457496643\n",
      "epoch 859, EV = -0.01677554166107847, val  loss = 0.15926011502742768 , train loss 0.1668410912156105, pcc = 0.4966515898704529\n",
      "epoch 860, EV = -0.016805879379573622, val  loss = 0.15921713411808014 , train loss 0.16741326004266738, pcc = 0.496917724609375\n",
      "epoch 861, EV = -0.017101442081886426, val  loss = 0.15917311012744903 , train loss 0.16786670833826065, pcc = 0.49696779251098633\n",
      "epoch 862, EV = -0.01687525291191904, val  loss = 0.1591388165950775 , train loss 0.16921895295381545, pcc = 0.4969887137413025\n",
      "epoch 863, EV = -0.01696397494851497, val  loss = 0.1591026395559311 , train loss 0.1621866688132286, pcc = 0.4968205392360687\n",
      "epoch 864, EV = -0.01658729503029271, val  loss = 0.15907283425331115 , train loss 0.1641457572579384, pcc = 0.49671363830566406\n",
      "epoch 865, EV = -0.01718661293648837, val  loss = 0.15901876091957093 , train loss 0.16583759635686873, pcc = 0.49706172943115234\n",
      "epoch 866, EV = -0.01684143250448662, val  loss = 0.15898875892162323 , train loss 0.16624891757965088, pcc = 0.49689292907714844\n",
      "epoch 867, EV = -0.016565456202155666, val  loss = 0.15895872712135314 , train loss 0.16016765385866166, pcc = 0.4966990053653717\n",
      "epoch 868, EV = -0.01725101732371146, val  loss = 0.1589102178812027 , train loss 0.1677427262067795, pcc = 0.49699774384498596\n",
      "epoch 869, EV = -0.016675004310775222, val  loss = 0.15888278186321259 , train loss 0.16411888897418975, pcc = 0.4967884123325348\n",
      "epoch 870, EV = -0.01712138663258469, val  loss = 0.1588311731815338 , train loss 0.16715140044689178, pcc = 0.49728861451148987\n",
      "epoch 871, EV = -0.016900999504223205, val  loss = 0.1588047116994858 , train loss 0.16465137898921967, pcc = 0.49695032835006714\n",
      "epoch 872, EV = -0.016898207497178463, val  loss = 0.15876882672309875 , train loss 0.15765118300914766, pcc = 0.4968530535697937\n",
      "epoch 873, EV = -0.016922365677984136, val  loss = 0.1587275743484497 , train loss 0.16514616906642915, pcc = 0.4969650208950043\n",
      "epoch 874, EV = -0.017059732947433202, val  loss = 0.1586885631084442 , train loss 0.16521759927272797, pcc = 0.4970706105232239\n",
      "epoch 875, EV = -0.017037905621946903, val  loss = 0.15864583253860473 , train loss 0.1629830315709114, pcc = 0.49723905324935913\n",
      "epoch 876, EV = -0.016445115992897434, val  loss = 0.15862910449504852 , train loss 0.16346696615219117, pcc = 0.4967225193977356\n",
      "epoch 877, EV = -0.016762032320624905, val  loss = 0.15858473181724547 , train loss 0.1601006269454956, pcc = 0.4968186318874359\n",
      "epoch 878, EV = -0.01659765369013736, val  loss = 0.158554807305336 , train loss 0.16734718084335326, pcc = 0.4966104030609131\n",
      "epoch 879, EV = -0.016673609352948374, val  loss = 0.15851550102233886 , train loss 0.16517464965581893, pcc = 0.49671778082847595\n",
      "epoch 880, EV = -0.016963733392849303, val  loss = 0.15846773087978364 , train loss 0.16353045254945756, pcc = 0.49703869223594666\n",
      "epoch 881, EV = -0.01700043521429363, val  loss = 0.1584276258945465 , train loss 0.15859748423099518, pcc = 0.4970420002937317\n",
      "epoch 882, EV = -0.016874775029065318, val  loss = 0.15839795172214508 , train loss 0.15973392874002457, pcc = 0.49680066108703613\n",
      "epoch 883, EV = -0.01697825601226405, val  loss = 0.1583545684814453 , train loss 0.15917158275842666, pcc = 0.49712568521499634\n",
      "epoch 884, EV = -0.016604726251802947, val  loss = 0.15833282768726348 , train loss 0.1634802058339119, pcc = 0.49663329124450684\n",
      "epoch 885, EV = -0.016827753238510667, val  loss = 0.15828776657581328 , train loss 0.1687277615070343, pcc = 0.4970301687717438\n",
      "epoch 886, EV = -0.01660373702383878, val  loss = 0.15826050341129302 , train loss 0.16554547548294068, pcc = 0.49665769934654236\n",
      "epoch 887, EV = -0.01670688077023155, val  loss = 0.15821360647678376 , train loss 0.1581604227423668, pcc = 0.4969356656074524\n",
      "epoch 888, EV = -0.017004139590681644, val  loss = 0.1581704467535019 , train loss 0.16624191850423814, pcc = 0.497048556804657\n",
      "epoch 889, EV = -0.017067445997606245, val  loss = 0.15813068747520448 , train loss 0.16390151977539064, pcc = 0.49714842438697815\n",
      "epoch 890, EV = -0.0167463317252042, val  loss = 0.15810860991477965 , train loss 0.1663065642118454, pcc = 0.4967374801635742\n",
      "epoch 891, EV = -0.01679411059931705, val  loss = 0.15806362330913543 , train loss 0.16125405430793763, pcc = 0.49680987000465393\n",
      "epoch 892, EV = -0.016787404553932055, val  loss = 0.1580262988805771 , train loss 0.16123098731040955, pcc = 0.4970623850822449\n",
      "epoch 893, EV = -0.016953055273022568, val  loss = 0.1579902708530426 , train loss 0.1639671191573143, pcc = 0.4969990849494934\n",
      "epoch 894, EV = -0.016545014423236512, val  loss = 0.15796113312244414 , train loss 0.16798998564481735, pcc = 0.49681052565574646\n",
      "epoch 895, EV = -0.01653792513044257, val  loss = 0.15792294442653657 , train loss 0.16549597680568695, pcc = 0.49677830934524536\n",
      "epoch 896, EV = -0.016725197695849233, val  loss = 0.15788533985614778 , train loss 0.16430580317974092, pcc = 0.4968244135379791\n",
      "epoch 897, EV = -0.016738784940619218, val  loss = 0.15784801542758942 , train loss 0.15884967893362045, pcc = 0.49688825011253357\n",
      "epoch 898, EV = -0.017018862460788927, val  loss = 0.15780202448368072 , train loss 0.16441507041454315, pcc = 0.49724632501602173\n",
      "epoch 899, EV = -0.01673516212848195, val  loss = 0.15777084827423096 , train loss 0.16730134338140487, pcc = 0.49710631370544434\n",
      "epoch 900, EV = -0.01701152010967857, val  loss = 0.15772198438644408 , train loss 0.16352924406528474, pcc = 0.49728235602378845\n",
      "epoch 901, EV = -0.01711390258973105, val  loss = 0.15768336355686188 , train loss 0.16663381457328796, pcc = 0.4974023699760437\n",
      "epoch 902, EV = -0.017028563900997763, val  loss = 0.1576559752225876 , train loss 0.16025735288858414, pcc = 0.49688154458999634\n",
      "epoch 903, EV = -0.016925952413625885, val  loss = 0.15761785507202147 , train loss 0.16390926837921144, pcc = 0.49716266989707947\n",
      "epoch 904, EV = -0.01664357070337262, val  loss = 0.15758752226829528 , train loss 0.16098434329032899, pcc = 0.49703970551490784\n",
      "epoch 905, EV = -0.016747589696917618, val  loss = 0.1575486421585083 , train loss 0.15896237194538115, pcc = 0.49706992506980896\n",
      "epoch 906, EV = -0.01697093091512981, val  loss = 0.1575034111738205 , train loss 0.16016369611024855, pcc = 0.4972590506076813\n",
      "epoch 907, EV = -0.016605609864519352, val  loss = 0.15748225450515746 , train loss 0.17018427848815917, pcc = 0.497060626745224\n",
      "epoch 908, EV = -0.016405667129315828, val  loss = 0.1574514776468277 , train loss 0.16885338574647904, pcc = 0.49677616357803345\n",
      "epoch 909, EV = -0.016880017100718982, val  loss = 0.1574029117822647 , train loss 0.1615526333451271, pcc = 0.49702852964401245\n",
      "epoch 910, EV = -0.01657733948607194, val  loss = 0.15737272500991822 , train loss 0.15889568924903869, pcc = 0.49684181809425354\n",
      "epoch 911, EV = -0.016657567860787374, val  loss = 0.15733125507831575 , train loss 0.16300485134124756, pcc = 0.4970248341560364\n",
      "epoch 912, EV = -0.016753610288887694, val  loss = 0.15729838013648986 , train loss 0.1665896475315094, pcc = 0.4969499111175537\n",
      "epoch 913, EV = -0.016874739998265317, val  loss = 0.15725193917751312 , train loss 0.1652212306857109, pcc = 0.4971582293510437\n",
      "epoch 914, EV = -0.01636618584917303, val  loss = 0.15723355412483214 , train loss 0.16753172427415847, pcc = 0.49677664041519165\n",
      "epoch 915, EV = -0.017137272316112853, val  loss = 0.15716924369335175 , train loss 0.16427172720432281, pcc = 0.49753332138061523\n",
      "epoch 916, EV = -0.0167651751585174, val  loss = 0.15715062022209167 , train loss 0.16025161743164062, pcc = 0.49689048528671265\n",
      "epoch 917, EV = -0.016760602331998057, val  loss = 0.15711287558078765 , train loss 0.16287723034620286, pcc = 0.4969724118709564\n",
      "epoch 918, EV = -0.016795037608397633, val  loss = 0.15707671642303467 , train loss 0.16466383934020995, pcc = 0.4969329833984375\n",
      "epoch 919, EV = -0.0171032610692476, val  loss = 0.1570292294025421 , train loss 0.16327282786369324, pcc = 0.4972965121269226\n",
      "epoch 920, EV = -0.016933547300204896, val  loss = 0.15699550807476043 , train loss 0.16254030764102936, pcc = 0.49717479944229126\n",
      "epoch 921, EV = -0.016434959152288604, val  loss = 0.1569746971130371 , train loss 0.15972591787576676, pcc = 0.4967955946922302\n",
      "epoch 922, EV = -0.016778239032678437, val  loss = 0.1569246232509613 , train loss 0.1635885640978813, pcc = 0.49711400270462036\n",
      "epoch 923, EV = -0.016908293230491773, val  loss = 0.15688665509223937 , train loss 0.16494908183813095, pcc = 0.4971666932106018\n",
      "epoch 924, EV = -0.01656256589973182, val  loss = 0.15685344338417054 , train loss 0.16429941803216935, pcc = 0.49713200330734253\n",
      "epoch 925, EV = -0.016872797618832504, val  loss = 0.1568196326494217 , train loss 0.16386037021875383, pcc = 0.4970962107181549\n",
      "epoch 926, EV = -0.016456493160180878, val  loss = 0.15678735077381134 , train loss 0.1620413228869438, pcc = 0.4969474673271179\n",
      "epoch 927, EV = -0.01644916806304664, val  loss = 0.15675176978111266 , train loss 0.16105253100395203, pcc = 0.49703940749168396\n",
      "epoch 928, EV = -0.016522392892000966, val  loss = 0.1567192405462265 , train loss 0.1610902264714241, pcc = 0.4968245029449463\n",
      "epoch 929, EV = -0.01623591048675671, val  loss = 0.156692174077034 , train loss 0.16090310215950013, pcc = 0.496626079082489\n",
      "epoch 930, EV = -0.01656898437884816, val  loss = 0.15664357244968413 , train loss 0.1561865195631981, pcc = 0.496755987405777\n",
      "epoch 931, EV = -0.016539719543958966, val  loss = 0.15660468339920045 , train loss 0.16217150390148163, pcc = 0.4969955384731293\n",
      "epoch 932, EV = -0.016753673553466797, val  loss = 0.1565671056509018 , train loss 0.16009869128465654, pcc = 0.496929407119751\n",
      "epoch 933, EV = -0.01680229002969307, val  loss = 0.15652582049369812 , train loss 0.16070740073919296, pcc = 0.49722927808761597\n",
      "epoch 934, EV = -0.01675746629112645, val  loss = 0.15648840069770814 , train loss 0.16539150029420852, pcc = 0.49731484055519104\n",
      "epoch 935, EV = -0.016831228607579282, val  loss = 0.15645044147968293 , train loss 0.16587295830249787, pcc = 0.4973846971988678\n",
      "epoch 936, EV = -0.016783935981884338, val  loss = 0.156411612033844 , train loss 0.16370662599802016, pcc = 0.4973544478416443\n",
      "epoch 937, EV = -0.016399528373751724, val  loss = 0.15639217495918273 , train loss 0.1598171576857567, pcc = 0.496917724609375\n",
      "epoch 938, EV = -0.01632427646402727, val  loss = 0.1563613474369049 , train loss 0.1626367151737213, pcc = 0.4967760145664215\n",
      "epoch 939, EV = -0.016566760184472066, val  loss = 0.15631430149078368 , train loss 0.15946275293827056, pcc = 0.49709439277648926\n",
      "epoch 940, EV = -0.016408008964438187, val  loss = 0.15628576576709746 , train loss 0.15990690588951112, pcc = 0.4968874454498291\n",
      "epoch 941, EV = -0.01653905134452017, val  loss = 0.15624288320541382 , train loss 0.15871070176362992, pcc = 0.4969947338104248\n",
      "epoch 942, EV = -0.016607190433301423, val  loss = 0.15620574951171876 , train loss 0.16017804890871049, pcc = 0.49705013632774353\n",
      "epoch 943, EV = -0.01671789246692992, val  loss = 0.15616371631622314 , train loss 0.16165116429328918, pcc = 0.4972984790802002\n",
      "epoch 944, EV = -0.016715521352332934, val  loss = 0.15612847208976746 , train loss 0.16189260631799698, pcc = 0.4972553253173828\n",
      "epoch 945, EV = -0.016714548855497127, val  loss = 0.1560901641845703 , train loss 0.16549317240715028, pcc = 0.49731770157814026\n",
      "epoch 946, EV = -0.016375116611781874, val  loss = 0.15606898665428162 , train loss 0.16282242089509963, pcc = 0.49704843759536743\n",
      "epoch 947, EV = -0.016332358121871948, val  loss = 0.15603797137737274 , train loss 0.1652004137635231, pcc = 0.4968422055244446\n",
      "epoch 948, EV = -0.016959709556479203, val  loss = 0.1559806674718857 , train loss 0.1591420814394951, pcc = 0.4972122609615326\n",
      "epoch 949, EV = -0.01670097781900774, val  loss = 0.15595197677612305 , train loss 0.15687357038259506, pcc = 0.49705880880355835\n",
      "epoch 950, EV = -0.016710308037306134, val  loss = 0.15591244101524354 , train loss 0.16142766773700715, pcc = 0.49717503786087036\n",
      "epoch 951, EV = -0.016440505521339282, val  loss = 0.15588472187519073 , train loss 0.164365254342556, pcc = 0.49704426527023315\n",
      "epoch 952, EV = -0.016472385640729937, val  loss = 0.15584736168384553 , train loss 0.162481090426445, pcc = 0.4970821440219879\n",
      "epoch 953, EV = -0.01663665342749211, val  loss = 0.15580254197120666 , train loss 0.16032237857580184, pcc = 0.4972914755344391\n",
      "epoch 954, EV = -0.016593579660382187, val  loss = 0.15577492415904998 , train loss 0.1685727283358574, pcc = 0.49710431694984436\n",
      "epoch 955, EV = -0.016545972280335008, val  loss = 0.15574072301387787 , train loss 0.16543684750795365, pcc = 0.49694007635116577\n",
      "epoch 956, EV = -0.016609529131337217, val  loss = 0.15570254623889923 , train loss 0.16285925805568696, pcc = 0.49716752767562866\n",
      "epoch 957, EV = -0.016387146293071277, val  loss = 0.15566803216934205 , train loss 0.16124720871448517, pcc = 0.4969866871833801\n",
      "epoch 958, EV = -0.016297387449364913, val  loss = 0.15564255714416503 , train loss 0.1547090858221054, pcc = 0.4967118203639984\n",
      "epoch 959, EV = -0.016504085377642985, val  loss = 0.15559292137622832 , train loss 0.1563989982008934, pcc = 0.49715423583984375\n",
      "epoch 960, EV = -0.01661855975786845, val  loss = 0.15555886328220367 , train loss 0.160684996843338, pcc = 0.49706539511680603\n",
      "epoch 961, EV = -0.016504839846962376, val  loss = 0.15552471578121185 , train loss 0.1642207071185112, pcc = 0.4970400333404541\n",
      "epoch 962, EV = -0.016720342531538847, val  loss = 0.15548081696033478 , train loss 0.1609864979982376, pcc = 0.4972880482673645\n",
      "epoch 963, EV = -0.016739169756571453, val  loss = 0.15544661283493041 , train loss 0.16258188039064408, pcc = 0.4973289370536804\n",
      "epoch 964, EV = -0.016403028316665114, val  loss = 0.15542170107364656 , train loss 0.1626390650868416, pcc = 0.4971059262752533\n",
      "epoch 965, EV = -0.01670229434967041, val  loss = 0.15537450611591339 , train loss 0.1599365085363388, pcc = 0.4973783493041992\n",
      "epoch 966, EV = -0.01644752684392427, val  loss = 0.1553452730178833 , train loss 0.16198114603757857, pcc = 0.4971911311149597\n",
      "epoch 967, EV = -0.016457643948103253, val  loss = 0.15530602335929872 , train loss 0.1575356125831604, pcc = 0.49712562561035156\n",
      "epoch 968, EV = -0.016665336332823102, val  loss = 0.15526807308197021 , train loss 0.16488658934831618, pcc = 0.49726182222366333\n",
      "epoch 969, EV = -0.01657380608090183, val  loss = 0.1552340120077133 , train loss 0.16161951124668122, pcc = 0.49743956327438354\n",
      "epoch 970, EV = -0.0163286124405108, val  loss = 0.15520743429660797 , train loss 0.1657305344939232, pcc = 0.4970504641532898\n",
      "epoch 971, EV = -0.01660408890038206, val  loss = 0.1551649957895279 , train loss 0.16252840757369996, pcc = 0.4972657561302185\n",
      "epoch 972, EV = -0.016534837191564997, val  loss = 0.15513124763965608 , train loss 0.16279326379299164, pcc = 0.497171014547348\n",
      "epoch 973, EV = -0.01673485051121628, val  loss = 0.15508690476417542 , train loss 0.15608241260051728, pcc = 0.49724912643432617\n",
      "epoch 974, EV = -0.016625545526805677, val  loss = 0.15505301654338838 , train loss 0.1603077009320259, pcc = 0.49728846549987793\n",
      "epoch 975, EV = -0.01648863052067004, val  loss = 0.1550228178501129 , train loss 0.1623440444469452, pcc = 0.4972940981388092\n",
      "epoch 976, EV = -0.016384405525107133, val  loss = 0.15498775541782378 , train loss 0.15989702492952346, pcc = 0.4971690773963928\n",
      "epoch 977, EV = -0.01688778138997262, val  loss = 0.15494407415390016 , train loss 0.15880788564682008, pcc = 0.49746400117874146\n",
      "epoch 978, EV = -0.016477387202413457, val  loss = 0.15491829216480255 , train loss 0.16373531222343446, pcc = 0.49700602889060974\n",
      "epoch 979, EV = -0.016324568213078015, val  loss = 0.15488702654838563 , train loss 0.15731334686279297, pcc = 0.4970172941684723\n",
      "epoch 980, EV = -0.01679216142286334, val  loss = 0.1548403114080429 , train loss 0.15726644694805145, pcc = 0.4972660541534424\n",
      "epoch 981, EV = -0.016707303754070348, val  loss = 0.15479988157749175 , train loss 0.15881320536136628, pcc = 0.49764639139175415\n",
      "epoch 982, EV = -0.016561815090346755, val  loss = 0.15476978123188018 , train loss 0.1550738051533699, pcc = 0.4973292350769043\n",
      "epoch 983, EV = -0.016732208561479, val  loss = 0.15472930073738098 , train loss 0.15506358444690704, pcc = 0.49745678901672363\n",
      "epoch 984, EV = -0.016876702768760816, val  loss = 0.15469697415828704 , train loss 0.1616845905780792, pcc = 0.49726518988609314\n",
      "epoch 985, EV = -0.01638501568844444, val  loss = 0.1546701669692993 , train loss 0.16344445943832397, pcc = 0.4972178339958191\n",
      "epoch 986, EV = -0.016683472353115417, val  loss = 0.15462545156478882 , train loss 0.15785116851329803, pcc = 0.49742984771728516\n",
      "epoch 987, EV = -0.016730151155538726, val  loss = 0.15458854734897615 , train loss 0.15679808259010314, pcc = 0.4973936080932617\n",
      "epoch 988, EV = -0.016112518937964188, val  loss = 0.15457904934883118 , train loss 0.1619301274418831, pcc = 0.49675577878952026\n",
      "epoch 989, EV = -0.016628585886536984, val  loss = 0.15452142655849457 , train loss 0.15993081033229828, pcc = 0.4974156320095062\n",
      "epoch 990, EV = -0.01632682900679739, val  loss = 0.15449551045894622 , train loss 0.15752430707216264, pcc = 0.4971385896205902\n",
      "epoch 991, EV = -0.01621538609789129, val  loss = 0.15446132719516753 , train loss 0.15760836005210876, pcc = 0.497175395488739\n",
      "epoch 992, EV = -0.016613960788961043, val  loss = 0.15441936254501343 , train loss 0.16377777010202407, pcc = 0.49730658531188965\n",
      "epoch 993, EV = -0.016233258602911967, val  loss = 0.15439085364341737 , train loss 0.1615752786397934, pcc = 0.4971417486667633\n",
      "epoch 994, EV = -0.016275214521508468, val  loss = 0.1543575644493103 , train loss 0.16055010706186296, pcc = 0.4971250891685486\n",
      "epoch 995, EV = -0.016138169326280292, val  loss = 0.15432639718055724 , train loss 0.159662364423275, pcc = 0.497004896402359\n",
      "epoch 996, EV = -0.01631406681579456, val  loss = 0.15428667962551118 , train loss 0.17030085176229476, pcc = 0.49727386236190796\n",
      "epoch 997, EV = -0.016272445519765217, val  loss = 0.1542484223842621 , train loss 0.1568250149488449, pcc = 0.4972032904624939\n",
      "epoch 998, EV = -0.016578003502728648, val  loss = 0.154200741648674 , train loss 0.161334627866745, pcc = 0.4973961412906647\n",
      "epoch 999, EV = -0.016258334904386287, val  loss = 0.1541764944791794 , train loss 0.15473231226205825, pcc = 0.4971359372138977\n",
      "epoch 1000, EV = -0.01628780260420682, val  loss = 0.1541720747947693 , train loss 0.15970909893512725, pcc = 0.49715977907180786\n",
      "epoch 1001, EV = -0.016278474477299472, val  loss = 0.1541682869195938 , train loss 0.15729618966579437, pcc = 0.4971540570259094\n",
      "epoch 1002, EV = -0.016345322132110596, val  loss = 0.1541629046201706 , train loss 0.16094088256359101, pcc = 0.497211754322052\n",
      "epoch 1003, EV = -0.016365978801459596, val  loss = 0.15415836572647096 , train loss 0.1565320760011673, pcc = 0.4972399175167084\n",
      "epoch 1004, EV = -0.016423375982987255, val  loss = 0.1541530042886734 , train loss 0.15748532712459565, pcc = 0.49727097153663635\n",
      "epoch 1005, EV = -0.016403619134635256, val  loss = 0.15414997339248657 , train loss 0.1550714373588562, pcc = 0.49725133180618286\n",
      "epoch 1006, EV = -0.01636568130108348, val  loss = 0.15414701104164125 , train loss 0.1582512766122818, pcc = 0.4972529411315918\n",
      "epoch 1007, EV = -0.01642978923362598, val  loss = 0.15414172112941743 , train loss 0.1595099851489067, pcc = 0.4972890019416809\n",
      "epoch 1008, EV = -0.016389532047405578, val  loss = 0.1541397124528885 , train loss 0.1590758129954338, pcc = 0.49725398421287537\n",
      "epoch 1009, EV = -0.016406674656951635, val  loss = 0.15413494408130646 , train loss 0.15966228842735292, pcc = 0.497279554605484\n",
      "epoch 1010, EV = -0.016430641475476716, val  loss = 0.15413084924221038 , train loss 0.16129538863897325, pcc = 0.49729442596435547\n",
      "epoch 1011, EV = -0.016413502525864987, val  loss = 0.15412793457508087 , train loss 0.1594725266098976, pcc = 0.4972814619541168\n",
      "epoch 1012, EV = -0.01638099655770419, val  loss = 0.1541246384382248 , train loss 0.16182737201452255, pcc = 0.49726882576942444\n",
      "epoch 1013, EV = -0.016481232747696993, val  loss = 0.1541179060935974 , train loss 0.15666037499904634, pcc = 0.49734440445899963\n",
      "epoch 1014, EV = -0.01642808997840212, val  loss = 0.1541169673204422 , train loss 0.15880134403705598, pcc = 0.4972630441188812\n",
      "epoch 1015, EV = -0.016457892823637577, val  loss = 0.15411249697208404 , train loss 0.15801074206829072, pcc = 0.49729761481285095\n",
      "epoch 1016, EV = -0.016427972337655854, val  loss = 0.15410931706428527 , train loss 0.15945175737142564, pcc = 0.4972866475582123\n",
      "epoch 1017, EV = -0.01640193451914871, val  loss = 0.15410658121109008 , train loss 0.1556307703256607, pcc = 0.49725770950317383\n",
      "epoch 1018, EV = -0.016456160106157, val  loss = 0.15410091280937194 , train loss 0.16105970293283461, pcc = 0.49732011556625366\n",
      "epoch 1019, EV = -0.016423587213482773, val  loss = 0.15409884452819825 , train loss 0.16110811680555343, pcc = 0.4972774386405945\n",
      "epoch 1020, EV = -0.016454551303595827, val  loss = 0.15409404337406157 , train loss 0.15541054159402848, pcc = 0.49730396270751953\n",
      "epoch 1021, EV = -0.016411990972987393, val  loss = 0.1540914922952652 , train loss 0.15455180704593657, pcc = 0.49727964401245117\n",
      "epoch 1022, EV = -0.01645361906603763, val  loss = 0.15408685207366943 , train loss 0.1603601098060608, pcc = 0.4973045885562897\n",
      "epoch 1023, EV = -0.01648926891778645, val  loss = 0.15408165752887726 , train loss 0.15467876493930816, pcc = 0.4973398745059967\n",
      "epoch 1024, EV = -0.016455536348777906, val  loss = 0.15407952666282654 , train loss 0.15742177069187163, pcc = 0.4973227381706238\n",
      "epoch 1025, EV = -0.0164149262403187, val  loss = 0.1540770262479782 , train loss 0.16189600080251693, pcc = 0.49729257822036743\n",
      "epoch 1026, EV = -0.01642905776960808, val  loss = 0.1540736585855484 , train loss 0.16170157641172409, pcc = 0.4972764551639557\n",
      "epoch 1027, EV = -0.016409956572348613, val  loss = 0.15406951010227204 , train loss 0.16431542932987214, pcc = 0.49731379747390747\n",
      "epoch 1028, EV = -0.01642581820487976, val  loss = 0.15406623780727385 , train loss 0.16160907447338105, pcc = 0.49728599190711975\n",
      "epoch 1029, EV = -0.016411607725578443, val  loss = 0.15406270921230317 , train loss 0.16152084320783616, pcc = 0.4972931444644928\n",
      "epoch 1030, EV = -0.016409438953065035, val  loss = 0.1540585070848465 , train loss 0.16591832637786866, pcc = 0.49731898307800293\n",
      "epoch 1031, EV = -0.01642843714931555, val  loss = 0.15405470430850982 , train loss 0.15725518763065338, pcc = 0.49730175733566284\n",
      "epoch 1032, EV = -0.016410212244903834, val  loss = 0.15405193865299224 , train loss 0.15581113398075103, pcc = 0.4972681999206543\n",
      "epoch 1033, EV = -0.01644126521913629, val  loss = 0.15404708683490753 , train loss 0.15782809257507324, pcc = 0.49732327461242676\n",
      "epoch 1034, EV = -0.01638721792321456, val  loss = 0.15404523611068727 , train loss 0.16266821771860124, pcc = 0.4972791075706482\n",
      "epoch 1035, EV = -0.01644326092904074, val  loss = 0.15403966307640077 , train loss 0.1562032639980316, pcc = 0.49732375144958496\n",
      "epoch 1036, EV = -0.01643438035981697, val  loss = 0.15403642356395722 , train loss 0.1590220093727112, pcc = 0.49731412529945374\n",
      "epoch 1037, EV = -0.016440410363046748, val  loss = 0.1540326237678528 , train loss 0.1584414318203926, pcc = 0.49731460213661194\n",
      "epoch 1038, EV = -0.016427730259142424, val  loss = 0.15402984619140625 , train loss 0.16279375702142715, pcc = 0.4973091185092926\n",
      "epoch 1039, EV = -0.016385206527877273, val  loss = 0.15402720868587494 , train loss 0.15885163098573685, pcc = 0.4972820281982422\n",
      "epoch 1040, EV = -0.01644664113981682, val  loss = 0.15402119755744934 , train loss 0.15701012015342714, pcc = 0.49733757972717285\n",
      "epoch 1041, EV = -0.016431790172008045, val  loss = 0.1540188282728195 , train loss 0.15888733118772508, pcc = 0.4972946047782898\n",
      "epoch 1042, EV = -0.016412898113853054, val  loss = 0.15401555001735687 , train loss 0.1554727867245674, pcc = 0.4972875714302063\n",
      "epoch 1043, EV = -0.016410166234300846, val  loss = 0.15401169657707214 , train loss 0.15810389071702957, pcc = 0.49731284379959106\n",
      "epoch 1044, EV = -0.016476798475834362, val  loss = 0.15400643050670623 , train loss 0.15829070061445236, pcc = 0.4973386824131012\n",
      "epoch 1045, EV = -0.016441330052258677, val  loss = 0.15400404930114747 , train loss 0.15853812396526337, pcc = 0.49730998277664185\n",
      "epoch 1046, EV = -0.016413562130509762, val  loss = 0.15400026738643646 , train loss 0.16291142255067825, pcc = 0.49733415246009827\n",
      "epoch 1047, EV = -0.01641319195429484, val  loss = 0.1539968341588974 , train loss 0.158595210313797, pcc = 0.4973030984401703\n",
      "epoch 1048, EV = -0.016441225482706438, val  loss = 0.15399234890937805 , train loss 0.15565042942762375, pcc = 0.4973355233669281\n",
      "epoch 1049, EV = -0.016432279034664755, val  loss = 0.15398896336555482 , train loss 0.1655745416879654, pcc = 0.4973427653312683\n",
      "epoch 1050, EV = -0.01644400337286163, val  loss = 0.1539852112531662 , train loss 0.1556779831647873, pcc = 0.4973316192626953\n",
      "epoch 1051, EV = -0.016447956101936206, val  loss = 0.15398212373256684 , train loss 0.1645389512181282, pcc = 0.49732694029808044\n",
      "epoch 1052, EV = -0.016386563840665315, val  loss = 0.1539797455072403 , train loss 0.16153580248355864, pcc = 0.49728909134864807\n",
      "epoch 1053, EV = -0.016475513838885122, val  loss = 0.15397443771362304 , train loss 0.1596863493323326, pcc = 0.49731874465942383\n",
      "epoch 1054, EV = -0.01647132112268816, val  loss = 0.15396971106529236 , train loss 0.1539085626602173, pcc = 0.49735093116760254\n",
      "epoch 1055, EV = -0.016444754182246692, val  loss = 0.15396696627140044 , train loss 0.15868062227964402, pcc = 0.4973359704017639\n",
      "epoch 1056, EV = -0.016437193280772158, val  loss = 0.1539638102054596 , train loss 0.15953289717435837, pcc = 0.4973234534263611\n",
      "epoch 1057, EV = -0.016480222605822378, val  loss = 0.15395913422107696 , train loss 0.15759287476539613, pcc = 0.49733567237854004\n",
      "epoch 1058, EV = -0.01641153975536949, val  loss = 0.1539573311805725 , train loss 0.1588668018579483, pcc = 0.4973052442073822\n",
      "epoch 1059, EV = -0.016420342420276842, val  loss = 0.15395348072052 , train loss 0.16102655977010727, pcc = 0.4973205029964447\n",
      "epoch 1060, EV = -0.016430221105876722, val  loss = 0.153949511051178 , train loss 0.1598341152071953, pcc = 0.4973216950893402\n",
      "epoch 1061, EV = -0.0164399758765572, val  loss = 0.15394501388072968 , train loss 0.15829782634973527, pcc = 0.4973454475402832\n",
      "epoch 1062, EV = -0.016434879156581143, val  loss = 0.15394207239151 , train loss 0.1603020280599594, pcc = 0.49732351303100586\n",
      "epoch 1063, EV = -0.016435280180813975, val  loss = 0.15393786430358886 , train loss 0.16064943224191666, pcc = 0.49734750390052795\n",
      "epoch 1064, EV = -0.016423051294527556, val  loss = 0.15393471717834473 , train loss 0.15927474945783615, pcc = 0.49733859300613403\n",
      "epoch 1065, EV = -0.016456505185679385, val  loss = 0.1539299875497818 , train loss 0.15773055255413054, pcc = 0.4973526895046234\n",
      "epoch 1066, EV = -0.016428968362640916, val  loss = 0.15392799079418182 , train loss 0.15890126377344133, pcc = 0.49730387330055237\n",
      "epoch 1067, EV = -0.016424383510623062, val  loss = 0.15392425358295442 , train loss 0.15986245274543762, pcc = 0.49731898307800293\n",
      "epoch 1068, EV = -0.016404909522909867, val  loss = 0.1539215326309204 , train loss 0.16569457650184632, pcc = 0.4973074495792389\n",
      "epoch 1069, EV = -0.016445234156491465, val  loss = 0.1539161831140518 , train loss 0.15400791466236113, pcc = 0.4973313808441162\n",
      "epoch 1070, EV = -0.016423473755518597, val  loss = 0.1539130061864853 , train loss 0.15813579261302949, pcc = 0.49734950065612793\n",
      "epoch 1071, EV = -0.01645995911798979, val  loss = 0.15390833020210265 , train loss 0.1569048285484314, pcc = 0.4973534941673279\n",
      "epoch 1072, EV = -0.016442197456694486, val  loss = 0.15390532910823823 , train loss 0.15645780563354492, pcc = 0.4973289966583252\n",
      "epoch 1073, EV = -0.01639501306048611, val  loss = 0.15390332341194152 , train loss 0.15438330322504043, pcc = 0.4972971975803375\n",
      "epoch 1074, EV = -0.01646402164509422, val  loss = 0.1538977652788162 , train loss 0.16228481531143188, pcc = 0.497354656457901\n",
      "epoch 1075, EV = -0.016418812044879848, val  loss = 0.15389530956745148 , train loss 0.15867993831634522, pcc = 0.49731746315956116\n",
      "epoch 1076, EV = -0.016418306451094777, val  loss = 0.1538913905620575 , train loss 0.15912365317344665, pcc = 0.4973219335079193\n",
      "epoch 1077, EV = -0.016431687693846852, val  loss = 0.153887739777565 , train loss 0.16109849363565446, pcc = 0.497324138879776\n",
      "epoch 1078, EV = -0.016457649699428624, val  loss = 0.15388360023498535 , train loss 0.15825741738080978, pcc = 0.4973214268684387\n",
      "epoch 1079, EV = -0.01648557604404918, val  loss = 0.15387878119945525 , train loss 0.1570280522108078, pcc = 0.4973528981208801\n",
      "epoch 1080, EV = -0.016430941067243878, val  loss = 0.15387641191482543 , train loss 0.1588572472333908, pcc = 0.497335284948349\n",
      "epoch 1081, EV = -0.01641884864422313, val  loss = 0.15387358367443085 , train loss 0.15725868493318557, pcc = 0.49731341004371643\n",
      "epoch 1082, EV = -0.016475189150425427, val  loss = 0.15386808812618255 , train loss 0.16293379664421082, pcc = 0.4973655641078949\n",
      "epoch 1083, EV = -0.016399362108163666, val  loss = 0.15386687219142914 , train loss 0.1551222622394562, pcc = 0.49728822708129883\n",
      "epoch 1084, EV = -0.01638133536305344, val  loss = 0.1538634866476059 , train loss 0.1556554615497589, pcc = 0.49730315804481506\n",
      "epoch 1085, EV = -0.01643439813664085, val  loss = 0.1538581818342209 , train loss 0.15638507902622223, pcc = 0.49733370542526245\n",
      "epoch 1086, EV = -0.01643183670545879, val  loss = 0.15385491847991944 , train loss 0.15910255610942842, pcc = 0.49733424186706543\n",
      "epoch 1087, EV = -0.016384492840683253, val  loss = 0.15385260283946992 , train loss 0.16178978383541107, pcc = 0.4973071217536926\n",
      "epoch 1088, EV = -0.016444536677578038, val  loss = 0.1538473665714264 , train loss 0.1633985534310341, pcc = 0.4973480701446533\n",
      "epoch 1089, EV = -0.016408197189632216, val  loss = 0.15384469032287598 , train loss 0.15887788236141204, pcc = 0.4973050653934479\n",
      "epoch 1090, EV = -0.016445933726795932, val  loss = 0.15383998453617095 , train loss 0.16107897758483886, pcc = 0.49733152985572815\n",
      "epoch 1091, EV = -0.016451785961786907, val  loss = 0.15383602678775787 , train loss 0.16179074943065644, pcc = 0.4973629117012024\n",
      "epoch 1092, EV = -0.016414492276676913, val  loss = 0.15383397042751312 , train loss 0.15742298662662507, pcc = 0.4973219931125641\n",
      "epoch 1093, EV = -0.016403508290909884, val  loss = 0.1538301020860672 , train loss 0.15982749462127685, pcc = 0.4973217844963074\n",
      "epoch 1094, EV = -0.01641080619996054, val  loss = 0.1538264900445938 , train loss 0.16425665467977524, pcc = 0.4973289370536804\n",
      "epoch 1095, EV = -0.016469870742998625, val  loss = 0.15382140278816223 , train loss 0.16105600893497468, pcc = 0.4973434507846832\n",
      "epoch 1096, EV = -0.016393799530832392, val  loss = 0.15381962358951567 , train loss 0.1599137082695961, pcc = 0.4973065257072449\n",
      "epoch 1097, EV = -0.016471428306479203, val  loss = 0.15381347835063935 , train loss 0.15968444645404817, pcc = 0.49738064408302307\n",
      "epoch 1098, EV = -0.01643043599630657, val  loss = 0.15381108224391937 , train loss 0.15867982804775238, pcc = 0.4973454475402832\n",
      "epoch 1099, EV = -0.016411469693769488, val  loss = 0.15380875170230865 , train loss 0.16326158195734025, pcc = 0.497305303812027\n",
      "epoch 1100, EV = -0.016393095777745833, val  loss = 0.15380524098873138 , train loss 0.16361481100320815, pcc = 0.49731507897377014\n",
      "epoch 1101, EV = -0.016425920683040954, val  loss = 0.1538008213043213 , train loss 0.1613370805978775, pcc = 0.4973268508911133\n",
      "epoch 1102, EV = -0.01640134370117857, val  loss = 0.15379766523838043 , train loss 0.16055196821689605, pcc = 0.4973069131374359\n",
      "epoch 1103, EV = -0.01643313859638415, val  loss = 0.15379364788532257 , train loss 0.15724698305130005, pcc = 0.497304767370224\n",
      "epoch 1104, EV = -0.016443866386748197, val  loss = 0.15378898084163667 , train loss 0.15410228967666625, pcc = 0.4973333477973938\n",
      "epoch 1105, EV = -0.016473670800526936, val  loss = 0.1537843316793442 , train loss 0.15634840726852417, pcc = 0.49736642837524414\n",
      "epoch 1106, EV = -0.0164002399695547, val  loss = 0.15378311574459075 , train loss 0.15522147268056868, pcc = 0.4973052442073822\n",
      "epoch 1107, EV = -0.016428099912509583, val  loss = 0.15377908051013947 , train loss 0.1552788332104683, pcc = 0.49732646346092224\n",
      "epoch 1108, EV = -0.016412066263065003, val  loss = 0.15377535820007324 , train loss 0.15923569947481156, pcc = 0.4973277449607849\n",
      "epoch 1109, EV = -0.016419762059261923, val  loss = 0.15377112925052644 , train loss 0.1593075528740883, pcc = 0.49734196066856384\n",
      "epoch 1110, EV = -0.01645074863182871, val  loss = 0.15376665890216829 , train loss 0.15980663150548935, pcc = 0.49736666679382324\n",
      "epoch 1111, EV = -0.016409532542814288, val  loss = 0.15376418828964233 , train loss 0.15851763486862183, pcc = 0.4973372519016266\n",
      "epoch 1112, EV = -0.0164385317710408, val  loss = 0.15376042127609252 , train loss 0.1602633401751518, pcc = 0.49733734130859375\n",
      "epoch 1113, EV = -0.01638440918504146, val  loss = 0.15375801920890808 , train loss 0.15676618963479996, pcc = 0.49729999899864197\n",
      "epoch 1114, EV = -0.01638462878110116, val  loss = 0.15375415682792665 , train loss 0.15859196931123734, pcc = 0.49730998277664185\n",
      "epoch 1115, EV = -0.016449614052186933, val  loss = 0.1537492871284485 , train loss 0.15513799637556075, pcc = 0.49733996391296387\n",
      "epoch 1116, EV = -0.016419664809578342, val  loss = 0.1537466675043106 , train loss 0.1620179757475853, pcc = 0.49731388688087463\n",
      "epoch 1117, EV = -0.016434927781422932, val  loss = 0.15374173521995543 , train loss 0.16210419535636902, pcc = 0.49735087156295776\n",
      "epoch 1118, EV = -0.016457694141488326, val  loss = 0.15373759865760803 , train loss 0.16078310012817382, pcc = 0.4973752498626709\n",
      "epoch 1119, EV = -0.016381356276963886, val  loss = 0.15373643040657042 , train loss 0.16228940188884736, pcc = 0.497306764125824\n",
      "epoch 1120, EV = -0.016432807110903554, val  loss = 0.1537313848733902 , train loss 0.156144917011261, pcc = 0.49733033776283264\n",
      "epoch 1121, EV = -0.016439915226216902, val  loss = 0.15372758805751802 , train loss 0.15297577828168868, pcc = 0.4973408579826355\n",
      "epoch 1122, EV = -0.01637205481529236, val  loss = 0.15372590124607086 , train loss 0.15958881825208665, pcc = 0.49729466438293457\n",
      "epoch 1123, EV = -0.01646016616570322, val  loss = 0.1537188023328781 , train loss 0.1597762867808342, pcc = 0.4973936975002289\n",
      "epoch 1124, EV = -0.01643248660522595, val  loss = 0.1537168562412262 , train loss 0.1617131933569908, pcc = 0.49734848737716675\n",
      "epoch 1125, EV = -0.01639095053338168, val  loss = 0.15371404588222504 , train loss 0.15949553102254868, pcc = 0.49733224511146545\n",
      "epoch 1126, EV = -0.016408848135094894, val  loss = 0.1537101149559021 , train loss 0.15824357122182847, pcc = 0.49734002351760864\n",
      "epoch 1127, EV = -0.016424371485124555, val  loss = 0.15370580554008484 , train loss 0.16027044504880905, pcc = 0.4973541796207428\n",
      "epoch 1128, EV = -0.016416149181232118, val  loss = 0.15370233356952667 , train loss 0.15720385760068895, pcc = 0.4973365366458893\n",
      "epoch 1129, EV = -0.016399149831972624, val  loss = 0.15369940996170045 , train loss 0.1579470232129097, pcc = 0.49732786417007446\n",
      "epoch 1130, EV = -0.01641796973713657, val  loss = 0.15369491577148436 , train loss 0.15695922523736955, pcc = 0.49733200669288635\n",
      "epoch 1131, EV = -0.016436827287339327, val  loss = 0.153691104054451 , train loss 0.16177239120006562, pcc = 0.49734964966773987\n",
      "epoch 1132, EV = -0.0163970082475428, val  loss = 0.1536889225244522 , train loss 0.15857673585414886, pcc = 0.4973183274269104\n",
      "epoch 1133, EV = -0.016425705792611104, val  loss = 0.15368396937847137 , train loss 0.15726543217897415, pcc = 0.49735385179519653\n",
      "epoch 1134, EV = -0.016392255561393603, val  loss = 0.15368137061595916 , train loss 0.16185338348150252, pcc = 0.4973258078098297\n",
      "epoch 1135, EV = -0.016382684833125064, val  loss = 0.1536777913570404 , train loss 0.16533136516809463, pcc = 0.49733811616897583\n",
      "epoch 1136, EV = -0.016376566991471407, val  loss = 0.15367473363876344 , train loss 0.16064738780260085, pcc = 0.49730920791625977\n",
      "epoch 1137, EV = -0.016453569395500318, val  loss = 0.1536687433719635 , train loss 0.1594007894396782, pcc = 0.4973553717136383\n",
      "epoch 1138, EV = -0.016432925797345347, val  loss = 0.15366568863391877 , train loss 0.1614014610648155, pcc = 0.49734869599342346\n",
      "epoch 1139, EV = -0.01640052701297559, val  loss = 0.15366315543651582 , train loss 0.16196897774934768, pcc = 0.4973328709602356\n",
      "epoch 1140, EV = -0.016396689833256237, val  loss = 0.15365957021713256 , train loss 0.16033912152051927, pcc = 0.49732860922813416\n",
      "epoch 1141, EV = -0.01638772822262948, val  loss = 0.1536562114953995 , train loss 0.15941691249608994, pcc = 0.49732011556625366\n",
      "epoch 1142, EV = -0.016395883602008485, val  loss = 0.15365212559700012 , train loss 0.1589610442519188, pcc = 0.49733632802963257\n",
      "epoch 1143, EV = -0.01641385701664707, val  loss = 0.1536483198404312 , train loss 0.15911614000797272, pcc = 0.4973420202732086\n",
      "epoch 1144, EV = -0.01643196846309461, val  loss = 0.153643798828125 , train loss 0.1586925730109215, pcc = 0.49734777212142944\n",
      "epoch 1145, EV = -0.016430548931422987, val  loss = 0.15364042520523072 , train loss 0.15909819602966307, pcc = 0.49735021591186523\n",
      "epoch 1146, EV = -0.016424596309661865, val  loss = 0.15363667607307435 , train loss 0.15793876498937606, pcc = 0.49734634160995483\n",
      "epoch 1147, EV = -0.01641622551700525, val  loss = 0.1536332458257675 , train loss 0.16077467203140258, pcc = 0.49736127257347107\n",
      "epoch 1148, EV = -0.016447721343291432, val  loss = 0.1536284029483795 , train loss 0.15748046785593034, pcc = 0.4973820745944977\n",
      "epoch 1149, EV = -0.016375232161137097, val  loss = 0.1536271780729294 , train loss 0.16016226559877395, pcc = 0.4973320960998535\n",
      "epoch 1150, EV = -0.016448122890372025, val  loss = 0.1536220282316208 , train loss 0.15969555377960204, pcc = 0.4973505437374115\n",
      "epoch 1151, EV = -0.0163923068004742, val  loss = 0.15361980795860292 , train loss 0.15772200673818587, pcc = 0.49733322858810425\n",
      "epoch 1152, EV = -0.016382059507202684, val  loss = 0.153616464138031 , train loss 0.15858847945928572, pcc = 0.4973154664039612\n",
      "epoch 1153, EV = -0.016417029656861957, val  loss = 0.15361140966415404 , train loss 0.16120655685663224, pcc = 0.49735864996910095\n",
      "epoch 1154, EV = -0.016446348345070555, val  loss = 0.15360665917396546 , train loss 0.15880396962165833, pcc = 0.49738460779190063\n",
      "epoch 1155, EV = -0.016413938058050054, val  loss = 0.15360434651374816 , train loss 0.1559936836361885, pcc = 0.49735260009765625\n",
      "epoch 1156, EV = -0.01642873778677823, val  loss = 0.15360060334205627 , train loss 0.15731187760829926, pcc = 0.49733924865722656\n",
      "epoch 1157, EV = -0.01640039368679649, val  loss = 0.15359746515750886 , train loss 0.16029510498046876, pcc = 0.49733614921569824\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\2\\ipykernel_6636\\1387766017.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mendure\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m10\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mepoch\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mtqdm_notebook\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mepoches\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m     \u001B[0mlosses_train\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0mtrain_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mencoder\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0moptimizer\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      7\u001B[0m     \u001B[0mpcc\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mev\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mvalidate_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mencoder\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m     \u001B[1;31m#ev,loss = validate_model(encoder)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\2\\ipykernel_6636\\2086906003.py\u001B[0m in \u001B[0;36mtrain_model\u001B[1;34m(encoder, optimizer)\u001B[0m\n\u001B[0;32m     14\u001B[0m         \u001B[0mloss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     15\u001B[0m         \u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 16\u001B[1;33m         \u001B[0mlosses\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mloss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mitem\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     17\u001B[0m \u001B[1;31m#         print(f'iteration {i}, train loss: {losses[-1]}')\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "epoches = 2000\n",
    "best_loss = 1e100\n",
    "not_improve = 0\n",
    "endure = 10\n",
    "for epoch in tqdm_notebook(range(epoches)):\n",
    "    losses_train += train_model(encoder,optimizer)\n",
    "    pcc, ev,loss = validate_model(encoder)\n",
    "    #ev,loss = validate_model(encoder)\n",
    "    EVs.append(ev)\n",
    "    pccs.append(pcc)\n",
    "    losses_val.append(loss)\n",
    "    train_loss = sum(losses_train[-10:])/10\n",
    "    if train_loss < best_loss - 1e-5:\n",
    "        not_improve = 0\n",
    "    else:\n",
    "        not_improve += 1\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'epoch {epoch}, EV = {ev}, val  loss = {loss} , train loss {sum(losses_train[-10:])/10}, pcc = {pcc}')\n",
    "        #print(f'epoch {epoch}, EV = {ev}, val  loss = {loss} , train loss {sum(losses_train[-10:])/10}')\n",
    "    if not_improve == endure:\n",
    "        print(\"Early stopping!\")\n",
    "\n",
    "    scheduler.step()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "-0.01327828513948541"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(EVs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "exp = \"layer3_114\"\n",
    "torch.save(encoder, f'./exp{exp}.pt')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
