{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "import models\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from PIL import Image\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "device = 'cuda:0' # device where you put your data and models\n",
    "data_path = './' # the path of the 'npc_v4_data.h5' file\n",
    "batch_size = 16 # the batch size of the data loader\n",
    "insp_layer = 'conv3' # the middle layer extracted from alexnet, available in {'conv1', 'conv2', 'conv3', 'conv4', 'conv5'}\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "root_dir = './Celldata_S18_155_L76_211203/stimuli/0_presented_images_800/'\n",
    "resolution = 300\n",
    "image_path = os.listdir(root_dir)\n",
    "path_dict = {}\n",
    "for j in image_path:\n",
    "    key = int(j.split('_')[0])  # 刺激呈现的顺序是图像名称下划线前面的数字顺序。\n",
    "    path_dict[key] = j\n",
    "\n",
    "stim_arr = np.zeros((len(image_path), resolution, resolution, 3))\n",
    "# stim_arr_gray3 = np.zeros((len(image_path), resolution, resolution, 3))\n",
    "for i in range(len(image_path)):\n",
    "    img_bgr = cv2.imread(os.path.join(root_dir, path_dict[i+1]))\n",
    "    stim_arr[i] = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "stim_arr = stim_arr.astype('float32')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4  15  16  18  19  40  42  54  61  78 120 124 126 133 142 146 161 165\n",
      " 166 173 174 178 212 215 222 224 262 278 285 293 305 311 336 351 352 364\n",
      " 368 369 393 400 410 412 420 432 442 463 473 483 484 502 509 510 534 541\n",
      " 552 579 582 599 604 605 610 615 620 622 633 637 672 675 681 683 692 698\n",
      " 701 705 711 722 723 727 787 788] [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79] (800, 299, 299, 3)\n"
     ]
    }
   ],
   "source": [
    "id = h5py.File('./Celldata_S18_155_L76_211203/Random_id_80_2021_12_22.mat', 'r')\n",
    "\n",
    "images_n  = np.zeros(shape=(stim_arr.shape[0], 299, 299, 3))\n",
    "for i in range(stim_arr.shape[0]):\n",
    "    images_n[i] = cv2.resize(stim_arr[i], (299, 299))\n",
    "\n",
    "idx = np.array(id['sampleidlist21']).squeeze().astype('int') - 1\n",
    "idx, unique_idx = np.unique(idx, return_index=True)\n",
    "print(idx, unique_idx, images_n.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 880, 114)\n"
     ]
    }
   ],
   "source": [
    "mat_file = h5py.File('./Celldata_S18_155_L76_211203/celldataS_Natural_objects_800_80_50.mat', 'r')\n",
    "#[num_repetitions, num_images, num_neurons]\n",
    "neural_n = np.transpose(np.array(mat_file['celldataS']), (1, 2, 0)).astype('float16')\n",
    "neural_n = neural_n[:,:880, :]\n",
    "print(neural_n.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800 114 (800, 299, 299, 3)\n"
     ]
    }
   ],
   "source": [
    "n_images = 800\n",
    "n_neurons = neural_n.shape[2]\n",
    "size_imags = images_n.shape[0]\n",
    "print(n_images, n_neurons, images_n.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(880, 12, 114)\n",
      "(720, 114)\n",
      "(80, 114)\n"
     ]
    }
   ],
   "source": [
    "reps = neural_n.shape[0] # trials\n",
    "rand_ind = np.arange(reps)\n",
    "np.random.shuffle(rand_ind)\n",
    "\n",
    "data_y_train = np.concatenate((np.delete(neural_n[:, :800, :], idx, 1), neural_n[:, 880:, :]), 1).mean(0)\n",
    "temp = np.transpose(neural_n, (1, 0, 2))\n",
    "print(temp.shape)\n",
    "data_y_val = np.concatenate((temp[idx], temp[800:880][unique_idx]), 1)\n",
    "data_y_val = np.transpose(data_y_val, (1, 0, 2))\n",
    "data_y_val = np.mean(data_y_val, 0)\n",
    "print(data_y_train.shape)\n",
    "print(data_y_val.shape)\n",
    "\n",
    "#\n",
    "# data_x = images_n[:, np.newaxis].astype(np.float16)\n",
    "# print('images_n', images_n.shape)\n",
    "# data_x = data_x / 255 # (640, 1, 299, 299)\n",
    "# data_x = np.tile(data_x, [1, 3, 1, 1])\n",
    "# print('data_x', data_x.shape)\n",
    "# data_x_train = data_x[:576]\n",
    "# data_x_val = data_x[576:]as indices must be"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 299, 299, 3)\n",
      "(800, 299, 299, 3)\n",
      "(800, 3, 299, 299) (720, 3, 299, 299) (80, 3, 299, 299)\n"
     ]
    }
   ],
   "source": [
    "print(images_n.shape)\n",
    "#data_x = images_n[:, np.newaxis].astype(np.float16)\n",
    "data_x = images_n.astype(np.float16)\n",
    "print(data_x.shape)\n",
    "data_x = data_x / 255 # (800, 1, 299, 299)\n",
    "#data_x = np.tile(data_x, [1, 3, 1, 1])\n",
    "data_x_train = np.delete(images_n, idx, 0)\n",
    "data_x_val = images_n[idx]\n",
    "\n",
    "data_x = np.transpose(data_x, (0, 3, 1, 2))\n",
    "data_x_train = np.transpose(data_x_train, (0, 3, 1, 2))\n",
    "data_x_val = np.transpose(data_x_val, (0, 3, 1, 2))\n",
    "print(data_x.shape, data_x_train.shape, data_x_val.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val: (80, 3, 299, 299), (80, 114)\n",
      "0 torch.Size([16, 3, 299, 299]) torch.Size([16, 114])\n",
      "1 torch.Size([16, 3, 299, 299]) torch.Size([16, 114])\n",
      "2 torch.Size([16, 3, 299, 299]) torch.Size([16, 114])\n",
      "3 torch.Size([16, 3, 299, 299]) torch.Size([16, 114])\n",
      "4 torch.Size([16, 3, 299, 299]) torch.Size([16, 114])\n"
     ]
    }
   ],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_x, data_y):\n",
    "        self.data_x = data_x\n",
    "        self.data_y = data_y\n",
    "    def __getitem__(self, index):\n",
    "        return self.data_x[index], self.data_y[index]\n",
    "    def __len__(self):\n",
    "        return self.data_x.shape[0]\n",
    "\n",
    "imagenet_mean = torch.tensor([0.485, 0.456, 0.406], dtype=torch.float32).view(1, 3, 1, 1).to(device)\n",
    "imagenet_std = torch.tensor([0.229, 0.224, 0.225], dtype=torch.float32).view(1, 3, 1, 1).to(device)\n",
    "transform = lambda x : (x - imagenet_mean) / imagenet_std\n",
    "\n",
    "dataset_train = Dataset(data_x_train, data_y_train)\n",
    "dataset_val = Dataset(data_x_val, data_y_val)\n",
    "\n",
    "loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle = True)\n",
    "loader_val = torch.utils.data.DataLoader(dataset_val, batch_size=batch_size, shuffle = True)\n",
    "\n",
    "print(f'val: {data_x_val.shape}, {data_y_val.shape}')\n",
    "for i,(x,y) in enumerate(loader_val):\n",
    "    print(i, x.shape, y.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([1, 3, 299, 299])\n",
      "fmap:  torch.Size([1, 384, 17, 17])\n",
      "size:  torch.Size([17, 17])\n",
      "114 torch.Size([17, 17])\n",
      "torch.Size([114, 17, 17])\n"
     ]
    }
   ],
   "source": [
    "# CHOOSE THE AUGMENTS IF NECESSARY\n",
    "lamd_s, lamd_d = [5e-3, 2e-3] # the coefficients of the losses. Try other coefficients!\n",
    "epoches = 10 # total epochs for training the encoder\n",
    "lr = 1e-3 # the learing rate for training the encoder\n",
    "\n",
    "alexnet = models.alexnet(pretrained=True)\n",
    "\n",
    "#\n",
    "alexnet.to(device)\n",
    "alexnet.eval()\n",
    "for param in alexnet.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "x = torch.from_numpy(data_x[0:1]).to(device)\n",
    "print(\"x:\", x.shape)\n",
    "x = x.float()\n",
    "fmap = alexnet(x, layer=insp_layer)\n",
    "\n",
    "neurons = data_y_train.shape[1]\n",
    "sizes = fmap.shape[2:]\n",
    "print(\"fmap: \", fmap.shape)\n",
    "print(\"size: \", sizes)\n",
    "channels = fmap.shape[1]\n",
    "print(neurons, sizes)\n",
    "w_s = nn.Parameter(torch.randn(size=(neurons,) + sizes))\n",
    "print(w_s.shape)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "mse_weight = 1.0\n",
    "l1_weight = 0\n",
    "spa_weight = 1e-1\n",
    "ch_weight = 1e-1\n",
    "lap_weight = 1e-1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def mse_loss(prediction, response, weight=None):\n",
    "    if weight is None:\n",
    "        mse_loss = torch.mean(torch.mean((prediction - response)**2, dim=1))\n",
    "    else:\n",
    "        mse_loss = torch.sum(weight*torch.mean((prediction - response)**2, dim=1))\n",
    "    return mse_loss\n",
    "\n",
    "def l2_norm_regularizer(W):\n",
    "    with torch.autograd.profiler.record_function('l2_norm'):\n",
    "        penalty = torch.mean(torch.sum(W**2))\n",
    "        return penalty\n",
    "\n",
    "def l1_norm_regularizer(W):\n",
    "    with torch.autograd.profiler.record_function('l1_norm'):\n",
    "        penalty = torch.mean(torch.sum(torch.abs(W)))\n",
    "        return penalty\n",
    "\n",
    "def smoothness_regularizer_2d(W_s):\n",
    "    K = torch.tensor([\n",
    "    [0,-1,0],\n",
    "    [-1,4,-1],\n",
    "    [0,-1,0]],dtype=torch.float).to(device)\n",
    "    return torch.sum(F.conv2d(torch.unsqueeze(W_s,1),K.unsqueeze(0).unsqueeze(0))**2)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "class conv_encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, neurons, sizes, channels):\n",
    "        super(conv_encoder, self).__init__()\n",
    "        # PUT YOUR CODES HERE\n",
    "        self.W_s = nn.Parameter(torch.randn(size=(neurons,) + sizes))\n",
    "        self.W_d = nn.Parameter(torch.randn(size = (neurons,channels,1,1)))\n",
    "        self.W_b = nn.Parameter(torch.randn(size = (1,neurons)))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # PUT YOUR CODES HERE\n",
    "        out = torch.einsum('bchw , nhw -> bnchw',x,self.W_s) # dimension : N,n,C,h,w\n",
    "        out = torch.stack(\n",
    "            [F.conv2d(out[:,n,:,:,:],torch.unsqueeze(self.W_d[n],0)) for n in range(neurons)],dim=1)\n",
    "            #dimension:N,n,1,h,w\n",
    "        out = torch.sum(out,dim=(2,3,4))\n",
    "        out = out + self.W_b\n",
    "        return out\n",
    "\n",
    "def Loss(y, pred, W_s, W_d):\n",
    "    return mse_loss(y, pred) * mse_weight + \\\n",
    "          l2_norm_regularizer(W_s) * spa_weight + \\\n",
    "          smoothness_regularizer_2d(W_s) * lap_weight + \\\n",
    "          l2_norm_regularizer(W_d) * ch_weight\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#encoder = conv_encoder(neurons, sizes, channels).to(device)\n",
    "encoder = conv_encoder(neurons, sizes, channels).to(device)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\n    You need to define the conv_encoder() class and train the encoder.\\n    The code of alexnet has been slightly modified from the torchvision, for convenience\\n    of extracting the middle layers.\\n\\n    Example:\\n        >>> x = x.to(device) # x is a batch of images\\n        >>> x = transform(x)\\n        >>> fmap = alexnet(x, layer=insp_layer)\\n        >>> out= encoder(fmap)\\n        >>> ...\\n'"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_model(encoder, optimizer):\n",
    "    losses = []\n",
    "    encoder.train()\n",
    "    for i,(x,y) in enumerate(loader_train):\n",
    "        optimizer.zero_grad()\n",
    "        x = x.float()\n",
    "        y = y.float()\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        x = transform(x)\n",
    "        x = x.float()\n",
    "        fmap = alexnet(x,layer = insp_layer)\n",
    "        out = encoder(fmap)\n",
    "#         print(f'L_e = {l_e} , L_2 = {l_2} , L_l = {l_l}')\n",
    "        loss = Loss(y, out, encoder.W_s, encoder.W_d)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "#         print(f'iteration {i}, train loss: {losses[-1]}')\n",
    "\n",
    "    return losses\n",
    "\n",
    "def validate_model(encoder):\n",
    "    encoder.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    losses = []\n",
    "    for i,(x,y) in enumerate(loader_val):\n",
    "        x = x.float()\n",
    "        y = y.float()\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        x = transform(x)\n",
    "        x = x.float()\n",
    "        fmap = alexnet(x,layer = insp_layer)\n",
    "        out = encoder(fmap)\n",
    "        y_pred.append(out)\n",
    "        y_true.append(y)\n",
    "        loss = Loss(y, out, encoder.W_s, encoder.W_d)\n",
    "        losses.append(loss.item())\n",
    "    y_pred = torch.cat(y_pred)\n",
    "    y_true = torch.cat(y_true)\n",
    "    explained_variance = metrics.explained_variance_score(y_true = y_true.detach().cpu().numpy(),y_pred = y_pred.detach().cpu().numpy())\n",
    "    return explained_variance,sum(losses)/len(losses)\n",
    "\n",
    "\"\"\"\n",
    "    You need to define the conv_encoder() class and train the encoder.\n",
    "    The code of alexnet has been slightly modified from the torchvision, for convenience\n",
    "    of extracting the middle layers.\n",
    "\n",
    "    Example:\n",
    "        >>> x = x.to(device) # x is a batch of images\n",
    "        >>> x = transform(x)\n",
    "        >>> fmap = alexnet(x, layer=insp_layer)\n",
    "        >>> out= encoder(fmap)\n",
    "        >>> ...\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# losses_train = []\n",
    "# losses_val = []\n",
    "# EVs = []\n",
    "\n",
    "losses_train = []\n",
    "losses_val = []\n",
    "EVs = []"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "optimizer = torch.optim.Adam(encoder.parameters(), lr=lr)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\envs\\neural_control\\lib\\site-packages\\ipykernel_launcher.py:2: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/500 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "97a555c9c21d47809c94e59c3e4f3c9b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, EV = -5326180330154.667, val  loss = 146733495091.2 , train loss 155736283545.6\n",
      "epoch 1, EV = -3722943490155.7896, val  loss = 102442138009.6 , train loss 104027371110.4\n",
      "epoch 2, EV = -2804483840610.807, val  loss = 78599441612.8 , train loss 81793497907.2\n",
      "epoch 3, EV = -2242119957665.684, val  loss = 63250720358.4 , train loss 56414336204.8\n",
      "epoch 4, EV = -1852909926579.6492, val  loss = 52559514009.6 , train loss 44581095014.4\n",
      "epoch 5, EV = -1584987687594.6667, val  loss = 45106200576.0 , train loss 39098322534.4\n",
      "epoch 6, EV = -1375020561174.456, val  loss = 39253991424.0 , train loss 31694286848.0\n",
      "epoch 7, EV = -1211206441714.5264, val  loss = 34711903027.2 , train loss 27405270835.2\n",
      "epoch 8, EV = -1083520428642.807, val  loss = 31060685209.6 , train loss 22953081241.6\n",
      "epoch 9, EV = -978899990204.6316, val  loss = 28006658048.0 , train loss 21962179481.6\n",
      "epoch 10, EV = -889310820819.0878, val  loss = 25497038848.0 , train loss 19156968140.8\n",
      "epoch 11, EV = -815662778044.6316, val  loss = 23359261081.6 , train loss 16583466700.8\n",
      "epoch 12, EV = -750438187079.8596, val  loss = 21499080499.2 , train loss 15659525632.0\n",
      "epoch 13, EV = -698601365504.0, val  loss = 19939917619.2 , train loss 14307460300.8\n",
      "epoch 14, EV = -648734662009.2632, val  loss = 18507741184.0 , train loss 13019642777.6\n",
      "epoch 15, EV = -605624969898.6666, val  loss = 17291697971.2 , train loss 11714503065.6\n",
      "epoch 16, EV = -567662017877.3334, val  loss = 16184461107.2 , train loss 10787191142.4\n",
      "epoch 17, EV = -534493623745.1228, val  loss = 15171023052.8 , train loss 10395588403.2\n",
      "epoch 18, EV = -503897734269.7544, val  loss = 14318108262.4 , train loss 8962086912.0\n",
      "epoch 19, EV = -475202503302.7368, val  loss = 13507278028.8 , train loss 8644081817.6\n",
      "epoch 20, EV = -452097025544.9825, val  loss = 12789906432.0 , train loss 8184845209.6\n",
      "epoch 21, EV = -428030312735.4386, val  loss = 12131674726.4 , train loss 7404878387.2\n",
      "epoch 22, EV = -406349335246.5965, val  loss = 11511692288.0 , train loss 6836522752.0\n",
      "epoch 23, EV = -387158374256.2807, val  loss = 10969075507.2 , train loss 6764311500.8\n",
      "epoch 24, EV = -369835095668.7719, val  loss = 10443014963.2 , train loss 6466086860.8\n",
      "epoch 25, EV = -353523300244.2105, val  loss = 9985242931.2 , train loss 5979446118.4\n",
      "epoch 26, EV = -337184055834.9474, val  loss = 9518661734.4 , train loss 5696016742.4\n",
      "epoch 27, EV = -323858622967.0175, val  loss = 9123694694.4 , train loss 5354950502.4\n",
      "epoch 28, EV = -310210419604.2105, val  loss = 8752193024.0 , train loss 5074584883.2\n",
      "epoch 29, EV = -297866446901.8947, val  loss = 8388213043.2 , train loss 4866271360.0\n",
      "epoch 30, EV = -286314483550.3158, val  loss = 8073321881.6 , train loss 4738737152.0\n",
      "epoch 31, EV = -274852574531.3684, val  loss = 7742754611.2 , train loss 4101937049.6\n",
      "epoch 32, EV = -264887495392.5614, val  loss = 7451237785.6 , train loss 4311311564.8\n",
      "epoch 33, EV = -255107052418.2456, val  loss = 7174809600.0 , train loss 3934515660.8\n",
      "epoch 34, EV = -246231070181.05264, val  loss = 6931331481.6 , train loss 3635316864.0\n",
      "epoch 35, EV = -237434945679.7193, val  loss = 6684841676.8 , train loss 3611262156.8\n",
      "epoch 36, EV = -229399764920.14035, val  loss = 6452425113.6 , train loss 3496370611.2\n",
      "epoch 37, EV = -221860273457.4035, val  loss = 6231102873.6 , train loss 3359881548.8\n",
      "epoch 38, EV = -213959764345.26315, val  loss = 6019715993.6 , train loss 3219221324.8\n",
      "epoch 39, EV = -207603033393.4035, val  loss = 5832272384.0 , train loss 2980847436.8\n",
      "epoch 40, EV = -200752784833.1228, val  loss = 5644971417.6 , train loss 2789641984.0\n",
      "epoch 41, EV = -194689076996.49124, val  loss = 5475046707.2 , train loss 2801632896.0\n",
      "epoch 42, EV = -188721071624.98245, val  loss = 5307677696.0 , train loss 2734325299.2\n",
      "epoch 43, EV = -183010454671.7193, val  loss = 5146144409.6 , train loss 2510096793.6\n",
      "epoch 44, EV = -177306530232.14035, val  loss = 4988743168.0 , train loss 2514292224.0\n",
      "epoch 45, EV = -172439602409.54385, val  loss = 4852100608.0 , train loss 2423331993.6\n",
      "epoch 46, EV = -167647584803.9298, val  loss = 4711038208.0 , train loss 2317698304.0\n",
      "epoch 47, EV = -162705908538.38596, val  loss = 4576729088.0 , train loss 2234317811.2\n",
      "epoch 48, EV = -158078204281.26315, val  loss = 4448697139.2 , train loss 2083227468.8\n",
      "epoch 49, EV = -153669208818.5263, val  loss = 4329718374.4 , train loss 2029084300.8\n",
      "epoch 50, EV = -149381980088.14035, val  loss = 4208820428.8 , train loss 2013996902.4\n",
      "epoch 51, EV = -145239184410.94736, val  loss = 4091814144.0 , train loss 1876861337.6\n",
      "epoch 52, EV = -141617545799.85965, val  loss = 3982881382.4 , train loss 1770496537.6\n",
      "epoch 53, EV = -137940511384.70175, val  loss = 3882893516.8 , train loss 1765463846.4\n",
      "epoch 54, EV = -134176095699.08772, val  loss = 3779754752.0 , train loss 1685455232.0\n",
      "epoch 55, EV = -130799993667.36842, val  loss = 3688036556.8 , train loss 1642350976.0\n",
      "epoch 56, EV = -127682063171.36842, val  loss = 3593111961.6 , train loss 1618311219.2\n",
      "epoch 57, EV = -124403532916.77193, val  loss = 3507467980.8 , train loss 1604663539.2\n",
      "epoch 58, EV = -121367495033.26315, val  loss = 3418409267.2 , train loss 1472017241.6\n",
      "epoch 59, EV = -118367511938.24562, val  loss = 3337468876.8 , train loss 1430524019.2\n",
      "epoch 60, EV = -115321611722.10527, val  loss = 3253623091.2 , train loss 1404559603.2\n",
      "epoch 61, EV = -112800641814.45615, val  loss = 3181178982.4 , train loss 1336695846.4\n",
      "epoch 62, EV = -109960538713.82455, val  loss = 3098777241.6 , train loss 1262353536.0\n",
      "epoch 63, EV = -107456457431.57895, val  loss = 3029653964.8 , train loss 1278688998.4\n",
      "epoch 64, EV = -104876636986.38597, val  loss = 2962988390.4 , train loss 1221653862.4\n",
      "epoch 65, EV = -102555939857.9649, val  loss = 2891581747.2 , train loss 1176755200.0\n",
      "epoch 66, EV = -100210098193.9649, val  loss = 2823623782.4 , train loss 1170270752.0\n",
      "epoch 67, EV = -97703135690.10527, val  loss = 2759878425.6 , train loss 1113387974.4\n",
      "epoch 68, EV = -95648777224.98245, val  loss = 2698796595.2 , train loss 1129064844.8\n",
      "epoch 69, EV = -93313592320.0, val  loss = 2641638988.8 , train loss 1033040588.8\n",
      "epoch 70, EV = -91439787264.0, val  loss = 2582528358.4 , train loss 996248000.0\n",
      "epoch 71, EV = -89349713196.91228, val  loss = 2523168588.8 , train loss 966764224.0\n",
      "epoch 72, EV = -87454308801.1228, val  loss = 2469530931.2 , train loss 980625248.0\n",
      "epoch 73, EV = -85665987471.7193, val  loss = 2416903065.6 , train loss 946101158.4\n",
      "epoch 74, EV = -83933476136.42105, val  loss = 2374246502.4 , train loss 893304256.0\n",
      "epoch 75, EV = -82007909178.38597, val  loss = 2313986585.6 , train loss 908694169.6\n",
      "epoch 76, EV = -80154573644.35088, val  loss = 2262120550.4 , train loss 856664697.6\n",
      "epoch 77, EV = -78484481536.0, val  loss = 2218092646.4 , train loss 834297292.8\n",
      "epoch 78, EV = -76756920086.45615, val  loss = 2169880268.8 , train loss 787279456.0\n",
      "epoch 79, EV = -75378356915.64912, val  loss = 2130975744.0 , train loss 770697984.0\n",
      "epoch 80, EV = -73768782681.82455, val  loss = 2082307302.4 , train loss 782012006.4\n",
      "epoch 81, EV = -72444313784.14035, val  loss = 2046740019.2 , train loss 747559404.8\n",
      "epoch 82, EV = -70891229480.42105, val  loss = 2003633868.8 , train loss 754798598.4\n",
      "epoch 83, EV = -69438096469.33333, val  loss = 1961142220.8 , train loss 713171948.8\n",
      "epoch 84, EV = -68012784181.89474, val  loss = 1923819468.8 , train loss 685322176.0\n",
      "epoch 85, EV = -66756268391.29825, val  loss = 1882489907.2 , train loss 664027910.4\n",
      "epoch 86, EV = -65361153540.49123, val  loss = 1848569779.2 , train loss 641409113.6\n",
      "epoch 87, EV = -64067464578.24561, val  loss = 1810468864.0 , train loss 623702585.6\n",
      "epoch 88, EV = -62885159729.40351, val  loss = 1778263500.8 , train loss 618428198.4\n",
      "epoch 89, EV = -61726779715.36842, val  loss = 1741910272.0 , train loss 588584009.6\n",
      "epoch 90, EV = -60408598640.2807, val  loss = 1707309004.8 , train loss 601069600.0\n",
      "epoch 91, EV = -59324829722.947365, val  loss = 1673918643.2 , train loss 565599136.0\n",
      "epoch 92, EV = -58181260674.24561, val  loss = 1641506176.0 , train loss 545752108.8\n",
      "epoch 93, EV = -57000919992.14035, val  loss = 1610500633.6 , train loss 540360912.0\n",
      "epoch 94, EV = -55989911938.24561, val  loss = 1581085900.8 , train loss 512760067.2\n",
      "epoch 95, EV = -54923971341.47369, val  loss = 1552534451.2 , train loss 503643193.6\n",
      "epoch 96, EV = -53803095376.8421, val  loss = 1519725977.6 , train loss 496753996.8\n",
      "epoch 97, EV = -52882615834.947365, val  loss = 1494515072.0 , train loss 471881216.0\n",
      "epoch 98, EV = -51929381744.2807, val  loss = 1466689996.8 , train loss 456666640.0\n",
      "epoch 99, EV = -50835340357.61404, val  loss = 1437379660.8 , train loss 458335744.0\n",
      "epoch 100, EV = -49805767605.89474, val  loss = 1407449907.2 , train loss 439123811.2\n",
      "epoch 101, EV = -48991949749.89474, val  loss = 1385325619.2 , train loss 446336025.6\n",
      "epoch 102, EV = -48106169564.070175, val  loss = 1360440550.4 , train loss 412867910.4\n",
      "epoch 103, EV = -47234106650.947365, val  loss = 1335044582.4 , train loss 412844108.8\n",
      "epoch 104, EV = -46365578394.947365, val  loss = 1309112473.6 , train loss 390182940.8\n",
      "epoch 105, EV = -45498456497.40351, val  loss = 1286004454.4 , train loss 374481513.6\n",
      "epoch 106, EV = -44650008023.57895, val  loss = 1262690368.0 , train loss 390389043.2\n",
      "epoch 107, EV = -43874024230.17544, val  loss = 1238598208.0 , train loss 369872595.2\n",
      "epoch 108, EV = -43081771268.49123, val  loss = 1217495513.6 , train loss 364100016.0\n",
      "epoch 109, EV = -42328524144.2807, val  loss = 1193645196.8 , train loss 363975244.8\n",
      "epoch 110, EV = -41667956724.77193, val  loss = 1178560985.6 , train loss 349625760.0\n",
      "epoch 111, EV = -40825912863.4386, val  loss = 1152306956.8 , train loss 338049392.0\n",
      "epoch 112, EV = -40003959127.57895, val  loss = 1132254988.8 , train loss 330139974.4\n",
      "epoch 113, EV = -39278604737.12281, val  loss = 1108987545.6 , train loss 317519145.6\n",
      "epoch 114, EV = -38609880373.89474, val  loss = 1091669452.8 , train loss 319459804.8\n",
      "epoch 115, EV = -37887837184.0, val  loss = 1073958656.0 , train loss 301500428.8\n",
      "epoch 116, EV = -37261388366.59649, val  loss = 1053021926.4 , train loss 290678820.8\n",
      "epoch 117, EV = -36628339449.26316, val  loss = 1032982950.4 , train loss 285964352.0\n",
      "epoch 118, EV = -35926671380.210526, val  loss = 1014303628.8 , train loss 276659036.8\n",
      "epoch 119, EV = -35285305768.42105, val  loss = 998153472.0 , train loss 278398331.2\n",
      "epoch 120, EV = -34718917932.91228, val  loss = 979918848.0 , train loss 267774899.2\n",
      "epoch 121, EV = -34108959047.85965, val  loss = 964604454.4 , train loss 259424284.8\n",
      "epoch 122, EV = -33483697342.877193, val  loss = 944761331.2 , train loss 256727608.0\n",
      "epoch 123, EV = -33019500373.333332, val  loss = 933651545.6 , train loss 254650553.6\n",
      "epoch 124, EV = -32311247411.649124, val  loss = 915507814.4 , train loss 243861251.2\n",
      "epoch 125, EV = -31677904181.894737, val  loss = 897415180.8 , train loss 235303252.8\n",
      "epoch 126, EV = -31166526149.614037, val  loss = 883193651.2 , train loss 232798411.2\n",
      "epoch 127, EV = -30750905784.14035, val  loss = 868205094.4 , train loss 230478643.2\n",
      "epoch 128, EV = -30197166136.14035, val  loss = 852986649.6 , train loss 226458403.2\n",
      "epoch 129, EV = -29559275322.385963, val  loss = 837603660.8 , train loss 214086648.0\n",
      "epoch 130, EV = -29101849110.45614, val  loss = 823687180.8 , train loss 210572523.2\n",
      "epoch 131, EV = -28632796966.175438, val  loss = 810337996.8 , train loss 203968836.8\n",
      "epoch 132, EV = -28152815380.210526, val  loss = 795667136.0 , train loss 201821763.2\n",
      "epoch 133, EV = -27708028694.45614, val  loss = 783306752.0 , train loss 192813721.6\n",
      "epoch 134, EV = -27110598110.31579, val  loss = 768301516.8 , train loss 188835484.8\n",
      "epoch 135, EV = -26662545320.42105, val  loss = 756162611.2 , train loss 183198353.6\n",
      "epoch 136, EV = -26191914731.789474, val  loss = 740300979.2 , train loss 177602640.0\n",
      "epoch 137, EV = -25806446562.80702, val  loss = 731420940.8 , train loss 182877355.2\n",
      "epoch 138, EV = -25315675982.596493, val  loss = 716796032.0 , train loss 170626385.6\n",
      "epoch 139, EV = -24906251528.982456, val  loss = 705062732.8 , train loss 168726555.2\n",
      "epoch 140, EV = -24479982558.31579, val  loss = 693774272.0 , train loss 163533736.0\n",
      "epoch 141, EV = -24083564203.789474, val  loss = 681902681.6 , train loss 160934734.4\n",
      "epoch 142, EV = -23624823859.649124, val  loss = 668611276.8 , train loss 158845204.8\n",
      "epoch 143, EV = -23235349213.19298, val  loss = 658022393.6 , train loss 153075075.2\n",
      "epoch 144, EV = -22886445035.789474, val  loss = 647163686.4 , train loss 150392555.2\n",
      "epoch 145, EV = -22432637182.877193, val  loss = 635653580.8 , train loss 146018846.4\n",
      "epoch 146, EV = -22031829392.842106, val  loss = 625989484.8 , train loss 142549360.0\n",
      "epoch 147, EV = -21685386255.7193, val  loss = 615601683.2 , train loss 137996236.8\n",
      "epoch 148, EV = -21289276837.05263, val  loss = 603883200.0 , train loss 134332429.6\n",
      "epoch 149, EV = -20960491192.14035, val  loss = 596007488.0 , train loss 130770768.8\n",
      "epoch 150, EV = -20653749730.80702, val  loss = 585622713.6 , train loss 124601848.0\n",
      "epoch 151, EV = -20303776231.298244, val  loss = 575900780.8 , train loss 128264144.0\n",
      "epoch 152, EV = -20009933868.91228, val  loss = 566494489.6 , train loss 125416602.4\n",
      "epoch 153, EV = -19603248337.964912, val  loss = 556082675.2 , train loss 121562233.6\n",
      "epoch 154, EV = -19271586350.035088, val  loss = 545973977.6 , train loss 116121677.6\n",
      "epoch 155, EV = -18977689831.298244, val  loss = 538230880.0 , train loss 113713624.0\n",
      "epoch 156, EV = -18591675700.77193, val  loss = 528201964.8 , train loss 110341536.8\n",
      "epoch 157, EV = -18282348544.0, val  loss = 519941286.4 , train loss 111062006.4\n",
      "epoch 158, EV = -18031912209.964912, val  loss = 511804224.0 , train loss 109786972.8\n",
      "epoch 159, EV = -17664950386.526318, val  loss = 501651308.8 , train loss 103838621.6\n",
      "epoch 160, EV = -17454323904.0, val  loss = 494868672.0 , train loss 102353584.8\n",
      "epoch 161, EV = -17124897518.035088, val  loss = 486647852.8 , train loss 100781649.6\n",
      "epoch 162, EV = -16835678570.666666, val  loss = 478512012.8 , train loss 97930494.4\n",
      "epoch 163, EV = -16537834371.368422, val  loss = 470417753.6 , train loss 94302677.6\n",
      "epoch 164, EV = -16293587188.77193, val  loss = 465246617.6 , train loss 92694190.4\n",
      "epoch 165, EV = -15993804089.263159, val  loss = 456165945.6 , train loss 90315510.4\n",
      "epoch 166, EV = -15747912049.40351, val  loss = 447389696.0 , train loss 89570316.8\n",
      "epoch 167, EV = -15512400901.614035, val  loss = 441574502.4 , train loss 83417463.2\n",
      "epoch 168, EV = -15298119444.210526, val  loss = 433595424.0 , train loss 88469362.4\n",
      "epoch 169, EV = -14988903367.85965, val  loss = 425829907.2 , train loss 82433044.8\n",
      "epoch 170, EV = -14761420446.31579, val  loss = 419222067.2 , train loss 79992859.2\n",
      "epoch 171, EV = -14433422974.877193, val  loss = 411650892.8 , train loss 76604649.6\n",
      "epoch 172, EV = -14285190536.982456, val  loss = 405232633.6 , train loss 76566684.4\n",
      "epoch 173, EV = -13980906534.175438, val  loss = 398682105.6 , train loss 76198224.8\n",
      "epoch 174, EV = -13852865275.508772, val  loss = 392370892.8 , train loss 75442416.0\n",
      "epoch 175, EV = -13608541303.017544, val  loss = 386931916.8 , train loss 71906262.4\n",
      "epoch 176, EV = -13293651676.631578, val  loss = 378342259.2 , train loss 69693055.6\n",
      "epoch 177, EV = -13113768759.578947, val  loss = 372728070.4 , train loss 68013169.6\n",
      "epoch 178, EV = -12863697861.052631, val  loss = 366474668.8 , train loss 67525062.8\n",
      "epoch 179, EV = -12702031288.701754, val  loss = 361896966.4 , train loss 66332061.6\n",
      "epoch 180, EV = -12517551952.842106, val  loss = 355841088.0 , train loss 62230352.0\n",
      "epoch 181, EV = -12312676808.982456, val  loss = 350267971.2 , train loss 62989452.8\n",
      "epoch 182, EV = -12086714841.824562, val  loss = 343884006.4 , train loss 61414761.6\n",
      "epoch 183, EV = -11886678511.157894, val  loss = 338204339.2 , train loss 60912578.4\n",
      "epoch 184, EV = -11668509412.491228, val  loss = 332621843.2 , train loss 57559814.8\n",
      "epoch 185, EV = -11510363680.0, val  loss = 327834931.2 , train loss 57991864.8\n",
      "epoch 186, EV = -11261619718.175438, val  loss = 320706787.2 , train loss 54909314.4\n",
      "epoch 187, EV = -11133016230.736841, val  loss = 317242329.6 , train loss 54019129.2\n",
      "epoch 188, EV = -10919434167.578947, val  loss = 310474758.4 , train loss 52591468.8\n",
      "epoch 189, EV = -10785872353.68421, val  loss = 306437017.6 , train loss 51769684.4\n",
      "epoch 190, EV = -10618738980.491228, val  loss = 302337740.8 , train loss 50169476.8\n",
      "epoch 191, EV = -10417876819.649122, val  loss = 297132339.2 , train loss 50017418.4\n",
      "epoch 192, EV = -10250759408.842106, val  loss = 291773686.4 , train loss 49875613.6\n",
      "epoch 193, EV = -10091313855.438597, val  loss = 287846412.8 , train loss 47271743.6\n",
      "epoch 194, EV = -9906384698.947369, val  loss = 282075952.0 , train loss 46556284.4\n",
      "epoch 195, EV = -9760634010.947369, val  loss = 278149712.0 , train loss 46377586.8\n",
      "epoch 196, EV = -9616424342.45614, val  loss = 273726038.4 , train loss 45021588.0\n",
      "epoch 197, EV = -9445240954.947369, val  loss = 268930976.0 , train loss 43396612.0\n",
      "epoch 198, EV = -9269427746.807018, val  loss = 263979715.2 , train loss 42003888.0\n",
      "epoch 199, EV = -9122331742.31579, val  loss = 260296880.0 , train loss 41474417.2\n",
      "epoch 200, EV = -8992501302.45614, val  loss = 256392716.8 , train loss 40905302.8\n",
      "epoch 201, EV = -8814703185.40351, val  loss = 252165468.8 , train loss 38822830.0\n",
      "epoch 202, EV = -8695407201.68421, val  loss = 248427219.2 , train loss 38574465.6\n",
      "epoch 203, EV = -8565075139.929825, val  loss = 244951094.4 , train loss 37576794.8\n",
      "epoch 204, EV = -8419228127.438597, val  loss = 239702121.6 , train loss 37165060.6\n",
      "epoch 205, EV = -8244971991.298245, val  loss = 235419664.0 , train loss 35038184.4\n",
      "epoch 206, EV = -8160990382.035088, val  loss = 233346227.2 , train loss 35177388.2\n",
      "epoch 207, EV = -7988004769.122807, val  loss = 228242275.2 , train loss 34812351.6\n",
      "epoch 208, EV = -7861581975.578947, val  loss = 223763296.0 , train loss 34438612.2\n",
      "epoch 209, EV = -7835445271.017544, val  loss = 222260608.0 , train loss 32405319.0\n",
      "epoch 210, EV = -7609659312.842105, val  loss = 217086329.6 , train loss 32071058.6\n",
      "epoch 211, EV = -7527926450.526316, val  loss = 214221212.8 , train loss 32238728.6\n",
      "epoch 212, EV = -7379836022.736842, val  loss = 209784796.8 , train loss 30897186.4\n",
      "epoch 213, EV = -7286565638.736842, val  loss = 207506499.2 , train loss 29964154.4\n",
      "epoch 214, EV = -7164394065.403509, val  loss = 204207417.6 , train loss 29669016.4\n",
      "epoch 215, EV = -7070077401.824561, val  loss = 200745945.6 , train loss 29100616.0\n",
      "epoch 216, EV = -6960081649.964912, val  loss = 197780988.8 , train loss 29008085.6\n",
      "epoch 217, EV = -6829241206.736842, val  loss = 195050742.4 , train loss 27934772.8\n",
      "epoch 218, EV = -6704625734.736842, val  loss = 190865123.2 , train loss 26632053.4\n",
      "epoch 219, EV = -6583137728.0, val  loss = 188119904.0 , train loss 27463983.0\n",
      "epoch 220, EV = -6465377071.719298, val  loss = 185101993.6 , train loss 25634222.4\n",
      "epoch 221, EV = -6408130997.894737, val  loss = 182609728.0 , train loss 25132722.4\n",
      "epoch 222, EV = -6263436074.385965, val  loss = 178827872.0 , train loss 24916344.8\n",
      "epoch 223, EV = -6192697094.175439, val  loss = 176407696.0 , train loss 23517597.2\n",
      "epoch 224, EV = -6090743850.947369, val  loss = 172933814.4 , train loss 23953762.0\n",
      "epoch 225, EV = -5981419609.824561, val  loss = 170723500.8 , train loss 23711657.8\n",
      "epoch 226, EV = -5895516030.315789, val  loss = 168932224.0 , train loss 22942007.0\n",
      "epoch 227, EV = -5782090704.280702, val  loss = 164274742.4 , train loss 21522860.4\n",
      "epoch 228, EV = -5680660985.824561, val  loss = 161975475.2 , train loss 21402543.8\n",
      "epoch 229, EV = -5576961758.596491, val  loss = 158899579.2 , train loss 21031480.8\n",
      "epoch 230, EV = -5514147060.77193, val  loss = 157114388.8 , train loss 20639191.0\n",
      "epoch 231, EV = -5426087663.719298, val  loss = 154765798.4 , train loss 20080484.6\n",
      "epoch 232, EV = -5311211934.035088, val  loss = 151508076.8 , train loss 20115744.4\n",
      "epoch 233, EV = -5263992784.280702, val  loss = 149975713.6 , train loss 19382995.0\n",
      "epoch 234, EV = -5154921441.964912, val  loss = 147459516.8 , train loss 18846335.4\n",
      "epoch 235, EV = -5064624002.526316, val  loss = 144582060.8 , train loss 18560713.0\n",
      "epoch 236, EV = -4985051780.77193, val  loss = 142506508.8 , train loss 18054838.6\n",
      "epoch 237, EV = -4897813103.719298, val  loss = 140156600.0 , train loss 17857034.2\n",
      "epoch 238, EV = -4821478707.087719, val  loss = 137588656.0 , train loss 18247488.0\n",
      "epoch 239, EV = -4757168815.157895, val  loss = 136164342.4 , train loss 16911189.5\n",
      "epoch 240, EV = -4674583279.719298, val  loss = 133297904.0 , train loss 16502145.5\n",
      "epoch 241, EV = -4620189228.350877, val  loss = 131655344.0 , train loss 16436946.9\n",
      "epoch 242, EV = -4510664515.649123, val  loss = 128846083.2 , train loss 16341043.1\n",
      "epoch 243, EV = -4442054291.368421, val  loss = 126567635.2 , train loss 15889117.4\n",
      "epoch 244, EV = -4393055720.0, val  loss = 124853112.0 , train loss 15696417.4\n",
      "epoch 245, EV = -4320598350.596491, val  loss = 123291518.4 , train loss 15622659.8\n",
      "epoch 246, EV = -4232198324.0701756, val  loss = 121072062.4 , train loss 15003972.0\n",
      "epoch 247, EV = -4186127851.6491227, val  loss = 119383540.8 , train loss 14254766.9\n",
      "epoch 248, EV = -4135655032.8421054, val  loss = 118086796.8 , train loss 14448888.6\n",
      "epoch 249, EV = -4035433826.245614, val  loss = 115322102.4 , train loss 14012373.2\n",
      "epoch 250, EV = -4004534234.9473686, val  loss = 113670241.6 , train loss 13440142.0\n",
      "epoch 251, EV = -3947700657.122807, val  loss = 112319344.0 , train loss 13295435.6\n",
      "epoch 252, EV = -3852696929.6842103, val  loss = 110414488.0 , train loss 13203001.7\n",
      "epoch 253, EV = -3797199211.368421, val  loss = 108189904.0 , train loss 13017989.1\n",
      "epoch 254, EV = -3743284192.5614033, val  loss = 106360390.4 , train loss 12476049.5\n",
      "epoch 255, EV = -3647192883.9298244, val  loss = 104038561.6 , train loss 12286309.4\n",
      "epoch 256, EV = -3602865531.6491227, val  loss = 103065744.0 , train loss 12257249.7\n",
      "epoch 257, EV = -3546595821.614035, val  loss = 100887392.0 , train loss 11726507.7\n",
      "epoch 258, EV = -3503618304.4210525, val  loss = 99902697.6 , train loss 11545005.7\n",
      "epoch 259, EV = -3439329154.8070173, val  loss = 98279428.8 , train loss 11531413.9\n",
      "epoch 260, EV = -3390426324.0701756, val  loss = 96772446.4 , train loss 11014921.7\n",
      "epoch 261, EV = -3346858742.1754384, val  loss = 95265998.4 , train loss 11131839.5\n",
      "epoch 262, EV = -3255019788.0701756, val  loss = 93059078.4 , train loss 10552201.1\n",
      "epoch 263, EV = -3197568795.508772, val  loss = 91436500.8 , train loss 10573688.5\n",
      "epoch 264, EV = -3131692223.1578946, val  loss = 89772718.4 , train loss 10046971.7\n",
      "epoch 265, EV = -3115072639.7192984, val  loss = 88638296.0 , train loss 10086239.7\n",
      "epoch 266, EV = -3059623164.491228, val  loss = 87334904.0 , train loss 9755618.4\n",
      "epoch 267, EV = -2996143327.017544, val  loss = 85645358.4 , train loss 9769266.8\n",
      "epoch 268, EV = -2979147849.263158, val  loss = 84655483.2 , train loss 9384480.5\n",
      "epoch 269, EV = -2929253934.0350876, val  loss = 83387980.8 , train loss 9499013.0\n",
      "epoch 270, EV = -2841688834.6666665, val  loss = 81031161.6 , train loss 8998921.6\n",
      "epoch 271, EV = -2807752636.0701756, val  loss = 80216968.8 , train loss 8802463.45\n",
      "epoch 272, EV = -2762677820.631579, val  loss = 79162974.4 , train loss 8969711.65\n",
      "epoch 273, EV = -2730604986.9473686, val  loss = 77802139.2 , train loss 8559423.25\n",
      "epoch 274, EV = -2706192571.368421, val  loss = 77152845.6 , train loss 8455725.1\n",
      "epoch 275, EV = -2654212860.631579, val  loss = 75685624.0 , train loss 8419208.9\n",
      "epoch 276, EV = -2602774797.1929827, val  loss = 73831980.0 , train loss 8077991.55\n",
      "epoch 277, EV = -2547979146.245614, val  loss = 72739752.8 , train loss 8008477.75\n",
      "epoch 278, EV = -2504669660.631579, val  loss = 71720930.4 , train loss 7877903.55\n",
      "epoch 279, EV = -2463520186.877193, val  loss = 70051227.2 , train loss 7723182.55\n",
      "epoch 280, EV = -2414710066.105263, val  loss = 69287790.4 , train loss 7492255.3\n",
      "epoch 281, EV = -2392806792.982456, val  loss = 68287282.4 , train loss 7269945.5\n",
      "epoch 282, EV = -2344838715.7894735, val  loss = 67015911.2 , train loss 7185713.5\n",
      "epoch 283, EV = -2332204672.5614033, val  loss = 66637843.2 , train loss 7138598.35\n",
      "epoch 284, EV = -2258859158.5964913, val  loss = 64719334.4 , train loss 6954423.15\n",
      "epoch 285, EV = -2251422454.8070173, val  loss = 63867022.4 , train loss 6738249.75\n",
      "epoch 286, EV = -2192240079.859649, val  loss = 62603245.6 , train loss 6548900.5\n",
      "epoch 287, EV = -2155601326.1754384, val  loss = 61267033.6 , train loss 6402390.2\n",
      "epoch 288, EV = -2118370080.2807016, val  loss = 60476239.2 , train loss 6245266.95\n",
      "epoch 289, EV = -2092235026.9473684, val  loss = 59183081.6 , train loss 6069325.0\n",
      "epoch 290, EV = -2051065015.0877192, val  loss = 58510500.8 , train loss 6071874.05\n",
      "epoch 291, EV = -2021089692.5614035, val  loss = 57990497.6 , train loss 5964822.75\n",
      "epoch 292, EV = -1994347919.1578948, val  loss = 56677467.2 , train loss 5957079.8\n",
      "epoch 293, EV = -1961083390.4561403, val  loss = 55812759.2 , train loss 5728364.9\n",
      "epoch 294, EV = -1919939129.1929824, val  loss = 54510431.2 , train loss 5645682.35\n",
      "epoch 295, EV = -1883376848.0701754, val  loss = 53704577.6 , train loss 5562639.15\n",
      "epoch 296, EV = -1877685260.0, val  loss = 53082877.6 , train loss 5560330.15\n",
      "epoch 297, EV = -1834147002.877193, val  loss = 52055624.8 , train loss 5473567.0\n",
      "epoch 298, EV = -1806179766.9473684, val  loss = 51178884.0 , train loss 5480106.15\n",
      "epoch 299, EV = -1766891380.0, val  loss = 50326721.6 , train loss 5273701.0\n",
      "epoch 300, EV = -1744927971.8596492, val  loss = 49986074.4 , train loss 5264047.9\n",
      "epoch 301, EV = -1714841766.5964913, val  loss = 48972300.8 , train loss 5150632.55\n",
      "epoch 302, EV = -1684996291.2280703, val  loss = 48081772.8 , train loss 5158565.65\n",
      "epoch 303, EV = -1636178966.0350878, val  loss = 46603213.6 , train loss 4904312.6\n",
      "epoch 304, EV = -1607486786.5263157, val  loss = 45781603.2 , train loss 4817749.15\n",
      "epoch 305, EV = -1589018031.6491227, val  loss = 45110364.0 , train loss 4903610.9\n",
      "epoch 306, EV = -1555492467.2982457, val  loss = 44321796.8 , train loss 4576145.375\n",
      "epoch 307, EV = -1533530676.491228, val  loss = 43594248.8 , train loss 4556964.775\n",
      "epoch 308, EV = -1501972347.7192984, val  loss = 42718984.0 , train loss 4555485.725\n",
      "epoch 309, EV = -1474452057.3333333, val  loss = 41967631.2 , train loss 4285755.675\n",
      "epoch 310, EV = -1447406513.2631578, val  loss = 41254094.4 , train loss 4252183.25\n",
      "epoch 311, EV = -1410537519.7894738, val  loss = 40551726.4 , train loss 4163543.425\n",
      "epoch 312, EV = -1395433165.6842105, val  loss = 39958215.6 , train loss 4196221.8\n",
      "epoch 313, EV = -1382303582.2807016, val  loss = 39434971.2 , train loss 4082173.375\n",
      "epoch 314, EV = -1356731184.2105262, val  loss = 38600058.0 , train loss 4256123.35\n",
      "epoch 315, EV = -1319686714.3157895, val  loss = 37720392.0 , train loss 3978179.4\n",
      "epoch 316, EV = -1311222556.1754386, val  loss = 37263477.6 , train loss 3831956.4\n",
      "epoch 317, EV = -1278373001.8245614, val  loss = 36570395.2 , train loss 3681517.075\n",
      "epoch 318, EV = -1254780820.631579, val  loss = 35906431.2 , train loss 3662374.65\n",
      "epoch 319, EV = -1239143405.4035087, val  loss = 35476759.6 , train loss 3787569.9\n",
      "epoch 320, EV = -1213450799.368421, val  loss = 34656590.4 , train loss 3526428.35\n",
      "epoch 321, EV = -1185642211.8596492, val  loss = 33988444.0 , train loss 3449199.325\n",
      "epoch 322, EV = -1169442985.2631578, val  loss = 33677370.0 , train loss 3632815.15\n",
      "epoch 323, EV = -1163208410.5614035, val  loss = 33212956.8 , train loss 3437730.875\n",
      "epoch 324, EV = -1120919585.368421, val  loss = 32036902.4 , train loss 3726977.575\n",
      "epoch 325, EV = -1109641049.4736843, val  loss = 31867236.4 , train loss 3432817.35\n",
      "epoch 326, EV = -1081369539.3333333, val  loss = 30896344.8 , train loss 3270774.675\n",
      "epoch 327, EV = -1090215420.5614035, val  loss = 31012827.2 , train loss 3198455.6\n",
      "epoch 328, EV = -1045493544.2807018, val  loss = 30122139.2 , train loss 3222837.35\n",
      "epoch 329, EV = -1035035655.8245614, val  loss = 29492466.4 , train loss 3316258.725\n",
      "epoch 330, EV = -1016066924.7017543, val  loss = 28990096.4 , train loss 3161002.3\n",
      "epoch 331, EV = -990692341.754386, val  loss = 28329610.4 , train loss 3000476.975\n",
      "epoch 332, EV = -981848840.9473684, val  loss = 27999878.4 , train loss 3023885.675\n",
      "epoch 333, EV = -965683464.8070176, val  loss = 27734898.8 , train loss 3198733.275\n",
      "epoch 334, EV = -957508726.0350877, val  loss = 27392050.4 , train loss 3041685.375\n",
      "epoch 335, EV = -925402238.1052631, val  loss = 26500234.0 , train loss 2761467.85\n",
      "epoch 336, EV = -907691565.5087719, val  loss = 25799098.4 , train loss 2704459.6\n",
      "epoch 337, EV = -895782566.3157895, val  loss = 25443800.4 , train loss 2688391.7\n",
      "epoch 338, EV = -879581901.0877193, val  loss = 24967923.6 , train loss 2650553.875\n",
      "epoch 339, EV = -860929085.1929824, val  loss = 24580877.2 , train loss 2688691.25\n",
      "epoch 340, EV = -835773467.122807, val  loss = 23841889.6 , train loss 2522648.0125\n",
      "epoch 341, EV = -831280460.0350877, val  loss = 23929749.2 , train loss 2542709.55\n",
      "epoch 342, EV = -806163744.4912281, val  loss = 23148076.4 , train loss 2505566.9\n",
      "epoch 343, EV = -804986893.1929824, val  loss = 22822886.4 , train loss 2460057.75\n",
      "epoch 344, EV = -781573210.6666666, val  loss = 22287296.8 , train loss 2328080.975\n",
      "epoch 345, EV = -764890995.122807, val  loss = 21768458.4 , train loss 2259676.1\n",
      "epoch 346, EV = -752560871.1578947, val  loss = 21451316.8 , train loss 2324289.6875\n",
      "epoch 347, EV = -740796227.9298246, val  loss = 21194315.2 , train loss 2241801.0875\n",
      "epoch 348, EV = -730526304.3859649, val  loss = 20833740.4 , train loss 2167623.875\n",
      "epoch 349, EV = -730822785.122807, val  loss = 20750923.6 , train loss 2432812.6\n",
      "epoch 350, EV = -700485137.4035088, val  loss = 20126411.2 , train loss 2177726.3\n",
      "epoch 351, EV = -696819945.6491228, val  loss = 19918992.0 , train loss 3151424.725\n",
      "epoch 352, EV = -679493146.5263158, val  loss = 19311970.6 , train loss 2259735.325\n",
      "epoch 353, EV = -661979120.7368422, val  loss = 18960745.4 , train loss 1968935.5875\n",
      "epoch 354, EV = -659289332.4912281, val  loss = 18963776.8 , train loss 2046156.6875\n",
      "epoch 355, EV = -647433376.1754386, val  loss = 18420391.6 , train loss 1946241.65\n",
      "epoch 356, EV = -628712184.7017543, val  loss = 17838981.6 , train loss 1985583.05\n",
      "epoch 357, EV = -622359382.2807018, val  loss = 18265344.4 , train loss 2263671.475\n",
      "epoch 358, EV = -599077790.4561404, val  loss = 17354002.2 , train loss 4432062.5125\n",
      "epoch 359, EV = -603779736.3859649, val  loss = 17333185.8 , train loss 2133902.95\n",
      "epoch 360, EV = -589141016.2105263, val  loss = 16922683.0 , train loss 2087739.3875\n",
      "epoch 361, EV = -576997231.4736842, val  loss = 16424788.6 , train loss 1890028.0875\n",
      "epoch 362, EV = -558585142.1052631, val  loss = 16159507.0 , train loss 3103275.6875\n",
      "epoch 363, EV = -553434542.0350877, val  loss = 15822690.6 , train loss 1751042.3\n",
      "epoch 364, EV = -540227061.7894737, val  loss = 15506122.8 , train loss 1787501.9625\n",
      "epoch 365, EV = -527605043.2280702, val  loss = 15310305.0 , train loss 1729954.475\n",
      "epoch 366, EV = -515820733.9649123, val  loss = 14865451.0 , train loss 1724566.225\n",
      "epoch 367, EV = -507776333.122807, val  loss = 15831148.6 , train loss 2588232.6875\n",
      "epoch 368, EV = -512457496.31578946, val  loss = 14857839.6 , train loss 2312071.3125\n",
      "epoch 369, EV = -494597384.1052632, val  loss = 14347974.0 , train loss 1760128.325\n",
      "epoch 370, EV = -485998444.49122804, val  loss = 13977494.8 , train loss 1499082.2\n",
      "epoch 371, EV = -479966366.5263158, val  loss = 13760868.2 , train loss 1497014.5625\n",
      "epoch 372, EV = -467524073.2280702, val  loss = 13315160.0 , train loss 1444653.4875\n",
      "epoch 373, EV = -465441874.7719298, val  loss = 13349208.8 , train loss 1444470.5625\n",
      "epoch 374, EV = -453273923.01754385, val  loss = 13224806.8 , train loss 1982404.8875\n",
      "epoch 375, EV = -444343205.3684211, val  loss = 12865265.0 , train loss 2153672.1875\n",
      "epoch 376, EV = -464006368.4385965, val  loss = 13252821.4 , train loss 9689715.0625\n",
      "epoch 377, EV = -429403754.0, val  loss = 12228116.4 , train loss 1599573.1375\n",
      "epoch 378, EV = -431321650.45614034, val  loss = 14977711.4 , train loss 8942524.475\n",
      "epoch 379, EV = -418729679.84210527, val  loss = 12703764.0 , train loss 7107896.8\n",
      "epoch 380, EV = -408948453.00877196, val  loss = 11750270.4 , train loss 1398825.475\n",
      "epoch 381, EV = -397957811.4736842, val  loss = 11423618.6 , train loss 1327231.5875\n",
      "epoch 382, EV = -388584952.5263158, val  loss = 11371020.4 , train loss 1257072.05\n",
      "epoch 383, EV = -384120990.1403509, val  loss = 11128597.9 , train loss 1286806.95\n",
      "epoch 384, EV = -387347578.69298244, val  loss = 13947897.6 , train loss 7110798.2\n",
      "epoch 385, EV = -396427638.1052632, val  loss = 12854540.8 , train loss 2073685.9875\n",
      "epoch 386, EV = -373182965.2017544, val  loss = 11074384.4 , train loss 4777645.25\n",
      "epoch 387, EV = -370874462.05263156, val  loss = 10682758.6 , train loss 1571638.075\n",
      "epoch 388, EV = -350913519.64912283, val  loss = 10207230.0 , train loss 1210591.51875\n",
      "epoch 389, EV = -389972729.68421054, val  loss = 11888050.4 , train loss 3436262.0625\n",
      "epoch 390, EV = -344859690.94736844, val  loss = 10822110.2 , train loss 3149244.15\n",
      "epoch 391, EV = -338299346.3684211, val  loss = 9953770.2 , train loss 1542825.04375\n",
      "epoch 392, EV = -328087252.99122804, val  loss = 9488084.5 , train loss 1126092.23125\n",
      "epoch 393, EV = -318468591.3596491, val  loss = 9262345.2 , train loss 1099792.53125\n",
      "epoch 394, EV = -316778183.2982456, val  loss = 9356750.3 , train loss 1161883.0375\n",
      "epoch 395, EV = -320649384.9298246, val  loss = 9444341.4 , train loss 1481490.3625\n",
      "epoch 396, EV = -336077107.0964912, val  loss = 9818791.4 , train loss 2279571.9625\n",
      "epoch 397, EV = -304775817.1666667, val  loss = 8870525.1 , train loss 1302251.50625\n",
      "epoch 398, EV = -351685730.99122804, val  loss = 9249425.3 , train loss 2151002.575\n",
      "epoch 399, EV = -302996945.2105263, val  loss = 8677745.4 , train loss 1385776.625\n",
      "epoch 400, EV = -302777172.1052632, val  loss = 8846520.2 , train loss 1606039.95\n",
      "epoch 401, EV = -302437472.24561405, val  loss = 9155143.4 , train loss 3007717.5125\n",
      "epoch 402, EV = -288713674.18421054, val  loss = 8398915.1 , train loss 1123304.18125\n",
      "epoch 403, EV = -289034046.2105263, val  loss = 8651385.2 , train loss 6199620.7375\n",
      "epoch 404, EV = -275239406.0964912, val  loss = 8349821.7 , train loss 1732083.275\n",
      "epoch 405, EV = -268085675.35964912, val  loss = 7806744.6 , train loss 926595.01875\n",
      "epoch 406, EV = -266380136.31578946, val  loss = 7729039.4 , train loss 850466.95625\n",
      "epoch 407, EV = -292894478.7850877, val  loss = 15370214.0 , train loss 6038538.8125\n",
      "epoch 408, EV = -277248088.07894737, val  loss = 9046254.1 , train loss 2354126.975\n",
      "epoch 409, EV = -280709402.05701756, val  loss = 11692782.6 , train loss 7336064.525\n",
      "epoch 410, EV = -280263012.9385965, val  loss = 9688987.6 , train loss 8238759.3875\n",
      "epoch 411, EV = -285895286.38596493, val  loss = 8419056.8 , train loss 7426864.95\n",
      "epoch 412, EV = -283389727.3684211, val  loss = 8633187.8 , train loss 2510705.75\n",
      "epoch 413, EV = -330987375.7105263, val  loss = 24852894.0 , train loss 7714341.425\n",
      "epoch 414, EV = -253414956.51754385, val  loss = 7626129.3 , train loss 5996796.2375\n",
      "epoch 415, EV = -246628546.3508772, val  loss = 7092058.6 , train loss 2000180.1\n",
      "epoch 416, EV = -243259983.54385966, val  loss = 7184015.2 , train loss 1413971.06875\n",
      "epoch 417, EV = -246316318.07017544, val  loss = 7218339.0 , train loss 1124357.1\n",
      "epoch 418, EV = -234358389.07017544, val  loss = 6787756.8 , train loss 716930.7625\n",
      "epoch 419, EV = -237755998.59649122, val  loss = 7343384.2 , train loss 1731937.68125\n",
      "epoch 420, EV = -250436875.50877193, val  loss = 14837989.4 , train loss 14300913.875\n",
      "epoch 421, EV = -246847690.77631578, val  loss = 10378093.2 , train loss 4149882.675\n",
      "epoch 422, EV = -229467708.26754385, val  loss = 6890825.8 , train loss 3114818.2125\n",
      "epoch 423, EV = -225063810.35964912, val  loss = 6626998.8 , train loss 927592.75625\n",
      "epoch 424, EV = -233392047.07017544, val  loss = 6931867.7 , train loss 1790067.26875\n",
      "epoch 425, EV = -219975195.4649123, val  loss = 6347613.7 , train loss 678082.45\n",
      "epoch 426, EV = -223837131.5635965, val  loss = 9399643.2 , train loss 4270364.33125\n",
      "epoch 427, EV = -222249591.02631578, val  loss = 7453197.5 , train loss 3181775.33125\n",
      "epoch 428, EV = -250092628.92543858, val  loss = 7616009.9 , train loss 12027959.725\n",
      "epoch 429, EV = -218353706.26973686, val  loss = 6237144.9 , train loss 772102.98125\n",
      "epoch 430, EV = -217084883.0767544, val  loss = 6575988.6 , train loss 5158251.8375\n",
      "epoch 431, EV = -237682985.31578946, val  loss = 29591585.6 , train loss 17913551.0\n",
      "epoch 432, EV = -219546696.39035088, val  loss = 7731188.9 , train loss 7806522.94375\n",
      "epoch 433, EV = -347331438.4385965, val  loss = 38923005.2 , train loss 14837802.61875\n",
      "epoch 434, EV = -209091161.90570176, val  loss = 6212266.1 , train loss 1061176.66875\n",
      "epoch 435, EV = -212456714.38157895, val  loss = 5967272.4 , train loss 2557343.175\n",
      "epoch 436, EV = -204334046.17105263, val  loss = 5789220.1 , train loss 593558.5125\n",
      "epoch 437, EV = -201354091.86842105, val  loss = 5742294.5 , train loss 519521.55\n",
      "epoch 438, EV = -203217274.67982456, val  loss = 6022394.0 , train loss 732817.9875\n",
      "epoch 439, EV = -200637703.32456142, val  loss = 5715687.1 , train loss 490362.85625\n",
      "epoch 440, EV = -195438054.2631579, val  loss = 5565856.0 , train loss 436580.69375\n",
      "epoch 441, EV = -195119818.69956142, val  loss = 5535976.0 , train loss 408368.9125\n",
      "epoch 442, EV = -194331647.16666666, val  loss = 5516114.7 , train loss 406412.140625\n",
      "epoch 443, EV = -194281452.72807017, val  loss = 5531989.0 , train loss 414931.5125\n",
      "epoch 444, EV = -195165164.57017544, val  loss = 5723364.2 , train loss 632447.134375\n",
      "epoch 445, EV = -204856660.3508772, val  loss = 17311006.4 , train loss 10523248.50625\n",
      "epoch 446, EV = -191613243.33333334, val  loss = 5590706.4 , train loss 589461.3125\n",
      "epoch 447, EV = -203090559.57894737, val  loss = 8386891.0 , train loss 4164185.721875\n",
      "epoch 448, EV = -192094799.50877193, val  loss = 5841789.0 , train loss 1751426.0\n",
      "epoch 449, EV = -190331750.52631578, val  loss = 5555447.3 , train loss 1093856.86875\n",
      "epoch 450, EV = -196840561.7982456, val  loss = 6482531.4 , train loss 7953349.33125\n",
      "epoch 451, EV = -195117778.28070176, val  loss = 5644126.1 , train loss 761418.35625\n",
      "epoch 452, EV = -186999588.67543858, val  loss = 5321006.5 , train loss 2362374.9875\n",
      "epoch 453, EV = -186823276.0526316, val  loss = 5328020.0 , train loss 497454.759375\n",
      "epoch 454, EV = -183749316.9473684, val  loss = 5212742.8 , train loss 386506.121875\n",
      "epoch 455, EV = -215501974.71929824, val  loss = 16552435.6 , train loss 4332159.98125\n",
      "epoch 456, EV = -195644059.45614034, val  loss = 9420360.6 , train loss 3764064.428125\n",
      "epoch 457, EV = -181697989.40350878, val  loss = 5176669.65 , train loss 370725.071875\n",
      "epoch 458, EV = -183813905.5, val  loss = 5437727.0 , train loss 1019627.43125\n",
      "epoch 459, EV = -183560680.09649122, val  loss = 5375210.0 , train loss 1210928.584375\n",
      "epoch 460, EV = -179777494.68201753, val  loss = 5107209.7 , train loss 370359.790625\n",
      "epoch 461, EV = -179023598.20614034, val  loss = 5088578.8 , train loss 361723.8375\n",
      "epoch 462, EV = -177779631.50219297, val  loss = 5067573.0 , train loss 355640.221875\n",
      "epoch 463, EV = -188008179.6381579, val  loss = 6788790.1 , train loss 549609.64375\n",
      "epoch 464, EV = -177181677.73464912, val  loss = 5025135.3 , train loss 419235.265625\n",
      "epoch 465, EV = -175964898.21052632, val  loss = 4984502.6 , train loss 331752.0\n",
      "epoch 466, EV = -175461656.51973686, val  loss = 4944959.4 , train loss 323843.346875\n",
      "epoch 467, EV = -214410308.29605263, val  loss = 6342737.7 , train loss 5262830.796875\n",
      "epoch 468, EV = -175361942.8881579, val  loss = 5100197.0 , train loss 17909541.4625\n",
      "epoch 469, EV = -174286486.47807017, val  loss = 4927824.5 , train loss 357601.409375\n",
      "epoch 470, EV = -180669614.0526316, val  loss = 12630268.2 , train loss 6232965.7875\n",
      "epoch 471, EV = -181809538.33552632, val  loss = 6474459.35 , train loss 4471351.096875\n",
      "epoch 472, EV = -171549131.97807017, val  loss = 4900544.7 , train loss 454377.390625\n",
      "epoch 473, EV = -170317164.73245615, val  loss = 4842772.0 , train loss 372689.090625\n",
      "epoch 474, EV = -170441249.05482456, val  loss = 4923406.1 , train loss 435684.25\n",
      "epoch 475, EV = -171632084.40570176, val  loss = 6139513.8 , train loss 1044141.209375\n",
      "epoch 476, EV = -172222620.92543858, val  loss = 4879247.6 , train loss 476399.109375\n",
      "epoch 477, EV = -174681960.8991228, val  loss = 5002293.4 , train loss 712011.08125\n",
      "epoch 478, EV = -179163286.49122807, val  loss = 5050181.0 , train loss 23410757.365625\n",
      "epoch 479, EV = -168482703.71710527, val  loss = 4918387.15 , train loss 572208.5625\n",
      "epoch 480, EV = -167659365.5614035, val  loss = 4934374.9 , train loss 901245.4875\n",
      "epoch 481, EV = -165139683.61842105, val  loss = 4661199.7 , train loss 315588.309375\n",
      "epoch 482, EV = -164513491.65350878, val  loss = 4648782.25 , train loss 312414.271875\n",
      "epoch 483, EV = -170049980.18201753, val  loss = 5002869.65 , train loss 1187099.403125\n",
      "epoch 484, EV = -166373184.68421054, val  loss = 4708836.2 , train loss 394678.175\n",
      "epoch 485, EV = -186761406.62938598, val  loss = 10090011.2 , train loss 21898607.65625\n",
      "epoch 486, EV = -164970565.91557017, val  loss = 4792848.1 , train loss 649550.728125\n",
      "epoch 487, EV = -163111279.18530703, val  loss = 4587929.8 , train loss 331914.1625\n",
      "epoch 488, EV = -160827247.74671054, val  loss = 4514976.4 , train loss 335871.15625\n",
      "epoch 489, EV = -212330138.2401316, val  loss = 5409731.9 , train loss 5234547.725\n",
      "epoch 490, EV = -162983342.94298247, val  loss = 4537287.85 , train loss 429924.671875\n",
      "epoch 491, EV = -167442647.70394737, val  loss = 4679880.1 , train loss 623427.621875\n",
      "epoch 492, EV = -161440547.22039473, val  loss = 4532781.3 , train loss 436166.628125\n",
      "epoch 493, EV = -160181789.35745615, val  loss = 4470394.65 , train loss 298265.715625\n",
      "epoch 494, EV = -158010662.46381578, val  loss = 4426336.2 , train loss 282270.3078125\n",
      "epoch 495, EV = -156877204.7017544, val  loss = 4376383.2 , train loss 277242.5796875\n",
      "epoch 496, EV = -156214586.30043858, val  loss = 4382854.9 , train loss 263145.678125\n",
      "epoch 497, EV = -155968962.6885965, val  loss = 4367794.2 , train loss 250619.028125\n",
      "epoch 498, EV = -154481967.23245615, val  loss = 4326966.85 , train loss 254426.1828125\n",
      "epoch 499, EV = -155006571.75219297, val  loss = 4350312.1 , train loss 259702.3703125\n"
     ]
    }
   ],
   "source": [
    "epoches = 500\n",
    "for epoch in tqdm_notebook(range(epoches)):\n",
    "    losses_train += train_model(encoder,optimizer)\n",
    "    ev,loss = validate_model(encoder)\n",
    "    EVs.append(ev)\n",
    "    losses_val.append(loss)\n",
    "    print(f'epoch {epoch}, EV = {ev}, val  loss = {loss} , train loss {sum(losses_train[-10:])/10}')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
