{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "import models\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from PIL import Image\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "device = 'cuda:0' # device where you put your data and models\n",
    "data_path = './' # the path of the 'npc_v4_data.h5' file\n",
    "batch_size = 16 # the batch size of the data loader\n",
    "insp_layer = 'conv3' # the middle layer extracted from alexnet, available in {'conv1', 'conv2', 'conv3', 'conv4', 'conv5'}\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "root_dir = 'C:/Users/admin/Desktop/pytorch_ovo/data/0_presented_images_800/'\n",
    "resolution = 300\n",
    "image_path = os.listdir(root_dir)\n",
    "path_dict = {}\n",
    "for j in image_path:\n",
    "    key = int(j.split('_')[0])  # 刺激呈现的顺序是图像名称下划线前面的数字顺序。\n",
    "    path_dict[key] = j\n",
    "\n",
    "stim_arr = np.zeros((len(image_path), resolution, resolution, 3))\n",
    "# stim_arr_gray3 = np.zeros((len(image_path), resolution, resolution, 3))\n",
    "for i in range(len(image_path)):\n",
    "    img_bgr = cv2.imread(os.path.join(root_dir, path_dict[i+1]))\n",
    "    stim_arr[i] = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "stim_arr = stim_arr.astype('float32')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4  15  16  18  19  40  42  54  61  78 120 124 126 133 142 146 161 165\n",
      " 166 173 174 178 212 215 222 224 262 278 285 293 305 311 336 351 352 364\n",
      " 368 369 393 400 410 412 420 432 442 463 473 483 484 502 509 510 534 541\n",
      " 552 579 582 599 604 605 610 615 620 622 633 637 672 675 681 683 692 698\n",
      " 701 705 711 722 723 727 787 788]\n",
      "[  4  15  16  18  19  40  42  54  61  78 120 124 126 133 142 146 161 165\n",
      " 166 173 174 178 212 215 222 224 262 278 285 293 305 311 336 351 352 364\n",
      " 368 369 393 400 410 412 420 432 442 463 473 483 484 502 509 510 534 541\n",
      " 552 579 582 599 604 605 610 615 620 622 633 637 672 675 681 683 692 698\n",
      " 701 705 711 722 723 727 787 788] [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79] (800, 299, 299, 3)\n"
     ]
    }
   ],
   "source": [
    "# 我们数据包括880张图片，前800张是unique的，后80张是random出的图片序号，然后这些序号对应图片进行重复播放，作为验证集。\n",
    "id = h5py.File('C:/Users/admin/Desktop/pytorch_ovo/data/5_L86LL_V4_S10_D270_objects/stimuli/Random_id_80_2021_12_22.mat', 'r')\n",
    "\n",
    "images_n  = np.zeros(shape=(stim_arr.shape[0], 299, 299, 3))\n",
    "for i in range(stim_arr.shape[0]):\n",
    "    images_n[i] = cv2.resize(stim_arr[i], (299, 299))\n",
    "\n",
    "idx = np.array(id['sampleidlist21']).squeeze().astype('int') - 1\n",
    "print(idx)\n",
    "idx, unique_idx = np.unique(idx, return_index=True)\n",
    "print(idx, unique_idx, images_n.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 880, 120)\n"
     ]
    }
   ],
   "source": [
    "mat_file = h5py.File('C:/Users/admin/Desktop/pytorch_ovo/data/5_L86LL_V4_S10_D270_objects/stimuli/celldataS_110_CalmAn_10_Objects_12_800_80_56_68_74_4_trial_mean_normal.mat', 'r')\n",
    "#[num_repetitions, num_images, num_neurons]\n",
    "#print(np.array(mat_file['celldataS']).shape)\n",
    "neural_n = np.transpose(np.array(mat_file['celldataS']), (1, 2, 0)).astype('float16')\n",
    "neural_n = neural_n[:,:880, :]\n",
    "print(neural_n.shape)\n",
    "#12个trials 880张图片（其中80张是重复），114个细胞"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800 120 (800, 299, 299, 3)\n"
     ]
    }
   ],
   "source": [
    "n_images = 800\n",
    "n_neurons = neural_n.shape[2]\n",
    "size_imags = images_n.shape[0]\n",
    "print(n_images, n_neurons, images_n.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(880, 12, 120) (80,) (80, 12, 120)\n",
      "(720, 120)\n",
      "(80, 120)\n"
     ]
    }
   ],
   "source": [
    "#划分训练集和验证集，找到前800张里不重复的作为训练集，取前800张random的和后80张作为验证集，多个trials取平均值\n",
    "reps = neural_n.shape[0] # trials\n",
    "rand_ind = np.arange(reps)\n",
    "np.random.shuffle(rand_ind)\n",
    "\n",
    "data_y_train = np.concatenate((np.delete(neural_n[:, :800, :], idx, 1), neural_n[:, 880:, :]), 1).mean(0)\n",
    "temp = np.transpose(neural_n, (1, 0, 2))\n",
    "print(temp.shape, idx.shape, temp[idx].shape)\n",
    "data_y_val = np.concatenate((temp[idx], temp[800:880][unique_idx]), 1)\n",
    "data_y_val = np.transpose(data_y_val, (1, 0, 2))\n",
    "data_y_val = np.mean(data_y_val, 0)\n",
    "print(data_y_train.shape)\n",
    "print(data_y_val.shape)\n",
    "\n",
    "#\n",
    "# data_x = images_n[:, np.newaxis].astype(np.float16)\n",
    "# print('images_n', images_n.shape)\n",
    "# data_x = data_x / 255 # (640, 1, 299, 299)\n",
    "# data_x = np.tile(data_x, [1, 3, 1, 1])\n",
    "# print('data_x', data_x.shape)\n",
    "# data_x_train = data_x[:576]\n",
    "# data_x_val = data_x[576:]as indices must be"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 299, 299, 3)\n",
      "(800, 299, 299, 3)\n",
      "(800, 3, 299, 299) (720, 3, 299, 299) (80, 3, 299, 299)\n"
     ]
    }
   ],
   "source": [
    "print(images_n.shape)\n",
    "#data_x = images_n[:, np.newaxis].astype(np.float16)\n",
    "data_x = images_n.astype(np.float16)\n",
    "print(data_x.shape)\n",
    "data_x = data_x / 255 # (800, 1, 299, 299)\n",
    "\n",
    "#data_x = np.tile(data_x, [1, 3, 1, 1])\n",
    "data_x_train = np.delete(images_n, idx, 0)\n",
    "data_x_val = images_n[idx]\n",
    "\n",
    "data_x = np.transpose(data_x, (0, 3, 1, 2))\n",
    "data_x_train = np.transpose(data_x_train, (0, 3, 1, 2))\n",
    "data_x_val = np.transpose(data_x_val, (0, 3, 1, 2))\n",
    "print(data_x.shape, data_x_train.shape, data_x_val.shape)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val: (80, 3, 299, 299), (80, 120)\n"
     ]
    }
   ],
   "source": [
    "#设置dataset\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_x, data_y):\n",
    "        self.data_x = data_x\n",
    "        self.data_y = data_y\n",
    "    def __getitem__(self, index):\n",
    "        return index, self.data_x[index], self.data_y[index]\n",
    "    def __len__(self):\n",
    "        return self.data_x.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "dataset_train = Dataset(data_x_train, data_y_train)\n",
    "dataset_val = Dataset(data_x_val, data_y_val)\n",
    "\n",
    "loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle = True)\n",
    "loader_val = torch.utils.data.DataLoader(dataset_val, batch_size=batch_size, shuffle = True)\n",
    "\n",
    "print(f'val: {data_x_val.shape}, {data_y_val.shape}')\n",
    "# for i,(x,y) in enumerate(loader_val):\n",
    "#     print(i, x.shape, y.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([1, 3, 299, 299])\n",
      "fmap:  torch.Size([1, 384, 17, 17])\n",
      "size:  torch.Size([17, 17])\n",
      "120 torch.Size([17, 17])\n",
      "torch.Size([120, 17, 17])\n"
     ]
    }
   ],
   "source": [
    "# CHOOSE THE AUGMENTS IF NECESSARY\n",
    "lamd_s, lamd_d = [5e-3, 2e-3] # the coefficients of the losses. Try other coefficients!\n",
    "epoches = 10 # total epochs for training the encoder\n",
    "lr = 1e-3 # the learing rate for training the encoder\n",
    "\n",
    "alexnet = models.alexnet(pretrained=True)\n",
    "\n",
    "#\n",
    "alexnet.to(device)\n",
    "alexnet.eval()\n",
    "for param in alexnet.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "x = torch.from_numpy(data_x[0:1]).to(device)\n",
    "print(\"x:\", x.shape)\n",
    "x = x.float()\n",
    "fmap = alexnet(x, layer=insp_layer)\n",
    "\n",
    "neurons = data_y_train.shape[1]\n",
    "sizes = fmap.shape[2:]\n",
    "print(\"fmap: \", fmap.shape)\n",
    "print(\"size: \", sizes)\n",
    "channels = fmap.shape[1]\n",
    "print(neurons, sizes)\n",
    "w_s = nn.Parameter(torch.randn(size=(neurons,) + sizes))\n",
    "print(w_s.shape)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 384, 17, 17])\n",
      "torch.Size([800, 384, 17, 17])\n"
     ]
    }
   ],
   "source": [
    "# 把每张图片的conv1 feature map 存起来 理论上会快一点？\n",
    "x = torch.from_numpy(data_x[0:1]).float().to(device)\n",
    "fmap = alexnet(x, layer=insp_layer)\n",
    "print(fmap.shape)\n",
    "sizes = fmap.shape[2:]\n",
    "\n",
    "imagenet_mean = torch.tensor([0.485, 0.456, 0.406], dtype=torch.float32).view(1, 3, 1, 1).to(device)\n",
    "imagenet_std = torch.tensor([0.229, 0.224, 0.225], dtype=torch.float32).view(1, 3, 1, 1).to(device)\n",
    "transform = lambda x : (x - imagenet_mean) / imagenet_std\n",
    "#data_x = transform(data_x)\n",
    "feature_map = torch.Tensor(n_images, fmap.shape[1], fmap.shape[2], fmap.shape[3])\n",
    "feature_map.to(device)\n",
    "print(feature_map.shape)\n",
    "for i in range(n_images):\n",
    "    x = torch.from_numpy(data_x[i:i + 1]).float().to(device)\n",
    "    #x = transform(x).to(device)\n",
    "    fmap = alexnet(x, layer = insp_layer)\n",
    "    feature_map[i] = fmap"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "mse_weight = 1.0\n",
    "l1_weight = 0\n",
    "spa_weight = 1e-1\n",
    "ch_weight = 1e-1\n",
    "lap_weight = 1e-1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "#定义损失函数\n",
    "def mse_loss(prediction, response, weight=None):\n",
    "    if weight is None:\n",
    "        mse_loss = torch.mean(torch.mean((prediction - response)**2, dim=1))\n",
    "    else:\n",
    "        mse_loss = torch.sum(weight*torch.mean((prediction - response)**2, dim=1))\n",
    "    return mse_loss\n",
    "\n",
    "def l2_norm_regularizer(W):\n",
    "    with torch.autograd.profiler.record_function('l2_norm'):\n",
    "        penalty = torch.mean(torch.sum(W**2))\n",
    "        return penalty\n",
    "\n",
    "def l1_norm_regularizer(W):\n",
    "    with torch.autograd.profiler.record_function('l1_norm'):\n",
    "        penalty = torch.mean(torch.sum(torch.abs(W)))\n",
    "        return penalty\n",
    "\n",
    "def smoothness_regularizer_2d(W_s):\n",
    "    K = torch.tensor([\n",
    "    [0,-1,0],\n",
    "    [-1,4,-1],\n",
    "    [0,-1,0]],dtype=torch.float).to(device)\n",
    "    return torch.sum(F.conv2d(torch.unsqueeze(W_s,1),K.unsqueeze(0).unsqueeze(0))**2)\n",
    "\n",
    "def pearson_corr(prediction, response):\n",
    "    prediction = torch.transpose(prediction, 1, 0)\n",
    "    response = torch.transpose(response, 1, 0)\n",
    "\n",
    "    prediction_mean = torch.mean(prediction, dim=0)\n",
    "    response_mean = torch.mean(response, dim=0)\n",
    "\n",
    "    num = torch.sum((prediction - prediction_mean)*(response - response_mean), dim=0)\n",
    "    den = torch.sqrt(torch.sum((prediction - prediction_mean)**2, dim=0) *\n",
    "                     torch.sum((response - response_mean)**2, dim=0))\n",
    "    pcc = torch.mean(num * (1 / den))\n",
    "    return pcc\n",
    "\n",
    "def explained_variance(y_true, y_pred):\n",
    "    total_var = torch.var(y_true)\n",
    "    residual_var = torch.var(y_true - y_pred)\n",
    "    explained_var = total_var - residual_var\n",
    "    return explained_var.item()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# 模型\n",
    "class conv_encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, neurons, sizes, channels):\n",
    "        super(conv_encoder, self).__init__()\n",
    "        # PUT YOUR CODES HERE\n",
    "        self.W_s = nn.Parameter(torch.randn(size=(neurons,) + sizes)) #感受野\n",
    "        self.W_d = nn.Parameter(torch.randn(size = (neurons,channels,1,1))) #通道权重\n",
    "        self.W_b = nn.Parameter(torch.randn(size = (1,neurons))) # 偏置\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # PUT YOUR CODES HERE\n",
    "        out = torch.einsum('bchw , nhw -> bnchw',x,self.W_s) # dimension : N,n,C,h,w\n",
    "        out = torch.stack(\n",
    "            [F.conv2d(out[:,n,:,:,:],torch.unsqueeze(self.W_d[n],0)) for n in range(neurons)],dim=1)\n",
    "            #dimension:N,n,1,h,w\n",
    "        out = torch.sum(out,dim=(2,3,4))\n",
    "        out = out + self.W_b\n",
    "        return out\n",
    "\n",
    "def Loss(y, pred, W_s, W_d):\n",
    "    return mse_loss(y, pred) * mse_weight + \\\n",
    "          l2_norm_regularizer(W_s) * spa_weight + \\\n",
    "          smoothness_regularizer_2d(W_s) * lap_weight + \\\n",
    "          l2_norm_regularizer(W_d) * ch_weight\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#encoder = conv_encoder(neurons, sizes, channels).to(device)\n",
    "encoder = conv_encoder(neurons, sizes, channels).to(device)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\n    You need to define the conv_encoder() class and train the encoder.\\n    The code of alexnet has been slightly modified from the torchvision, for convenience\\n    of extracting the middle layers.\\n\\n    Example:\\n        >>> x = x.to(device) # x is a batch of images\\n        >>> x = transform(x)\\n        >>> fmap = alexnet(x, layer=insp_layer)\\n        >>> out= encoder(fmap)\\n        >>> ...\\n'"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_model(encoder, optimizer):\n",
    "    losses = []\n",
    "    encoder.train()\n",
    "    for i,(z, x,y) in enumerate(loader_train):\n",
    "        optimizer.zero_grad()\n",
    "        x = x.float().to(device)\n",
    "        y = y.float().to(device)\n",
    "        z = z.to(device)\n",
    "        #x = transform(x)\n",
    "        fmap = feature_map[z - 1].to(device) # 得到z图片id对应的特征图\n",
    "        out = encoder(fmap) #输出结果\n",
    "#         print(f'L_e = {l_e} , L_2 = {l_2} , L_l = {l_l}')\n",
    "        loss = Loss(y, out, encoder.W_s, encoder.W_d)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "#         print(f'iteration {i}, train loss: {losses[-1]}')\n",
    "\n",
    "    return losses\n",
    "\n",
    "def validate_model(encoder):\n",
    "    encoder.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    losses = []\n",
    "    for i,(z, x,y) in enumerate(loader_val):\n",
    "        x = x.float().to(device)\n",
    "        y = y.float().to(device)\n",
    "        z = z.to(device)\n",
    "        #x = transform(x)\n",
    "        fmap = feature_map[z - 1].to(device)\n",
    "        out = encoder(fmap)\n",
    "        y_pred.append(out)\n",
    "        y_true.append(y)\n",
    "        loss = Loss(y, out, encoder.W_s, encoder.W_d)\n",
    "        losses.append(loss.item())\n",
    "    y_pred = torch.cat(y_pred)\n",
    "    y_true = torch.cat(y_true)\n",
    "    ev = explained_variance(y_true, y_pred)\n",
    "    pcc = pearson_corr(y_pred, y_true)\n",
    "    return pcc, ev,sum(losses)/len(losses)\n",
    "    #return explained_variance,sum(losses)/len(losses)\n",
    "\n",
    "\"\"\"\n",
    "    You need to define the conv_encoder() class and train the encoder.\n",
    "    The code of alexnet has been slightly modified from the torchvision, for convenience\n",
    "    of extracting the middle layers.\n",
    "\n",
    "    Example:\n",
    "        >>> x = x.to(device) # x is a batch of images\n",
    "        >>> x = transform(x)\n",
    "        >>> fmap = alexnet(x, layer=insp_layer)\n",
    "        >>> out= encoder(fmap)\n",
    "        >>> ...\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# losses_train = []\n",
    "# losses_val = []\n",
    "# EVs = []\n",
    "\n",
    "losses_train = []\n",
    "losses_val = []\n",
    "EVs = []\n",
    "pccs = []"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "optimizer = torch.optim.Adam(encoder.parameters(), lr=lr)\n",
    "#optimizer = torch.optim.SGD(encoder.parameters(), lr=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=200, gamma=0.1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\envs\\neural_control\\lib\\site-packages\\ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b16d0a396154464e88e96c57cf06d0b2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, EV = -120296.9453125, val  loss = 178516.615625 , train loss 253563.2328125, pcc = -0.011878948658704758\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 8.00 GiB total capacity; 4.27 GiB already allocated; 532.98 MiB free; 5.68 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\2\\ipykernel_12708\\1387766017.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mendure\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m10\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mepoch\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mtqdm_notebook\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mepoches\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m     \u001B[0mlosses_train\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0mtrain_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mencoder\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0moptimizer\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      7\u001B[0m     \u001B[0mpcc\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mev\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mvalidate_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mencoder\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m     \u001B[1;31m#ev,loss = validate_model(encoder)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\2\\ipykernel_12708\\362410839.py\u001B[0m in \u001B[0;36mtrain_model\u001B[1;34m(encoder, optimizer)\u001B[0m\n\u001B[0;32m      9\u001B[0m         \u001B[1;31m#x = transform(x)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m         \u001B[0mfmap\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfeature_map\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mz\u001B[0m \u001B[1;33m-\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 11\u001B[1;33m         \u001B[0mout\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mencoder\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfmap\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     12\u001B[0m \u001B[1;31m#         print(f'L_e = {l_e} , L_2 = {l_2} , L_l = {l_l}')\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     13\u001B[0m         \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mLoss\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mout\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mencoder\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mW_s\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mencoder\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mW_d\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\neural_control\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m    725\u001B[0m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    726\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 727\u001B[1;33m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    728\u001B[0m         for hook in itertools.chain(\n\u001B[0;32m    729\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\2\\ipykernel_12708\\904184613.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     11\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     12\u001B[0m         \u001B[1;31m# PUT YOUR CODES HERE\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 13\u001B[1;33m         \u001B[0mout\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0meinsum\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'bchw , nhw -> bnchw'\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mW_s\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;31m# dimension : N,n,C,h,w\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     14\u001B[0m         out = torch.stack(\n\u001B[0;32m     15\u001B[0m             [F.conv2d(out[:,n,:,:,:],torch.unsqueeze(self.W_d[n],0)) for n in range(neurons)],dim=1)\n",
      "\u001B[1;32m~\\anaconda3\\envs\\neural_control\\lib\\site-packages\\torch\\functional.py\u001B[0m in \u001B[0;36meinsum\u001B[1;34m(equation, *operands)\u001B[0m\n\u001B[0;32m    342\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0meinsum\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mequation\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0m_operands\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    343\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 344\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0m_VF\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0meinsum\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mequation\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moperands\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# type: ignore\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    345\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    346\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 8.00 GiB total capacity; 4.27 GiB already allocated; 532.98 MiB free; 5.68 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "epoches = 2000\n",
    "best_loss = 1e100\n",
    "not_improve = 0\n",
    "endure = 10\n",
    "for epoch in tqdm_notebook(range(epoches)):\n",
    "    losses_train += train_model(encoder,optimizer)\n",
    "    pcc, ev,loss = validate_model(encoder)\n",
    "    #ev,loss = validate_model(encoder)\n",
    "    EVs.append(ev)\n",
    "    pccs.append(pcc)\n",
    "    losses_val.append(loss)\n",
    "    train_loss = sum(losses_train[-10:])/10\n",
    "    if train_loss < best_loss - 1e-5:\n",
    "        not_improve = 0\n",
    "    else:\n",
    "        not_improve += 1\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'epoch {epoch}, EV = {ev}, val  loss = {loss} , train loss {sum(losses_train[-10:])/10}, pcc = {pcc}')\n",
    "        #print(f'epoch {epoch}, EV = {ev}, val  loss = {loss} , train loss {sum(losses_train[-10:])/10}')\n",
    "    if not_improve == endure:\n",
    "        print(\"Early stopping!\")\n",
    "\n",
    "    scheduler.step()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max(EVs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "exp = \"layer1_67\"\n",
    "torch.save(encoder, f'./exp{exp}.pt')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
