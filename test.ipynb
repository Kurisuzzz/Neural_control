{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "import models\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from PIL import Image\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "device = 'cuda:0' # device where you put your data and models\n",
    "data_path = './' # the path of the 'npc_v4_data.h5' file\n",
    "batch_size = 16 # the batch size of the data loader\n",
    "insp_layer = 'conv3' # the middle layer extracted from alexnet, available in {'conv1', 'conv2', 'conv3', 'conv4', 'conv5'}\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "root_dir = './Celldata_S18_155_L76_211203/stimuli/0_presented_images_800/'\n",
    "resolution = 300\n",
    "image_path = os.listdir(root_dir)\n",
    "path_dict = {}\n",
    "for j in image_path:\n",
    "    key = int(j.split('_')[0])  # 刺激呈现的顺序是图像名称下划线前面的数字顺序。\n",
    "    path_dict[key] = j\n",
    "\n",
    "stim_arr = np.zeros((len(image_path), resolution, resolution, 3))\n",
    "# stim_arr_gray3 = np.zeros((len(image_path), resolution, resolution, 3))\n",
    "for i in range(len(image_path)):\n",
    "    img_bgr = cv2.imread(os.path.join(root_dir, path_dict[i+1]))\n",
    "    stim_arr[i] = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "stim_arr = stim_arr.astype('float32')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4  15  16  18  19  40  42  54  61  78 120 124 126 133 142 146 161 165\n",
      " 166 173 174 178 212 215 222 224 262 278 285 293 305 311 336 351 352 364\n",
      " 368 369 393 400 410 412 420 432 442 463 473 483 484 502 509 510 534 541\n",
      " 552 579 582 599 604 605 610 615 620 622 633 637 672 675 681 683 692 698\n",
      " 701 705 711 722 723 727 787 788] [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79] (800, 299, 299, 3)\n"
     ]
    }
   ],
   "source": [
    "id = h5py.File('./Celldata_S18_155_L76_211203/Random_id_80_2021_12_22.mat', 'r')\n",
    "\n",
    "images_n  = np.zeros(shape=(stim_arr.shape[0], 299, 299, 3))\n",
    "for i in range(stim_arr.shape[0]):\n",
    "    images_n[i] = cv2.resize(stim_arr[i], (299, 299))\n",
    "\n",
    "idx = np.array(id['sampleidlist21']).squeeze().astype('int') - 1\n",
    "idx, unique_idx = np.unique(idx, return_index=True)\n",
    "print(idx, unique_idx, images_n.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 880, 114)\n"
     ]
    }
   ],
   "source": [
    "mat_file = h5py.File('./Celldata_S18_155_L76_211203/celldataS_Natural_objects_800_80_50.mat', 'r')\n",
    "#[num_repetitions, num_images, num_neurons]\n",
    "neural_n = np.transpose(np.array(mat_file['celldataS']), (1, 2, 0)).astype('float16')\n",
    "neural_n = neural_n[:,:880, :]\n",
    "print(neural_n.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800 114 (800, 299, 299, 3)\n"
     ]
    }
   ],
   "source": [
    "n_images = 800\n",
    "n_neurons = neural_n.shape[2]\n",
    "size_imags = images_n.shape[0]\n",
    "print(n_images, n_neurons, images_n.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 11  4  8  0  9  6  5  2 10  3  7]\n",
      "(880, 12, 114)\n",
      "(720, 114)\n",
      "(80, 114)\n"
     ]
    }
   ],
   "source": [
    "reps = neural_n.shape[0] # trials\n",
    "rand_ind = np.arange(reps)\n",
    "np.random.shuffle(rand_ind)\n",
    "print(rand_ind)\n",
    "data_y_train = np.concatenate((np.delete(neural_n[:, :800, :], idx, 1), neural_n[:, 880:, :]), 1).mean(0)\n",
    "temp = np.transpose(neural_n, (1, 0, 2))\n",
    "print(temp.shape)\n",
    "data_y_val = np.concatenate((temp[idx], temp[800:880][unique_idx]), 1)\n",
    "data_y_val = np.transpose(data_y_val, (1, 0, 2))\n",
    "data_y_val = np.mean(data_y_val, 0)\n",
    "print(data_y_train.shape)\n",
    "print(data_y_val.shape)\n",
    "\n",
    "#\n",
    "# data_x = images_n[:, np.newaxis].astype(np.float16)\n",
    "# print('images_n', images_n.shape)\n",
    "# data_x = data_x / 255 # (640, 1, 299, 299)\n",
    "# data_x = np.tile(data_x, [1, 3, 1, 1])\n",
    "# print('data_x', data_x.shape)\n",
    "# data_x_train = data_x[:576]\n",
    "# data_x_val = data_x[576:]as indices must be"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 299, 299, 3)\n",
      "(800, 299, 299, 3)\n",
      "(800, 3, 299, 299) (720, 3, 299, 299) (80, 3, 299, 299)\n"
     ]
    }
   ],
   "source": [
    "print(images_n.shape)\n",
    "#data_x = images_n[:, np.newaxis].astype(np.float16)\n",
    "data_x = images_n.astype(np.float16)\n",
    "print(data_x.shape)\n",
    "data_x = data_x / 255 # (800, 1, 299, 299)\n",
    "#data_x = np.tile(data_x, [1, 3, 1, 1])\n",
    "data_x_train = np.delete(images_n, idx, 0)\n",
    "data_x_val = images_n[idx]\n",
    "\n",
    "data_x = np.transpose(data_x, (0, 3, 1, 2))\n",
    "data_x_train = np.transpose(data_x_train, (0, 3, 1, 2))\n",
    "data_x_val = np.transpose(data_x_val, (0, 3, 1, 2))\n",
    "print(data_x.shape, data_x_train.shape, data_x_val.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val: (80, 3, 299, 299), (80, 114)\n",
      "0 torch.Size([16, 3, 299, 299]) torch.Size([16, 114])\n",
      "1 torch.Size([16, 3, 299, 299]) torch.Size([16, 114])\n",
      "2 torch.Size([16, 3, 299, 299]) torch.Size([16, 114])\n",
      "3 torch.Size([16, 3, 299, 299]) torch.Size([16, 114])\n",
      "4 torch.Size([16, 3, 299, 299]) torch.Size([16, 114])\n"
     ]
    }
   ],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_x, data_y):\n",
    "        self.data_x = data_x\n",
    "        self.data_y = data_y\n",
    "    def __getitem__(self, index):\n",
    "        return self.data_x[index], self.data_y[index]\n",
    "    def __len__(self):\n",
    "        return self.data_x.shape[0]\n",
    "\n",
    "imagenet_mean = torch.tensor([0.485, 0.456, 0.406], dtype=torch.float32).view(1, 3, 1, 1).to(device)\n",
    "imagenet_std = torch.tensor([0.229, 0.224, 0.225], dtype=torch.float32).view(1, 3, 1, 1).to(device)\n",
    "transform = lambda x : (x - imagenet_mean) / imagenet_std\n",
    "\n",
    "dataset_train = Dataset(data_x_train, data_y_train)\n",
    "dataset_val = Dataset(data_x_val, data_y_val)\n",
    "\n",
    "loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle = True)\n",
    "loader_val = torch.utils.data.DataLoader(dataset_val, batch_size=batch_size, shuffle = True)\n",
    "\n",
    "print(f'val: {data_x_val.shape}, {data_y_val.shape}')\n",
    "for i,(x,y) in enumerate(loader_val):\n",
    "    print(i, x.shape, y.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([1, 3, 299, 299])\n",
      "fmap:  torch.Size([1, 384, 17, 17])\n",
      "size:  torch.Size([17, 17])\n",
      "114 torch.Size([17, 17])\n",
      "torch.Size([114, 17, 17])\n"
     ]
    }
   ],
   "source": [
    "# CHOOSE THE AUGMENTS IF NECESSARY\n",
    "lamd_s, lamd_d = [5e-3, 2e-3] # the coefficients of the losses. Try other coefficients!\n",
    "epoches = 10 # total epochs for training the encoder\n",
    "lr = 1e-3 # the learing rate for training the encoder\n",
    "\n",
    "alexnet = models.alexnet(pretrained=True)\n",
    "\n",
    "#\n",
    "alexnet.to(device)\n",
    "alexnet.eval()\n",
    "for param in alexnet.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "x = torch.from_numpy(data_x[0:1]).to(device)\n",
    "print(\"x:\", x.shape)\n",
    "x = x.float()\n",
    "fmap = alexnet(x, layer=insp_layer)\n",
    "\n",
    "neurons = data_y_train.shape[1]\n",
    "sizes = fmap.shape[2:]\n",
    "print(\"fmap: \", fmap.shape)\n",
    "print(\"size: \", sizes)\n",
    "channels = fmap.shape[1]\n",
    "print(neurons, sizes)\n",
    "w_s = nn.Parameter(torch.randn(size=(neurons,) + sizes))\n",
    "print(w_s.shape)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "mse_weight = 1.0\n",
    "l1_weight = 0\n",
    "spa_weight = 1e-1\n",
    "ch_weight = 1e-1\n",
    "lap_weight = 1e-1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def mse_loss(prediction, response, weight=None):\n",
    "    if weight is None:\n",
    "        mse_loss = torch.mean(torch.mean((prediction - response)**2, dim=1))\n",
    "    else:\n",
    "        mse_loss = torch.sum(weight*torch.mean((prediction - response)**2, dim=1))\n",
    "    return mse_loss\n",
    "\n",
    "def l2_norm_regularizer(W):\n",
    "    with torch.autograd.profiler.record_function('l2_norm'):\n",
    "        penalty = torch.mean(torch.sum(W**2))\n",
    "        return penalty\n",
    "\n",
    "def l1_norm_regularizer(W):\n",
    "    with torch.autograd.profiler.record_function('l1_norm'):\n",
    "        penalty = torch.mean(torch.sum(torch.abs(W)))\n",
    "        return penalty\n",
    "\n",
    "def smoothness_regularizer_2d(W_s):\n",
    "    K = torch.tensor([\n",
    "    [0,-1,0],\n",
    "    [-1,4,-1],\n",
    "    [0,-1,0]],dtype=torch.float).to(device)\n",
    "    return torch.sum(F.conv2d(torch.unsqueeze(W_s,1),K.unsqueeze(0).unsqueeze(0))**2)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "class conv_encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, neurons, sizes, channels):\n",
    "        super(conv_encoder, self).__init__()\n",
    "        # PUT YOUR CODES HERE\n",
    "        self.W_s = nn.Parameter(torch.randn(size=(neurons,) + sizes))\n",
    "        self.W_d = nn.Parameter(torch.randn(size = (neurons,channels,1,1)))\n",
    "        self.W_b = nn.Parameter(torch.randn(size = (1,neurons)))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # PUT YOUR CODES HERE\n",
    "        out = torch.einsum('bchw , nhw -> bnchw',x,self.W_s) # dimension : N,n,C,h,w\n",
    "        out = torch.stack(\n",
    "            [F.conv2d(out[:,n,:,:,:],torch.unsqueeze(self.W_d[n],0)) for n in range(neurons)],dim=1)\n",
    "            #dimension:N,n,1,h,w\n",
    "        out = torch.sum(out,dim=(2,3,4))\n",
    "        out = out + self.W_b\n",
    "        return out\n",
    "\n",
    "def Loss(y, pred, W_s, W_d):\n",
    "    return mse_loss(y, pred) * mse_weight + \\\n",
    "          l2_norm_regularizer(W_s) * spa_weight + \\\n",
    "          smoothness_regularizer_2d(W_s) * lap_weight + \\\n",
    "          l2_norm_regularizer(W_d) * ch_weight\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#encoder = conv_encoder(neurons, sizes, channels).to(device)\n",
    "encoder = conv_encoder(neurons, sizes, channels).to(device)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\n    You need to define the conv_encoder() class and train the encoder.\\n    The code of alexnet has been slightly modified from the torchvision, for convenience\\n    of extracting the middle layers.\\n\\n    Example:\\n        >>> x = x.to(device) # x is a batch of images\\n        >>> x = transform(x)\\n        >>> fmap = alexnet(x, layer=insp_layer)\\n        >>> out= encoder(fmap)\\n        >>> ...\\n'"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_model(encoder, optimizer):\n",
    "    losses = []\n",
    "    encoder.train()\n",
    "    for i,(x,y) in enumerate(loader_train):\n",
    "        optimizer.zero_grad()\n",
    "        x = x.float()\n",
    "        y = y.float()\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        x = transform(x)\n",
    "        x = x.float()\n",
    "        fmap = alexnet(x,layer = insp_layer)\n",
    "        out = encoder(fmap)\n",
    "#         print(f'L_e = {l_e} , L_2 = {l_2} , L_l = {l_l}')\n",
    "        loss = Loss(y, out, encoder.W_s, encoder.W_d)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "#         print(f'iteration {i}, train loss: {losses[-1]}')\n",
    "\n",
    "    return losses\n",
    "\n",
    "def validate_model(encoder):\n",
    "    encoder.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    losses = []\n",
    "    for i,(x,y) in enumerate(loader_val):\n",
    "        x = x.float()\n",
    "        y = y.float()\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        x = transform(x)\n",
    "        x = x.float()\n",
    "        fmap = alexnet(x,layer = insp_layer)\n",
    "        out = encoder(fmap)\n",
    "        y_pred.append(out)\n",
    "        y_true.append(y)\n",
    "        loss = loss = Loss(y, out, encoder.W_s, encoder.W_d)\n",
    "        losses.append(loss.item())\n",
    "    y_pred = torch.cat(y_pred)\n",
    "    y_true = torch.cat(y_true)\n",
    "    explained_variance = metrics.explained_variance_score(y_true = y_true.detach().cpu().numpy(),y_pred = y_pred.detach().cpu().numpy())\n",
    "    return explained_variance,sum(losses)/len(losses)\n",
    "\n",
    "\"\"\"\n",
    "    You need to define the conv_encoder() class and train the encoder.\n",
    "    The code of alexnet has been slightly modified from the torchvision, for convenience\n",
    "    of extracting the middle layers.\n",
    "\n",
    "    Example:\n",
    "        >>> x = x.to(device) # x is a batch of images\n",
    "        >>> x = transform(x)\n",
    "        >>> fmap = alexnet(x, layer=insp_layer)\n",
    "        >>> out= encoder(fmap)\n",
    "        >>> ...\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# losses_train = []\n",
    "# losses_val = []\n",
    "# EVs = []\n",
    "\n",
    "losses_train = []\n",
    "losses_val = []\n",
    "EVs = []"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "lr = 0.08\n",
    "optimizer = torch.optim.Adam(encoder.parameters(), lr=lr)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\envs\\neural_control\\lib\\site-packages\\ipykernel_launcher.py:2: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4d47f5e5fd9e46d3a3fa3dedb6777802"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, EV = -2037245028890.9473, val  loss = 55962638745.6 , train loss 75481081856.0\n",
      "epoch 1, EV = -946358136257.1228, val  loss = 25092228300.8 , train loss 25826214707.2\n",
      "epoch 2, EV = -657709915872.5614, val  loss = 17207381811.2 , train loss 14387182284.8\n",
      "epoch 3, EV = -505557451470.5965, val  loss = 13226241843.2 , train loss 10099002419.2\n",
      "epoch 4, EV = -408490575297.1228, val  loss = 10651725004.8 , train loss 7948655974.4\n",
      "epoch 5, EV = -341116086918.7368, val  loss = 8906642124.8 , train loss 5864937984.0\n",
      "epoch 6, EV = -291121013688.1404, val  loss = 7608258764.8 , train loss 4864969344.0\n",
      "epoch 7, EV = -253586389477.05264, val  loss = 6628985753.6 , train loss 4115235200.0\n",
      "epoch 8, EV = -226030384882.5263, val  loss = 5850041856.0 , train loss 3419361177.6\n",
      "epoch 9, EV = -200295453228.9123, val  loss = 5233480908.8 , train loss 3069682329.6\n",
      "epoch 10, EV = -181684663601.4035, val  loss = 4707577241.6 , train loss 2468195481.6\n",
      "epoch 11, EV = -164891119301.61404, val  loss = 4296281497.6 , train loss 2233622208.0\n",
      "epoch 12, EV = -151635212620.3509, val  loss = 3955985510.4 , train loss 1985854860.8\n",
      "epoch 13, EV = -139513577472.0, val  loss = 3635269273.6 , train loss 1805355494.4\n",
      "epoch 14, EV = -129889042791.29825, val  loss = 3383927244.8 , train loss 1584454054.4\n",
      "epoch 15, EV = -121068553745.9649, val  loss = 3136845363.2 , train loss 1399880972.8\n",
      "epoch 16, EV = -112705018727.29825, val  loss = 2922610432.0 , train loss 1272832499.2\n",
      "epoch 17, EV = -106647057416.98245, val  loss = 2765557862.4 , train loss 1234204928.0\n",
      "epoch 18, EV = -99765119056.8421, val  loss = 2583585484.8 , train loss 1116475104.0\n",
      "epoch 19, EV = -94305803502.0351, val  loss = 2453890304.0 , train loss 1004358508.8\n",
      "epoch 20, EV = -89006074826.10527, val  loss = 2302579123.2 , train loss 927199123.2\n",
      "epoch 21, EV = -84559836936.98245, val  loss = 2194345164.8 , train loss 870400230.4\n",
      "epoch 22, EV = -80180428274.52632, val  loss = 2076815872.0 , train loss 795829644.8\n",
      "epoch 23, EV = -76521550565.05263, val  loss = 1987757465.6 , train loss 760775788.8\n",
      "epoch 24, EV = -72736715097.82455, val  loss = 1880373427.2 , train loss 683794720.0\n",
      "epoch 25, EV = -69808049524.77193, val  loss = 1804984012.8 , train loss 668068524.8\n",
      "epoch 26, EV = -66384297777.40351, val  loss = 1725743334.4 , train loss 620272371.2\n",
      "epoch 27, EV = -63520438658.24561, val  loss = 1650798156.8 , train loss 575200032.0\n",
      "epoch 28, EV = -60855814858.10526, val  loss = 1583246054.4 , train loss 555120134.4\n",
      "epoch 29, EV = -58816961091.36842, val  loss = 1525834624.0 , train loss 512298547.2\n",
      "epoch 30, EV = -56357091080.98245, val  loss = 1457730329.6 , train loss 483079107.2\n",
      "epoch 31, EV = -54174777132.91228, val  loss = 1400247756.8 , train loss 458074528.0\n",
      "epoch 32, EV = -51955847590.17544, val  loss = 1346774809.6 , train loss 436896070.4\n",
      "epoch 33, EV = -50330814248.42105, val  loss = 1299241446.4 , train loss 409331532.8\n",
      "epoch 34, EV = -48358163961.26316, val  loss = 1250740761.6 , train loss 404477731.2\n",
      "epoch 35, EV = -46565249585.40351, val  loss = 1202453849.6 , train loss 371590640.0\n",
      "epoch 36, EV = -45044904117.89474, val  loss = 1168568281.6 , train loss 361469654.4\n",
      "epoch 37, EV = -43520354160.2807, val  loss = 1127501478.4 , train loss 344234124.8\n",
      "epoch 38, EV = -42003626426.38596, val  loss = 1085971750.4 , train loss 332491529.6\n",
      "epoch 39, EV = -40495173018.947365, val  loss = 1047104896.0 , train loss 313771068.8\n",
      "epoch 40, EV = -39381922000.8421, val  loss = 1020218752.0 , train loss 301288608.0\n",
      "epoch 41, EV = -37968128121.26316, val  loss = 983193612.8 , train loss 277770190.4\n",
      "epoch 42, EV = -36995502342.73684, val  loss = 952433510.4 , train loss 263710504.0\n",
      "epoch 43, EV = -35772385987.36842, val  loss = 928093747.2 , train loss 259355347.2\n",
      "epoch 44, EV = -34709783927.01755, val  loss = 897497638.4 , train loss 250246288.0\n",
      "epoch 45, EV = -33600564648.42105, val  loss = 868631296.0 , train loss 236970486.4\n",
      "epoch 46, EV = -32637786886.736843, val  loss = 844293248.0 , train loss 235069414.4\n",
      "epoch 47, EV = -31707301295.157894, val  loss = 819166208.0 , train loss 222378699.2\n",
      "epoch 48, EV = -30617433049.824562, val  loss = 794820787.2 , train loss 210814432.0\n",
      "epoch 49, EV = -29769694021.614037, val  loss = 773015091.2 , train loss 202376070.4\n"
     ]
    }
   ],
   "source": [
    "epoches = 50\n",
    "for epoch in tqdm_notebook(range(epoches)):\n",
    "    losses_train += train_model(encoder,optimizer)\n",
    "    ev,loss = validate_model(encoder)\n",
    "    EVs.append(ev)\n",
    "    losses_val.append(loss)\n",
    "    print(f'epoch {epoch}, EV = {ev}, val  loss = {loss} , train loss {sum(losses_train[-10:])/10}')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
